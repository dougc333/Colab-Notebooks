{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMeRrlZN5Cfifgy9TdrG2CH"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dougc333/Colab-Notebooks/blob/main/broadcast_1.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Numpy, Pytorch**\n",
    "\n",
    "<ol>\n",
    "<li>(3,)</li>\n",
    "<li>np.reshape() is a view, np.reshape(-1,1). the -1 means let numpy figure out the row dim, specify 1 column. </li>\n",
    "<li>numpy newaxis()/squeeze() vs pytorch squeeze()/unsqueeze()</li>\n",
    "<li></li>\n",
    "<li></li>\n",
    "\n",
    "</ol>"
   ],
   "metadata": {
    "id": "asSzBYWVYWmD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSMd1ZDAcpjy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767134965673,
     "user_tz": 480,
     "elapsed": 15993,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "0bd4e89e-618b-4fc5-bfcb-970f481e2e06"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/drive/MyDrive/'Colab Notebooks'\n",
    "#move to Colab-Notebooks to check into git repo"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8D-v9X5csAl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767134967503,
     "user_tz": 480,
     "elapsed": 739,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "f020b069-bde0-4268-eec6-efe9f47722fc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **(3,) is a list, not a 3d array; not (3,1) or (1,3)**"
   ],
   "metadata": {
    "id": "gFeBjYgQYzb9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,1,0])\n",
    "print(a.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7acOMhYY7Ft",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767133924597,
     "user_tz": 480,
     "elapsed": 45,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "3de57584-4c26-494a-b91f-817230e81803"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **np,dot has 2 APIs**\n",
    "\n",
    "It figures out which operation depending on input shapes\n",
    "\n",
    "matrix multiply: np.dot() = $\\sum_N a[i]*b[i]$, output is matrix\n",
    "\n",
    "vector dot product np.dot(), the math is exactly the same; output is scalar\n",
    "\n"
   ],
   "metadata": {
    "id": "p30YrghPZAJ-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xX2Avp6XEvCc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767134087852,
     "user_tz": 480,
     "elapsed": 4,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "241e763e-b087-42d2-a007-b0bbe8f8a27d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 1 0]\n",
      "[[-1]\n",
      " [ 2]\n",
      " [ 3]]\n",
      "a.T * b = [1]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#this is a confusing example because the matrix case is a 1d vector. They should use a 2x2\n",
    "a = np.array([1,1,0])\n",
    "\n",
    "b = np.array([-1,2,3]).reshape(-1,1)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(f'a.T * b = {np.dot(a,b)}')\n",
    "\n",
    "print(\"I am a vector dot product:\",np.dot(np.array([1,1,1]), np.array([1,1,1]) ))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x = np.zeros((3,1,4))\n",
    "y = np.ones((1,5,4))\n",
    "z = x + y\n",
    "\n",
    "print(z.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szJ0mQdyFAjS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767111921460,
     "user_tz": 480,
     "elapsed": 7,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "323c0d7c-c816-4ef4-c4f1-3e23de6bc90d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 5, 4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**np.reshape()**\n",
    "\n",
    "\n",
    "Multiple ways to do reshaping from row major to column major\n",
    "\n",
    "[1,2,3] to column major [[1],\n",
    "                         [2],\n",
    "                         [3]]\n",
    "\n",
    "row major shape is (1,3), column major is (3,1). The brackets in the column major format are important because it indicates 3 separate rows.\n",
    "\n",
    "There are multiple ways to reshape a row major to column major format. The clearest but the least used is to reshape explicitly with known dimensions\n",
    "\n",
    "np.reshape(array, shape) so to reshape a row major to column major format\n",
    "\n",
    "a = [1,2,3]\n",
    "b = np.reshape(a, (1,3))\n",
    "\n",
    "Unfortunately this isn't what people do.\n",
    "\n",
    "b = a.reshape(-1,1) they ignore the first dimension using -1 and specify 1 column. Numpy figures out the -1 says numpy will figure out the num rows.\n",
    "\n",
    "This is ok but it carries into pytorch and it is necessary to practice these for formatting batches in LLMs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "7OGcNc-uQGFX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "b = np.array([-1,2,3]).reshape(-1,1)\n",
    "print(b)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5IVXd1JPInc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767114583552,
     "user_tz": 480,
     "elapsed": 9,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "65a46502-340a-4795-ad96-1cf6aa564a12"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1]\n",
      " [ 2]\n",
      " [ 3]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Shapes**\n",
    "\n",
    "<ul>\n",
    "<b>(3,) isn't 3 rows!!!</b>\n",
    "<li>A list is 1d and shape (len, ) is neither a row or col</li>\n",
    "<li>They defined a new convention which is reveresed from normal APIs</li>\n",
    "<li>a = np.array([10, 20, 30]) is shape (3,)</li>\n",
    "<li>Same in pytorch, b = torch.tensor([10,20,30]); torch.Size([3]) </li>\n",
    "<li>both numpy and pytorch have to be resized to row,col format for a 2D matrix. </li>\n",
    "<b>Adding an axis in numpy or adding a dim in pytorch</b>\n",
    "<li>Numpy convert (3,) to (3,1)</li>\n",
    "<li>np.reshape(np.array([1,2,3]), (3,1))</li>\n",
    "<li>np.rehape(np.array([1,2,3]), (1,3))</li>\n",
    "<b>np.resize() cycles through the original data if not enough elements</b>\n",
    "<li>np.resize(np.array([1,2,3]), (3,1))</li>\n",
    "<li>np.resize(np.array([1,2,3]), (1,3))</li>\n",
    "<li>pytorch pads with 0 if not enough data in resize</li>\n",
    "\n",
    "\n",
    "<b>Numpy newaxis() and squeeze() vs pytorch squeeze() and unsqueeze()</b>\n",
    "<li>np.newaxis()</li>\n",
    "<li>np.squeeze()</li>\n",
    "\n",
    "<li>Pytorch convert torch.Size([3]) to torch.Size([3,1])</li>\n",
    "<li></li>\n",
    "\n",
    "<b>Remove an axis</b>\n",
    "<li></li>\n",
    "<b>Testing for Contiguous</b>\n",
    "<li>Performance bottlenecks when data operations break contiguous memory both in CPU and GPU models</li>\n",
    "<li>The normal pytorch format is (Batch, Channels, Height, Width) but CNNS require Channels last! Use tensor.to(memory_format=torch.channels_last)</li>\n",
    "\n",
    "<b>Transpose Example</b>\n",
    "<li>a.T breaks contiguouis. It is a view but it causes 2-10x slowdown when materializing the calculation</li>\n",
    "<li>GPU expects (NHWC) for channels last. </li>\n",
    "</ul>\n"
   ],
   "metadata": {
    "id": "hl1cjyqznLTk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "b_prime = np.reshape(np.array([-1,2,3]),(3,1))\n",
    "print(b_prime)\n",
    "# reshape is a new view, resize is a new memory layout with removed or added elements\n",
    "print(---------------------------------------)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7_P4kwHPIqC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767114643352,
     "user_tz": 480,
     "elapsed": 3,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "2d5f0e66-1b90-42f1-c34f-35618029cb71"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1]\n",
      " [ 2]\n",
      " [ 3]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**np.dot(), confusing**\n",
    "\n",
    "<ul>\n",
    "<b>np.dot() is a dot product or multiply if 1d  AND matmul for 2d arrays/matrices</b>\n",
    "<li>a = np.ones(2,2)</li>\n",
    "<li>b = 5*np.ones(2,2)</li>\n",
    "<li>print(a,b)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<b>mental model</b>\n",
    "<li>$result[i,j]=\\sum_{k}A[i,k]\u00d7B[k,j]$</li>\n",
    "<li>First match the i,j, k indexes. There can only be one k dim which has to match the first and second matrix from matrix math rules. Match in order, (i,k,j) to avoid mistakes. </li>\n",
    "<li>i=2, k=2, j=2</li>\n",
    "<li>result shape =(i,j)= (2,2); </li>\n",
    "<li>Iterate through the result dims. </li>\n",
    "<li>result(1,1) = a(1,1)*b(1,1) + a(1,2)*b(2,1) = 1*5 + 1+5=10  </li>\n",
    "<li>result(1,2) = a(1,1)*b(1,1) + a(1,2)*b(2,1) = 1*5 + 1+5=10</li>\n",
    "<li>result(2,1) = a(1,1)*b(1,1) + a(1,2)*b(2,1) = 1*5 + 1+5=10</li>\n",
    "<li>result(2,2) = a(1,1)*b(1,1) + a(1,2)*b(2,1) = 1*5 + 1+5=10</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<b>Matmul np.matmul(a,b) or a@b</b>\n",
    "<li>Assume multi dimensions not 2dim case which np.dot handles</li>\n",
    "<li>(B,T,C) convention Batch, Time, Columns. Time is rows. </li>\n",
    "<li>MatMul rules (B,T,C) * (C,X). </li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<b>The big issue a@b != np.dot(a,b) for dims>2</b>\n",
    "<li>Why? Because np.dot doesnt broadcast but a@b does</li>\n",
    "<li></li>\n",
    "<li></li>\n",
    "<li></li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<b>CPU and GPU performance</b>\n",
    "<li>CPU memory layout is row major, increment columns first for sequential cache/dram or memory access. CPU preloads data in chunks to L1 cache</li>\n",
    "<li>GPU has 32 threads. Each thread should have data. </li>\n",
    "</ul>\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "WgCNif11V_a3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# test np.dot\n",
    "a = np.ones((2,2))\n",
    "print(a.flags) # verify row major and not column major(fortan)\n",
    "b = 5 * np.ones((2,2))\n",
    "print(a)\n",
    "print('------')\n",
    "print(b)\n",
    "print('------')\n",
    "print(a*b)\n",
    "print('------')\n",
    "print(np.dot(a,b))\n",
    "\n",
    "\n",
    "print('1d dot product same as multiply')\n",
    "print(np.dot(5,6))\n",
    "print('1d array dot product same as element wise multiply')\n",
    "print(np.dot([1,1,1],[2,2,2]))\n",
    "print('1d numpy array dot product same as elemnent wise multiply')\n",
    "print(np.dot(np.array([1,1,1]),np.array([2,2,2])))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1knaVCWWpzV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767119164636,
     "user_tz": 480,
     "elapsed": 7,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "63aa261f-a420-4728-992a-904f8498f856"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  C_CONTIGUOUS : True\n",
      "  F_CONTIGUOUS : False\n",
      "  OWNDATA : True\n",
      "  WRITEABLE : True\n",
      "  ALIGNED : True\n",
      "  WRITEBACKIFCOPY : False\n",
      "\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "------\n",
      "[[5. 5.]\n",
      " [5. 5.]]\n",
      "------\n",
      "[[5. 5.]\n",
      " [5. 5.]]\n",
      "------\n",
      "[[10. 10.]\n",
      " [10. 10.]]\n",
      "1d dot product same as multiply\n",
      "30\n",
      "1d array dot product same as element wise multiply\n",
      "6\n",
      "1d numpy array dot product same as elemnent wise multiply\n",
      "6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Broadcasting**\n",
    "\n",
    "Broadcasting is a numpy and pytorch feature for adjusting vector and matrix multiply dimensions beyond the standard (a,b) * (b,c) rule when there are more than 2 dims.  \n",
    "\n",
    "Broadcasting widely used in image processing. X images, multiply each subimage with a filter will use broadcasting rules.\n",
    "\n",
    "LLMs will use broadcasting extensively\n",
    "\n",
    "\n",
    "<b>Numpy Broadcasting; vector scalar right aligned, matrix batch aligned</b>\n",
    "<ul>\n",
    "<li>Elements are broadcast or replicated with stride 0 trick. Use stride 0 to make it match the broadcast dimension. When computing the loop iteration pytorc and GPU replace the stride element with stride[0] which means keep on reading the current element. It doesn't allocate new memory for broadcast.  </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<b>Numpy warning. np.dot() doesn't support broadcasting, *, @ do. </b>\n",
    "<li>both pytorch and numpy broadcasting don't allocate memory</li>\n",
    "<li>They use a stride trick; instead of a stride of 1 they keep on reading the same element which is a stride of 0. </li>\n",
    "</ul>\n",
    "\n",
    "<b>Element wise Broadcast Rules</b>\n",
    "<ol>\n",
    "<li>first left pad the smaller shape then compare each dim </li>\n",
    "<li>either dim ==1 or both equal, (3,1), (1,1) then dimension passes </li>\n",
    "<li>Broadcast fails if neither dim ==1 and they aren't equal to each other. </li>\n",
    "</ol>\n",
    "\n",
    "<ul>\n",
    "<b>Example</b>\n",
    "<li>a=(2,5),b=(3,). Can we broadcast these?</li>\n",
    "<li>(2,5)</li>>\n",
    "<li>1) (1,3) left pad the smaller one to right align</li>\n",
    "<li>Iterate through the dims</li>\n",
    "<li>2) Compare 2<->1, this ok bc one of the dims is a 1</li>\n",
    "<li>3) Compare 5<->3, this fails because the dims not equal and neither one is 1. </li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<b>Example</b>\n",
    "<li>a=(2,10),b=(10,). Can we broadcast these?</li>\n",
    "<li>1) left pad. (2,10), (1,10)   </li>\n",
    "<li>Iterate through the dims</li>\n",
    "<li>2) compare dim 1: 2<->1, ok one dim ==1</li>\n",
    "<li>3) comapare dim2: 10<->10 ok, both equal</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<b>Example</b>\n",
    "<li>a=(4,3,2), b=(2,)</li>\n",
    "<li>left pad (4,3,2), (1,1,2)</li>\n",
    "<li>Iterate through the dims</li>\n",
    "<li> 4<->1; ok </li>\n",
    "<li> 3<->1; ok </li>\n",
    "<li> 2<->2; ok </li>\n",
    "<li>all tests pass</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<b>Example</b>\n",
    "<li>a=(6,1,5), b=(3,5)</li>\n",
    "<li>left pad b;a=(6,1,5), b=(1,3,5) </li>\n",
    "<li>Iterate through the dims</li>\n",
    "<li> 6<->1; ok, second dim is 1 </li>\n",
    "<li> 1<->3; ok, first dim is 1  </li>\n",
    "<li> 5<->5; ok, equal dims</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<b>Example</b>\n",
    "<li>a = (4,3,2),  b= (4,2)</li>\n",
    "<li>left pad b: a = (4,3,2),  b= (1, 4,2)</li>\n",
    "<li>Iterate through the dims</li>\n",
    "<li> 4<->1; ok </li>\n",
    "<li> 3<->4; fails </li>\n",
    "<li> 2<->2; ok </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "<b>Rules modification for matmul</b>\n",
    "<ol>\n",
    "<li>pytorch and numpy same</li>\n",
    "<li>last 2 dims do not broadcast</li>\n",
    "<li>Verify matmul, last 2 dims follow matmul rules (a,b)x(b,c); the b has to match</li>\n",
    "<li>Follow earlier elementwise broacasting</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<b>Matmul broadcast Example</b>\n",
    "<li>A: (3, 4), B: (2, 5, 4, 6)</li>\n",
    "<li>match matrix dims B:(2, 5, 4, 6) and left pad for right alignment</li>\n",
    "<li>(2, 5, 4, 6)</li>\n",
    "<li>(1, 1, 3, 4)</li>\n",
    "<li>Iterate through the dims</li>\n",
    "<li> 2<->1; ok </li>\n",
    "<li> 5<->1; ok </li>\n",
    "<li> 4<->3; dont align, verify matrix rules; ok </li>\n",
    "<li> 2<->1; dont align, verify matrix rules; ok </li>\n",
    "<li>OK</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<b>Matmul broadcast Example</b>\n",
    "<li>A: (4, 3, 5, 3, 8) B: (8, 6)</li>\n",
    "<li>match matrix dims B:(2, 5, 4, 6) and left pad for right alignment</li>\n",
    "<li>(4, 3, 5, 3, 8)</li>\n",
    "<li>(1, 1, 1, 8, 6)</li>\n",
    "<li>Iterate through the dims</li>\n",
    "<li> 4<->1; ok </li>\n",
    "<li> 3<->1; ok </li>\n",
    "<li> 5<->1; ok </li>\n",
    "<li> 3<->8; dont align, verify matrix rules; ok </li>\n",
    "<li> 8<->6; dont align, verify matrix rules; ok </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<b>Matmul broadcast Example</b>\n",
    "<li>A: (4,2,3,5) B: (3,2,5,6)</li>\n",
    "<li>match matrix dims, no left pad needed</li>\n",
    "<li>(4, 2, 3, 5)</li>\n",
    "<li>(3, 2, 5, 6)</li>\n",
    "<li>Iterate through the dims</li>\n",
    "<li> 4<->3; fails</li>\n",
    "<li> 2<->2; ok </li>\n",
    "<li> 3<->5; dont align, verify matrix rules; ok</li>\n",
    "<li> 5<->6; dont align, verify matrix rules; ok</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<b>pytorch matrix broadcast</b>\n",
    "<ul>\n",
    "<li>[..., m, k]  @  [..., k, n]  \u2192  [..., m, n]</li>\n",
    "</ul>\n",
    "\n"
   ],
   "metadata": {
    "id": "hX-PbF8HRefN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "AWt7etseRkP4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# test examples with pytorch\n",
    "\n",
    "#Broadcast vector to scalar\n",
    "print('vector scalar broadcasting')\n",
    "a = np.array([1,1,1])\n",
    "b = np.array([3])\n",
    "print(a.shape, b.shape)\n",
    "# right->left align (3,), (1,) means 1 gets expanded to 3\n",
    "\n",
    "\n",
    "print('--------------')\n",
    "print('matrix broadcasting')\n",
    "print('careful matrix broadcasting rules different than scalar')\n",
    "print('--------------')\n",
    "\n",
    "A = np.ones((10, 3, 4))\n",
    "B = np.ones((1, 4, 5))\n",
    "\n",
    "C = A @ B\n",
    "print(C.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5hWlmHrjP44",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767126425864,
     "user_tz": 480,
     "elapsed": 7,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "04eca1e2-d168-49d1-c837-f77af7d590f6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3,) (1,)\n",
      "--------------\n",
      "(10, 3, 5)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **DANGER**\n",
    "\n",
    "\n",
    "<b>Numpy</b>\n",
    "<ul>\n",
    "<li>np.dot doesnt broadcast</li>\n",
    "<li>Wont generate error messages</li>\n",
    "<li>*, np.matmul(), @ broadcast</li>\n",
    "</ul>\n",
    "\n",
    "<b>pytorch</b>\n",
    "<ul>\n",
    "<li>torch.mm(), torch.bmm(), torch.dot() dont broadcast</li>\n",
    "<li>Will generate error messages warning if dims dont match</li>\n",
    "<li>@, torch.matmul() broadcast</li>\n",
    "</ul>"
   ],
   "metadata": {
    "id": "Kc6dd4d08nvi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **np.dot() does not broadcast np.dot(a,b) != a@b**\n",
    "\n",
    "do not use!!!"
   ],
   "metadata": {
    "id": "IzZ4_fqhT0BE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# np.dot() doesnt broadcase\n",
    "# * and @ are not the same for more than 2 dims because of broadcasting. np.dot() doesnt broadcast but @ does.\n",
    "# np.dot considered legacy.\n",
    "\n",
    "a = np.random.rand(3,4,2)\n",
    "b = np.random.rand(2,4)\n",
    "d = np.dot(a,b)\n",
    "e = a@b\n",
    "print('-----------')\n",
    "print(d==e)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThNs_esL6rFj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767126133045,
     "user_tz": 480,
     "elapsed": 18,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "caab5d77-8c2c-4e92-f6a9-1151b4d41c13"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------\n",
      "[[[ True  True  True  True]\n",
      "  [ True  True  True False]\n",
      "  [ True  True  True  True]\n",
      "  [ True False  True  True]]\n",
      "\n",
      " [[ True  True  True  True]\n",
      "  [False  True  True False]\n",
      "  [ True  True  True  True]\n",
      "  [False False  True  True]]\n",
      "\n",
      " [[ True  True  True  True]\n",
      "  [False  True  True False]\n",
      "  [ True  True  True  True]\n",
      "  [ True  True  True  True]]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# **Operations which produce (3,).**\n",
    "\n",
    "<ul>\n",
    "<li>x.mean(), center a batch. Uses broadcasting, need keepdim=True to prevent (4,). keepdim=True gives (4,1). We can unsqueeze to get same result. </li>\n",
    "</ul>\n"
   ],
   "metadata": {
    "id": "FPknRig6fmCg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1) [1,2,3]\n",
    "# 2) use x.repeat() to make (B,T,C)\n",
    "# 3) calculate x.mean(dim=0, keepdim=True) dim=0 is a row mean if (T,C), dim=1 is a row mean if (B,T,C).\n",
    "\n",
    "\n",
    "# compute a average across all rows and subtract the mean from each entry in the row\n",
    "# thsi is the lousy api but easy to remember\n",
    "a = torch.tensor([1,2,3])\n",
    "a_mean = torch.mean(a, dtype=torch.float32)\n",
    "print(f'a_mean:{a_mean}')\n",
    "a_centered = a-a_mean\n",
    "print(f'a_centered:{a_centered}')\n",
    "\n",
    "# create a batch of 4 for [1,2,3]\n",
    "\n",
    "a_batch = a.repeat(4, 1)\n",
    "print(a_batch.shape)\n",
    "print(\"a_batch:\",a_batch)\n",
    "\n",
    "# how to traverse rows with torch.mean(), use broadcasting\n",
    "# this is the more complicated api, requries a dim=X term.\n",
    "# keepdim prevents (4,) which screws up broadcasting\n",
    "a_mean_nodims = a_batch.mean(dim=1, keepdim=False, dtype=torch.float)\n",
    "a_mean = a_batch.mean(dim=1, keepdim=True, dtype=torch.float)\n",
    "\n",
    "#see how mean_nodims missing 1? shape is (4,)\n",
    "print(a_mean_nodims.shape, a_mean.shape)\n",
    "\n",
    "print(a_mean)\n",
    "a_centered = a_batch - a_mean\n",
    "print(a_centered)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1rXAvRWkK2x",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767139220921,
     "user_tz": 480,
     "elapsed": 19,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "f697f72a-808a-4933-cd23-caba2bece398"
   },
   "execution_count": 83,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a_mean:2.0\n",
      "a_centered:tensor([-1.,  0.,  1.])\n",
      "torch.Size([4, 3])\n",
      "a_batch: tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n",
      "torch.Size([4]) torch.Size([4, 1])\n",
      "tensor([[2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "tensor([[-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# **torch.unsqueeze(index_pos)**\n",
    "\n",
    "Adding dim of 1 to shape. index_pos tells torch to insert at index position 0,1,2,-1 using python indexing.\n",
    "\n",
    "\n",
    "torch.unsqueeze(index) inserts a 1 at index position.\n",
    "\n",
    "torch.unsqueeze(0) inserts a 1 at dim 0, (2,2) -> 1,2,2\n",
    "\n",
    "torch.usqueeze(1) inserts a 1 at dim 1, (2,2) -> 2,1,2\n",
    "\n",
    "torch.unsqueeze(-1) inserts a 1 at last dim, (2,2)->(2,2,-1)\n",
    "\n"
   ],
   "metadata": {
    "id": "jRx9Fpudivw0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(torch.ones((2,2)).unsqueeze(0).shape)\n",
    "print(torch.ones((2,2)).unsqueeze(1).shape)\n",
    "print( torch.ones((2,2)).unsqueeze(-1).shape)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZ6bY9CADq2f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767136708080,
     "user_tz": 480,
     "elapsed": 8,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "0e7a1336-cfc5-4f9e-8876-bed3ca14dc44"
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 2, 2])\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 2, 1])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test Programs; chatGPT generated. Are these useful?"
   ],
   "metadata": {
    "id": "XatTkdpAfT6T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "def broadcast_shape(shape_a: Tuple[int, ...],\n",
    "                    shape_b: Tuple[int, ...]) -> Tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Compute the broadcasted shape of two tensors, or raise ValueError\n",
    "    if they are not broadcastable (NumPy/PyTorch rules).\n",
    "    \"\"\"\n",
    "    # Align from the right by prepending 1s on the shorter shape\n",
    "    a = list(shape_a)\n",
    "    b = list(shape_b)\n",
    "\n",
    "    max_len = max(len(a), len(b))\n",
    "    a = [1] * (max_len - len(a)) + a\n",
    "    b = [1] * (max_len - len(b)) + b\n",
    "\n",
    "    out: List[int] = []\n",
    "    for dim_a, dim_b in zip(a, b):\n",
    "        if dim_a == dim_b or dim_a == 1 or dim_b == 1:\n",
    "            out.append(max(dim_a, dim_b))\n",
    "        else:\n",
    "            raise ValueError(f\"Shapes {shape_a} and {shape_b} are not \"\n",
    "                             f\"broadcastable: conflict {dim_a} vs {dim_b}\")\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "def explain_broadcast(shape_a: Tuple[int, ...],\n",
    "                      shape_b: Tuple[int, ...]) -> None:\n",
    "    \"\"\"\n",
    "    Print a step-by-step explanation of broadcasting between two shapes.\n",
    "    \"\"\"\n",
    "    print(f\"A shape: {shape_a}\")\n",
    "    print(f\"B shape: {shape_b}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Align with leading 1s\n",
    "    a = list(shape_a)\n",
    "    b = list(shape_b)\n",
    "    max_len = max(len(a), len(b))\n",
    "    a = [1] * (max_len - len(a)) + a\n",
    "    b = [1] * (max_len - len(b)) + b\n",
    "\n",
    "    print(\"Aligned (with leading 1s):\")\n",
    "    print(f\"A aligned: {tuple(a)}\")\n",
    "    print(f\"B aligned: {tuple(b)}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    out: List[int] = []\n",
    "    print(\"Compare dimensions (left \u2192 right):\")\n",
    "    for i, (dim_a, dim_b) in enumerate(zip(a, b)):\n",
    "        pos = i - max_len  # relative from the right: -1 is last dim\n",
    "        msg = f\"  dim {i} (from left, pos {pos:+} from right): {dim_a} vs {dim_b} -> \"\n",
    "        if dim_a == dim_b:\n",
    "            out_dim = dim_a\n",
    "            reason = \"same, keep\"\n",
    "        elif dim_a == 1:\n",
    "            out_dim = dim_b\n",
    "            reason = \"A expands to match B\"\n",
    "        elif dim_b == 1:\n",
    "            out_dim = dim_a\n",
    "            reason = \"B expands to match A\"\n",
    "        else:\n",
    "            print(msg + \"\u274c conflict (not broadcastable)\")\n",
    "            print()\n",
    "            print(f\"Result: shapes {shape_a} and {shape_b} are NOT broadcastable.\")\n",
    "            return\n",
    "        out.append(out_dim)\n",
    "        print(msg + f\"\u2714 {out_dim} ({reason})\")\n",
    "\n",
    "    out_shape = tuple(out)\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Broadcasted shape: {out_shape}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# --------- quick tests / examples ---------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Your classic (B,T,1) \u00d7 (B,1,D)\n",
    "    B, T, D = 2, 4, 8\n",
    "    explain_broadcast((B, T, 1), (B, 1, D))\n",
    "\n",
    "    # 2) (B,T,D) + (D,)\n",
    "    explain_broadcast((B, T, D), (D,))\n",
    "\n",
    "    # 3) (B,T) \u00d7 (D,) \u2013 usually invalid unless T == D\n",
    "    try:\n",
    "        explain_broadcast((B, T), (D,))\n",
    "    except Exception as e:\n",
    "        print(\"Caught error:\", e)\n",
    "\n",
    "    # 4) Quick programmatic use:\n",
    "    print(\"broadcast_shape((3, 1), (1, 4)) ->\",\n",
    "          broadcast_shape((3, 1), (1, 4)))\n"
   ],
   "metadata": {
    "id": "BBwPGRCcE73k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764529894656,
     "user_tz": 480,
     "elapsed": 14,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "c9f3de19-76f2-4ded-b3a7-59da0b74b3b8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A shape: (2, 4, 1)\n",
      "B shape: (2, 1, 8)\n",
      "----------------------------------------\n",
      "Aligned (with leading 1s):\n",
      "A aligned: (2, 4, 1)\n",
      "B aligned: (2, 1, 8)\n",
      "----------------------------------------\n",
      "Compare dimensions (left \u2192 right):\n",
      "  dim 0 (from left, pos -3 from right): 2 vs 2 -> \u2714 2 (same, keep)\n",
      "  dim 1 (from left, pos -2 from right): 4 vs 1 -> \u2714 4 (B expands to match A)\n",
      "  dim 2 (from left, pos -1 from right): 1 vs 8 -> \u2714 8 (A expands to match B)\n",
      "----------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n",
      "A shape: (2, 4, 8)\n",
      "B shape: (8,)\n",
      "----------------------------------------\n",
      "Aligned (with leading 1s):\n",
      "A aligned: (2, 4, 8)\n",
      "B aligned: (1, 1, 8)\n",
      "----------------------------------------\n",
      "Compare dimensions (left \u2192 right):\n",
      "  dim 0 (from left, pos -3 from right): 2 vs 1 -> \u2714 2 (B expands to match A)\n",
      "  dim 1 (from left, pos -2 from right): 4 vs 1 -> \u2714 4 (B expands to match A)\n",
      "  dim 2 (from left, pos -1 from right): 8 vs 8 -> \u2714 8 (same, keep)\n",
      "----------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n",
      "A shape: (2, 4)\n",
      "B shape: (8,)\n",
      "----------------------------------------\n",
      "Aligned (with leading 1s):\n",
      "A aligned: (2, 4)\n",
      "B aligned: (1, 8)\n",
      "----------------------------------------\n",
      "Compare dimensions (left \u2192 right):\n",
      "  dim 0 (from left, pos -2 from right): 2 vs 1 -> \u2714 2 (B expands to match A)\n",
      "  dim 1 (from left, pos -1 from right): 4 vs 8 -> \u274c conflict (not broadcastable)\n",
      "\n",
      "Result: shapes (2, 4) and (8,) are NOT broadcastable.\n",
      "broadcast_shape((3, 1), (1, 4)) -> (3, 4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "B, T, D = 2, 4, 8\n",
    "explain_broadcast((B, T, 1), (B, 1, D))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ijNHOiBLXYA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764529961278,
     "user_tz": 480,
     "elapsed": 9,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "54237c22-0c86-4914-a929-7c56b5ecd489"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A shape: (2, 4, 1)\n",
      "B shape: (2, 1, 8)\n",
      "----------------------------------------\n",
      "Aligned (with leading 1s):\n",
      "A aligned: (2, 4, 1)\n",
      "B aligned: (2, 1, 8)\n",
      "----------------------------------------\n",
      "Compare dimensions (left \u2192 right):\n",
      "  dim 0 (from left, pos -3 from right): 2 vs 2 -> \u2714 2 (same, keep)\n",
      "  dim 1 (from left, pos -2 from right): 4 vs 1 -> \u2714 4 (B expands to match A)\n",
      "  dim 2 (from left, pos -1 from right): 1 vs 8 -> \u2714 8 (A expands to match B)\n",
      "----------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Sequence\n",
    "\n",
    "Shape = Tuple[int, ...]\n",
    "\n",
    "\n",
    "def broadcast_shape_many(shapes: Sequence[Shape]) -> Shape:\n",
    "    \"\"\"\n",
    "    Compute the broadcasted shape of N tensors (NumPy/PyTorch rules).\n",
    "\n",
    "    shapes: iterable of shapes, e.g. [(B,T,1), (B,1,D), (1,T,1)]\n",
    "    returns: single broadcasted shape, or raises ValueError if incompatible.\n",
    "    \"\"\"\n",
    "    if not shapes:\n",
    "        return ()\n",
    "\n",
    "    # Step 1: compute max rank and left-pad each shape with 1s\n",
    "    max_len = max(len(s) for s in shapes)\n",
    "    aligned: List[List[int]] = []\n",
    "    for s in shapes:\n",
    "        padded = [1] * (max_len - len(s)) + list(s)\n",
    "        aligned.append(padded)\n",
    "\n",
    "    out: List[int] = []\n",
    "    # Step 2: for each dimension (left \u2192 right, but rules are symmetric)\n",
    "    for dim_idx in range(max_len):\n",
    "        dims_here = [a[dim_idx] for a in aligned]\n",
    "        non_ones = sorted({d for d in dims_here if d != 1})\n",
    "\n",
    "        if len(non_ones) == 0:\n",
    "            # all 1s \u2192 result is 1\n",
    "            out_dim = 1\n",
    "        elif len(non_ones) == 1:\n",
    "            # all non-1 dims agree \u2192 that is the result dim\n",
    "            out_dim = non_ones[0]\n",
    "        else:\n",
    "            # more than one distinct non-1 size \u2192 conflict\n",
    "            raise ValueError(\n",
    "                f\"Incompatible shapes at dim {dim_idx}: {dims_here} \"\n",
    "                f\"(non-1 dims {non_ones})\"\n",
    "            )\n",
    "        out.append(out_dim)\n",
    "\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "def explain_broadcast_many(shapes: Sequence[Shape]) -> None:\n",
    "    \"\"\"\n",
    "    Verbose explanation of N-way broadcasting.\n",
    "    \"\"\"\n",
    "    print(\"Input shapes:\")\n",
    "    for i, s in enumerate(shapes):\n",
    "        print(f\"  Tensor {i}: {s}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    if not shapes:\n",
    "        print(\"No shapes given \u2192 result shape is ().\")\n",
    "        return\n",
    "\n",
    "    max_len = max(len(s) for s in shapes)\n",
    "    aligned: List[List[int]] = []\n",
    "    for s in shapes:\n",
    "        padded = [1] * (max_len - len(s)) + list(s)\n",
    "        aligned.append(padded)\n",
    "\n",
    "    print(\"Aligned with leading 1s (so all have same rank):\")\n",
    "    for i, a in enumerate(aligned):\n",
    "        print(f\"  Tensor {i} aligned: {tuple(a)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    out: List[int] = []\n",
    "    print(\"Per-dimension analysis (left \u2192 right):\")\n",
    "    for dim_idx in range(max_len):\n",
    "        dims_here = [a[dim_idx] for a in aligned]\n",
    "        non_ones = sorted({d for d in dims_here if d != 1})\n",
    "\n",
    "        print(f\"\\nDim {dim_idx} (from left):\")\n",
    "        for i, d in enumerate(dims_here):\n",
    "            print(f\"  - Tensor {i}: {d}\")\n",
    "\n",
    "        if len(non_ones) == 0:\n",
    "            out_dim = 1\n",
    "            print(\"  -> All dims are 1 \u2192 result dim = 1\")\n",
    "        elif len(non_ones) == 1:\n",
    "            out_dim = non_ones[0]\n",
    "            expanders = [\n",
    "                i for i, d in enumerate(dims_here) if d == 1 and out_dim != 1\n",
    "            ]\n",
    "            print(f\"  -> Non-1 dims agree on {out_dim} \u2192 result dim = {out_dim}\")\n",
    "            if expanders:\n",
    "                print(f\"     Tensors {expanders} broadcast (their dim 1 expands)\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"  -> \u274c Conflict: multiple distinct non-1 dims {non_ones}. \"\n",
    "                f\"Shapes are NOT broadcastable.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        out.append(out_dim)\n",
    "\n",
    "    out_shape = tuple(out)\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"Broadcasted shape: {out_shape}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Example usage / quick tests\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    B, T, D = 2, 4, 8\n",
    "\n",
    "    # 1) Classic attention-style shapes: (B,T,1) * (B,1,D) * (1,T,D)\n",
    "    shapes1 = [(B, T, 1), (B, 1, D), (1, T, D)]\n",
    "    explain_broadcast_many(shapes1)\n",
    "    print(\"broadcast_shape_many:\", broadcast_shape_many(shapes1))\n",
    "    print()\n",
    "\n",
    "    # 2) (B,T,D) + (D,) + (1,1,D)\n",
    "    # (D,) is the same as\n",
    "    shapes2 = [(B, T, D), (D,), (1, 1, D)]\n",
    "    explain_broadcast_many(shapes2)\n",
    "    print(\"broadcast_shape_many:\", broadcast_shape_many(shapes2))\n",
    "    print()\n",
    "\n",
    "    # 3) Incompatible case: (B,T) and (D,2)\n",
    "    shapes3 = [(B, T), (D, 2)]\n",
    "    try:\n",
    "        explain_broadcast_many(shapes3)\n",
    "        print(\"broadcast_shape_many:\", broadcast_shape_many(shapes3))\n",
    "    except ValueError as e:\n",
    "        print(\"Caught error:\", e)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-wVKIOFL5YD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764530037754,
     "user_tz": 480,
     "elapsed": 14,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "98d43d65-3810-46dc-aaef-cd8257d719f0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input shapes:\n",
      "  Tensor 0: (2, 4, 1)\n",
      "  Tensor 1: (2, 1, 8)\n",
      "  Tensor 2: (1, 4, 8)\n",
      "------------------------------------------------------------\n",
      "Aligned with leading 1s (so all have same rank):\n",
      "  Tensor 0 aligned: (2, 4, 1)\n",
      "  Tensor 1 aligned: (2, 1, 8)\n",
      "  Tensor 2 aligned: (1, 4, 8)\n",
      "------------------------------------------------------------\n",
      "Per-dimension analysis (left \u2192 right):\n",
      "\n",
      "Dim 0 (from left):\n",
      "  - Tensor 0: 2\n",
      "  - Tensor 1: 2\n",
      "  - Tensor 2: 1\n",
      "  -> Non-1 dims agree on 2 \u2192 result dim = 2\n",
      "     Tensors [2] broadcast (their dim 1 expands)\n",
      "\n",
      "Dim 1 (from left):\n",
      "  - Tensor 0: 4\n",
      "  - Tensor 1: 1\n",
      "  - Tensor 2: 4\n",
      "  -> Non-1 dims agree on 4 \u2192 result dim = 4\n",
      "     Tensors [1] broadcast (their dim 1 expands)\n",
      "\n",
      "Dim 2 (from left):\n",
      "  - Tensor 0: 1\n",
      "  - Tensor 1: 8\n",
      "  - Tensor 2: 8\n",
      "  -> Non-1 dims agree on 8 \u2192 result dim = 8\n",
      "     Tensors [0] broadcast (their dim 1 expands)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n",
      "broadcast_shape_many: (2, 4, 8)\n",
      "\n",
      "Input shapes:\n",
      "  Tensor 0: (2, 4, 8)\n",
      "  Tensor 1: (8,)\n",
      "  Tensor 2: (1, 1, 8)\n",
      "------------------------------------------------------------\n",
      "Aligned with leading 1s (so all have same rank):\n",
      "  Tensor 0 aligned: (2, 4, 8)\n",
      "  Tensor 1 aligned: (1, 1, 8)\n",
      "  Tensor 2 aligned: (1, 1, 8)\n",
      "------------------------------------------------------------\n",
      "Per-dimension analysis (left \u2192 right):\n",
      "\n",
      "Dim 0 (from left):\n",
      "  - Tensor 0: 2\n",
      "  - Tensor 1: 1\n",
      "  - Tensor 2: 1\n",
      "  -> Non-1 dims agree on 2 \u2192 result dim = 2\n",
      "     Tensors [1, 2] broadcast (their dim 1 expands)\n",
      "\n",
      "Dim 1 (from left):\n",
      "  - Tensor 0: 4\n",
      "  - Tensor 1: 1\n",
      "  - Tensor 2: 1\n",
      "  -> Non-1 dims agree on 4 \u2192 result dim = 4\n",
      "     Tensors [1, 2] broadcast (their dim 1 expands)\n",
      "\n",
      "Dim 2 (from left):\n",
      "  - Tensor 0: 8\n",
      "  - Tensor 1: 8\n",
      "  - Tensor 2: 8\n",
      "  -> Non-1 dims agree on 8 \u2192 result dim = 8\n",
      "\n",
      "------------------------------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n",
      "broadcast_shape_many: (2, 4, 8)\n",
      "\n",
      "Input shapes:\n",
      "  Tensor 0: (2, 4)\n",
      "  Tensor 1: (8, 2)\n",
      "------------------------------------------------------------\n",
      "Aligned with leading 1s (so all have same rank):\n",
      "  Tensor 0 aligned: (2, 4)\n",
      "  Tensor 1 aligned: (8, 2)\n",
      "------------------------------------------------------------\n",
      "Per-dimension analysis (left \u2192 right):\n",
      "\n",
      "Dim 0 (from left):\n",
      "  - Tensor 0: 2\n",
      "  - Tensor 1: 8\n",
      "  -> \u274c Conflict: multiple distinct non-1 dims [2, 8]. Shapes are NOT broadcastable.\n",
      "Caught error: Incompatible shapes at dim 0: [2, 8] (non-1 dims [2, 8])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "3PGCxw9ufTNk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pytorch tensor practice**\n",
    "\n",
    "[build gpt in code karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
   ],
   "metadata": {
    "id": "ugmoAyqQb-kM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agaA0Q4vPIsY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767139418617,
     "user_tz": 480,
     "elapsed": 681,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "52aee35c-792f-4e38-8bdd-6f5fa5469458"
   },
   "execution_count": 84,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2025-12-31 00:03:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: \u2018input.txt\u2019\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-12-31 00:03:38 (9.34 MB/s) - \u2018input.txt\u2019 saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "  text = f.read()\n",
    "\n",
    "len(text)\n",
    "chars = set(text)\n",
    "len(chars)\n",
    "print(\"\".join(chars))\n",
    "\n",
    "stoi = {char:idx for idx, char in enumerate(chars)}\n",
    "itos = {idx:char for idx, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda s:[stoi[c] for c in s] #encoder\n",
    "decode = lambda l: \"\".join(itos[i] for i in l) #decoder\n",
    "\n",
    "print(encode(\"hi\"))\n",
    "print(decode(encode(\"asdf\")))\n",
    "\n",
    "# GPT2 tokenizer"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHqiSFIut61u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767140283809,
     "user_tz": 480,
     "elapsed": 26,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "b4295a77-8f66-4ebe-95d1-63f331d89345"
   },
   "execution_count": 93,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EuLMOsQftNgD$!:lBXYijFnUe&Ch,VGmK.x;dwWRSqJ3ko-AvZ?Tpz Icr'ay\n",
      "bHP\n",
      "[27, 19]\n",
      "asdf\n"
     ]
    }
   ]
  }
 ]
}