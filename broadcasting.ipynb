{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrRMqw2IpnnnEwyATe0V/t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Broadcasting Simulator**"],"metadata":{"id":"A_XOWlW0SlxZ"}},{"cell_type":"code","source":["import torch\n","from typing import List, Tuple\n","\n","def broadcast_shape(shape_a: Tuple[int, ...],\n","                    shape_b: Tuple[int, ...]) -> Tuple[int, ...]:\n","    \"\"\"\n","    Compute the broadcasted shape of two tensors, or raise ValueError\n","    if they are not broadcastable (NumPy/PyTorch rules).\n","    \"\"\"\n","    # Align from the right by prepending 1s on the shorter shape\n","    a = list(shape_a)\n","    b = list(shape_b)\n","\n","    max_len = max(len(a), len(b))\n","    a = [1] * (max_len - len(a)) + a\n","    b = [1] * (max_len - len(b)) + b\n","\n","    out: List[int] = []\n","    for dim_a, dim_b in zip(a, b):\n","        if dim_a == dim_b or dim_a == 1 or dim_b == 1:\n","            out.append(max(dim_a, dim_b))\n","        else:\n","            raise ValueError(f\"Shapes {shape_a} and {shape_b} are not \"\n","                             f\"broadcastable: conflict {dim_a} vs {dim_b}\")\n","    return tuple(out)\n","\n","\n","def explain_broadcast(shape_a: Tuple[int, ...],\n","                      shape_b: Tuple[int, ...]) -> None:\n","    \"\"\"\n","    Print a step-by-step explanation of broadcasting between two shapes.\n","    \"\"\"\n","    print(f\"A shape: {shape_a}\")\n","    print(f\"B shape: {shape_b}\")\n","    print(\"-\" * 40)\n","\n","    # Align with leading 1s\n","    a = list(shape_a)\n","    b = list(shape_b)\n","    max_len = max(len(a), len(b))\n","    a = [1] * (max_len - len(a)) + a\n","    b = [1] * (max_len - len(b)) + b\n","\n","    print(\"Aligned (with leading 1s):\")\n","    print(f\"A aligned: {tuple(a)}\")\n","    print(f\"B aligned: {tuple(b)}\")\n","    print(\"-\" * 40)\n","\n","    out: List[int] = []\n","    print(\"Compare dimensions (left → right):\")\n","    for i, (dim_a, dim_b) in enumerate(zip(a, b)):\n","        pos = i - max_len  # relative from the right: -1 is last dim\n","        msg = f\"  dim {i} (from left, pos {pos:+} from right): {dim_a} vs {dim_b} -> \"\n","        if dim_a == dim_b:\n","            out_dim = dim_a\n","            reason = \"same, keep\"\n","        elif dim_a == 1:\n","            out_dim = dim_b\n","            reason = \"A expands to match B\"\n","        elif dim_b == 1:\n","            out_dim = dim_a\n","            reason = \"B expands to match A\"\n","        else:\n","            print(msg + \"❌ conflict (not broadcastable)\")\n","            print()\n","            print(f\"Result: shapes {shape_a} and {shape_b} are NOT broadcastable.\")\n","            return\n","        out.append(out_dim)\n","        print(msg + f\"✔ {out_dim} ({reason})\")\n","\n","    out_shape = tuple(out)\n","    print(\"-\" * 40)\n","    print(f\"Broadcasted shape: {out_shape}\")\n","    print()\n","\n","\n","# --------- quick tests / examples ---------\n","if __name__ == \"__main__\":\n","    # 1) Your classic (B,T,1) × (B,1,D)\n","    B, T, D = 2, 4, 8\n","    explain_broadcast((B, T, 1), (B, 1, D))\n","\n","    # 2) (B,T,D) + (D,)\n","    explain_broadcast((B, T, D), (D,))\n","\n","    # 3) (B,T) × (D,) – usually invalid unless T == D\n","    try:\n","        explain_broadcast((B, T), (D,))\n","    except Exception as e:\n","        print(\"Caught error:\", e)\n","\n","    # 4) Quick programmatic use:\n","    print(\"broadcast_shape((3, 1), (1, 4)) ->\",\n","          broadcast_shape((3, 1), (1, 4)))\n"],"metadata":{"id":"BBwPGRCcE73k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764529894656,"user_tz":480,"elapsed":14,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"c9f3de19-76f2-4ded-b3a7-59da0b74b3b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A shape: (2, 4, 1)\n","B shape: (2, 1, 8)\n","----------------------------------------\n","Aligned (with leading 1s):\n","A aligned: (2, 4, 1)\n","B aligned: (2, 1, 8)\n","----------------------------------------\n","Compare dimensions (left → right):\n","  dim 0 (from left, pos -3 from right): 2 vs 2 -> ✔ 2 (same, keep)\n","  dim 1 (from left, pos -2 from right): 4 vs 1 -> ✔ 4 (B expands to match A)\n","  dim 2 (from left, pos -1 from right): 1 vs 8 -> ✔ 8 (A expands to match B)\n","----------------------------------------\n","Broadcasted shape: (2, 4, 8)\n","\n","A shape: (2, 4, 8)\n","B shape: (8,)\n","----------------------------------------\n","Aligned (with leading 1s):\n","A aligned: (2, 4, 8)\n","B aligned: (1, 1, 8)\n","----------------------------------------\n","Compare dimensions (left → right):\n","  dim 0 (from left, pos -3 from right): 2 vs 1 -> ✔ 2 (B expands to match A)\n","  dim 1 (from left, pos -2 from right): 4 vs 1 -> ✔ 4 (B expands to match A)\n","  dim 2 (from left, pos -1 from right): 8 vs 8 -> ✔ 8 (same, keep)\n","----------------------------------------\n","Broadcasted shape: (2, 4, 8)\n","\n","A shape: (2, 4)\n","B shape: (8,)\n","----------------------------------------\n","Aligned (with leading 1s):\n","A aligned: (2, 4)\n","B aligned: (1, 8)\n","----------------------------------------\n","Compare dimensions (left → right):\n","  dim 0 (from left, pos -2 from right): 2 vs 1 -> ✔ 2 (B expands to match A)\n","  dim 1 (from left, pos -1 from right): 4 vs 8 -> ❌ conflict (not broadcastable)\n","\n","Result: shapes (2, 4) and (8,) are NOT broadcastable.\n","broadcast_shape((3, 1), (1, 4)) -> (3, 4)\n"]}]},{"cell_type":"code","source":["\n","B, T, D = 2, 4, 8\n","explain_broadcast((B, T, 1), (B, 1, D))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ijNHOiBLXYA","executionInfo":{"status":"ok","timestamp":1764529961278,"user_tz":480,"elapsed":9,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"54237c22-0c86-4914-a929-7c56b5ecd489"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A shape: (2, 4, 1)\n","B shape: (2, 1, 8)\n","----------------------------------------\n","Aligned (with leading 1s):\n","A aligned: (2, 4, 1)\n","B aligned: (2, 1, 8)\n","----------------------------------------\n","Compare dimensions (left → right):\n","  dim 0 (from left, pos -3 from right): 2 vs 2 -> ✔ 2 (same, keep)\n","  dim 1 (from left, pos -2 from right): 4 vs 1 -> ✔ 4 (B expands to match A)\n","  dim 2 (from left, pos -1 from right): 1 vs 8 -> ✔ 8 (A expands to match B)\n","----------------------------------------\n","Broadcasted shape: (2, 4, 8)\n","\n"]}]},{"cell_type":"code","source":["import torch\n","from typing import List, Tuple, Sequence\n","\n","Shape = Tuple[int, ...]\n","\n","\n","def broadcast_shape_many(shapes: Sequence[Shape]) -> Shape:\n","    \"\"\"\n","    Compute the broadcasted shape of N tensors (NumPy/PyTorch rules).\n","\n","    shapes: iterable of shapes, e.g. [(B,T,1), (B,1,D), (1,T,1)]\n","    returns: single broadcasted shape, or raises ValueError if incompatible.\n","    \"\"\"\n","    if not shapes:\n","        return ()\n","\n","    # Step 1: compute max rank and left-pad each shape with 1s\n","    max_len = max(len(s) for s in shapes)\n","    aligned: List[List[int]] = []\n","    for s in shapes:\n","        padded = [1] * (max_len - len(s)) + list(s)\n","        aligned.append(padded)\n","\n","    out: List[int] = []\n","    # Step 2: for each dimension (left → right, but rules are symmetric)\n","    for dim_idx in range(max_len):\n","        dims_here = [a[dim_idx] for a in aligned]\n","        non_ones = sorted({d for d in dims_here if d != 1})\n","\n","        if len(non_ones) == 0:\n","            # all 1s → result is 1\n","            out_dim = 1\n","        elif len(non_ones) == 1:\n","            # all non-1 dims agree → that is the result dim\n","            out_dim = non_ones[0]\n","        else:\n","            # more than one distinct non-1 size → conflict\n","            raise ValueError(\n","                f\"Incompatible shapes at dim {dim_idx}: {dims_here} \"\n","                f\"(non-1 dims {non_ones})\"\n","            )\n","        out.append(out_dim)\n","\n","    return tuple(out)\n","\n","\n","def explain_broadcast_many(shapes: Sequence[Shape]) -> None:\n","    \"\"\"\n","    Verbose explanation of N-way broadcasting.\n","    \"\"\"\n","    print(\"Input shapes:\")\n","    for i, s in enumerate(shapes):\n","        print(f\"  Tensor {i}: {s}\")\n","    print(\"-\" * 60)\n","\n","    if not shapes:\n","        print(\"No shapes given → result shape is ().\")\n","        return\n","\n","    max_len = max(len(s) for s in shapes)\n","    aligned: List[List[int]] = []\n","    for s in shapes:\n","        padded = [1] * (max_len - len(s)) + list(s)\n","        aligned.append(padded)\n","\n","    print(\"Aligned with leading 1s (so all have same rank):\")\n","    for i, a in enumerate(aligned):\n","        print(f\"  Tensor {i} aligned: {tuple(a)}\")\n","    print(\"-\" * 60)\n","\n","    out: List[int] = []\n","    print(\"Per-dimension analysis (left → right):\")\n","    for dim_idx in range(max_len):\n","        dims_here = [a[dim_idx] for a in aligned]\n","        non_ones = sorted({d for d in dims_here if d != 1})\n","\n","        print(f\"\\nDim {dim_idx} (from left):\")\n","        for i, d in enumerate(dims_here):\n","            print(f\"  - Tensor {i}: {d}\")\n","\n","        if len(non_ones) == 0:\n","            out_dim = 1\n","            print(\"  -> All dims are 1 → result dim = 1\")\n","        elif len(non_ones) == 1:\n","            out_dim = non_ones[0]\n","            expanders = [\n","                i for i, d in enumerate(dims_here) if d == 1 and out_dim != 1\n","            ]\n","            print(f\"  -> Non-1 dims agree on {out_dim} → result dim = {out_dim}\")\n","            if expanders:\n","                print(f\"     Tensors {expanders} broadcast (their dim 1 expands)\")\n","        else:\n","            print(\n","                f\"  -> ❌ Conflict: multiple distinct non-1 dims {non_ones}. \"\n","                f\"Shapes are NOT broadcastable.\"\n","            )\n","            return\n","\n","        out.append(out_dim)\n","\n","    out_shape = tuple(out)\n","    print(\"\\n\" + \"-\" * 60)\n","    print(f\"Broadcasted shape: {out_shape}\")\n","    print()\n","\n","\n","# --------------------------\n","# Example usage / quick tests\n","# --------------------------\n","if __name__ == \"__main__\":\n","    B, T, D = 2, 4, 8\n","\n","    # 1) Classic attention-style shapes: (B,T,1) * (B,1,D) * (1,T,D)\n","    shapes1 = [(B, T, 1), (B, 1, D), (1, T, D)]\n","    explain_broadcast_many(shapes1)\n","    print(\"broadcast_shape_many:\", broadcast_shape_many(shapes1))\n","    print()\n","\n","    # 2) (B,T,D) + (D,) + (1,1,D)\n","    # (D,) is the same as\n","    shapes2 = [(B, T, D), (D,), (1, 1, D)]\n","    explain_broadcast_many(shapes2)\n","    print(\"broadcast_shape_many:\", broadcast_shape_many(shapes2))\n","    print()\n","\n","    # 3) Incompatible case: (B,T) and (D,2)\n","    shapes3 = [(B, T), (D, 2)]\n","    try:\n","        explain_broadcast_many(shapes3)\n","        print(\"broadcast_shape_many:\", broadcast_shape_many(shapes3))\n","    except ValueError as e:\n","        print(\"Caught error:\", e)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-wVKIOFL5YD","executionInfo":{"status":"ok","timestamp":1764530037754,"user_tz":480,"elapsed":14,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"98d43d65-3810-46dc-aaef-cd8257d719f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shapes:\n","  Tensor 0: (2, 4, 1)\n","  Tensor 1: (2, 1, 8)\n","  Tensor 2: (1, 4, 8)\n","------------------------------------------------------------\n","Aligned with leading 1s (so all have same rank):\n","  Tensor 0 aligned: (2, 4, 1)\n","  Tensor 1 aligned: (2, 1, 8)\n","  Tensor 2 aligned: (1, 4, 8)\n","------------------------------------------------------------\n","Per-dimension analysis (left → right):\n","\n","Dim 0 (from left):\n","  - Tensor 0: 2\n","  - Tensor 1: 2\n","  - Tensor 2: 1\n","  -> Non-1 dims agree on 2 → result dim = 2\n","     Tensors [2] broadcast (their dim 1 expands)\n","\n","Dim 1 (from left):\n","  - Tensor 0: 4\n","  - Tensor 1: 1\n","  - Tensor 2: 4\n","  -> Non-1 dims agree on 4 → result dim = 4\n","     Tensors [1] broadcast (their dim 1 expands)\n","\n","Dim 2 (from left):\n","  - Tensor 0: 1\n","  - Tensor 1: 8\n","  - Tensor 2: 8\n","  -> Non-1 dims agree on 8 → result dim = 8\n","     Tensors [0] broadcast (their dim 1 expands)\n","\n","------------------------------------------------------------\n","Broadcasted shape: (2, 4, 8)\n","\n","broadcast_shape_many: (2, 4, 8)\n","\n","Input shapes:\n","  Tensor 0: (2, 4, 8)\n","  Tensor 1: (8,)\n","  Tensor 2: (1, 1, 8)\n","------------------------------------------------------------\n","Aligned with leading 1s (so all have same rank):\n","  Tensor 0 aligned: (2, 4, 8)\n","  Tensor 1 aligned: (1, 1, 8)\n","  Tensor 2 aligned: (1, 1, 8)\n","------------------------------------------------------------\n","Per-dimension analysis (left → right):\n","\n","Dim 0 (from left):\n","  - Tensor 0: 2\n","  - Tensor 1: 1\n","  - Tensor 2: 1\n","  -> Non-1 dims agree on 2 → result dim = 2\n","     Tensors [1, 2] broadcast (their dim 1 expands)\n","\n","Dim 1 (from left):\n","  - Tensor 0: 4\n","  - Tensor 1: 1\n","  - Tensor 2: 1\n","  -> Non-1 dims agree on 4 → result dim = 4\n","     Tensors [1, 2] broadcast (their dim 1 expands)\n","\n","Dim 2 (from left):\n","  - Tensor 0: 8\n","  - Tensor 1: 8\n","  - Tensor 2: 8\n","  -> Non-1 dims agree on 8 → result dim = 8\n","\n","------------------------------------------------------------\n","Broadcasted shape: (2, 4, 8)\n","\n","broadcast_shape_many: (2, 4, 8)\n","\n","Input shapes:\n","  Tensor 0: (2, 4)\n","  Tensor 1: (8, 2)\n","------------------------------------------------------------\n","Aligned with leading 1s (so all have same rank):\n","  Tensor 0 aligned: (2, 4)\n","  Tensor 1 aligned: (8, 2)\n","------------------------------------------------------------\n","Per-dimension analysis (left → right):\n","\n","Dim 0 (from left):\n","  - Tensor 0: 2\n","  - Tensor 1: 8\n","  -> ❌ Conflict: multiple distinct non-1 dims [2, 8]. Shapes are NOT broadcastable.\n","Caught error: Incompatible shapes at dim 0: [2, 8] (non-1 dims [2, 8])\n"]}]},{"cell_type":"markdown","source":["# Computing mean\n","\n","\n","https://www.youtube.com/watch?v=kCc8FmEb1nY\n","Karpathy Trick behind attention\n","\n","44:00\n","\n","\n","<ul>\n","<li>Summary: A mean reduces the dims of a shape like [10,10] to [10] because one of the dims becomes the mean. Use keepdims=True to get [1,10] instead of [10]. This helps to make clear it is ready for broadcasting</li>\n","<li>create data pattern for debugging. Show the batch, time, Column dims</li>\n","<li>We are calculating the mean per vertical column. 4,8,2 has 2 colmns so the mean is [mean col0, mean col1]</li>\n","<li>Broadcasting requires a 1 as one of the dimensions or pytorch uses the shift trick to add a 1 to the arg with the smaller number of dims.  </li>\n","<li>Pytorch first aligns left then adds ones until num dims match the other arg. [4,8,2] amnd [2] become [4,8,2] and [1,1,2].</li>\n","<li>We use the phrase left aligned but pytorch calls it right aligned</li>\n","<li>[4,8,2]</li>\n","<li>[6]</li>\n","<li>Unsqueeze(1) adds a 1 to the front of the shape tuple,  (6)->(1,6) and unsqueeze(-1) adds a 1 to the end of the shape vector (1,6)->(1,6,1). A unsqueeze(2) adds to the 2 index, (1,6,1)->(1,6,1,1)</li>\n","<li>the 6 is in the same column as the 4. I call this left aligned but pytorch calls it right aligned.</li>\n","<li>Some operations like mean, sum eliminate one of the dimensions, to keep this dim=1, use keepdims=True</li>\n","<ul>"],"metadata":{"id":"v8oNMPk8Hm-t"}},{"cell_type":"code","source":["import torch\n","x = torch.tensor([6])\n","print(x.shape)\n","x=x.unsqueeze(1)\n","print(x.shape)\n","x = x.unsqueeze(-1)\n","print(x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZ6bY9CADq2f","executionInfo":{"status":"ok","timestamp":1764614186523,"user_tz":480,"elapsed":16,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"19f71328-3d46-476d-d5be-ec2128f12a47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1])\n","torch.Size([1, 1])\n","torch.Size([1, 1, 1])\n"]}]},{"cell_type":"code","source":["import torch\n","x = torch.tensor([1,2,3,4,5,6], dtype=torch.float16)\n","last_5=x[0:5] #this is not really right should index fro 6\n","print('-------------')\n","print('reverse indexing')\n","print(x[-1:])\n","print(x[-2:])\n","print(x[-3:])\n","print('-------------')\n","print('foward indexing')\n","print(x[:(0+1)])\n","print(x[:(1+1):])\n","print(x[:(2+1):])\n","\n","\n","#print(last_5)\n","#torch.mean(last_5)\n","print('------------')\n","print(\"easier to do forward indexing since we tokenize from l->r\")\n","print(\"the below is incorrect because of the first empty array\")\n","for idx in range(len(x)):\n","  print(\"start from index:\",idx,\" previous tokens:\", x[:idx], \"mean:\",torch.mean(x[:idx]))\n","\n","print(\"\\n\")\n","print(\"\\n\")\n","print(\"since we index from 0, we need to start from 1\")\n","\n","for idx in range(len(x)):\n","  print(\"start from index+1:\",idx+1,\" previous tokens:\", x[:idx+1], \"mean:\",torch.mean(x[:idx+1]))\n","\n","#convert to tensor x(B,T,C) convention\n","B,T,C = 4,8,2\n","\n","\n","B, T, C = 4, 8, 2\n","x = torch.zeros((B, T, C))\n","\n","for b in range(B):\n","    for t in range(T):\n","        x[b, t, 0] = b     # channel 0 shows batch index\n","        x[b, t, 1] = t     # channel 1 shows timestep index\n","\n","print('----------')\n","print(x)\n","print('----------')\n","\n","for b in range(B):\n","  for t in range(T):\n","    xprev = x[b,:t+1] #(t,C)\n","    xbow = torch.mean(xprev,0) #the 0 means 0 dimension, which is t because (t,C)\n","    print(f'b:{b},t:{t},xprev:{xprev}, xbow:{xbow}')\n","\n","#c isnt incremented [b,t] so we are stepping through rows [0,0],[0,1],[0,2],[0,3],,...\n","# xprev is the previous row."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UOP5ppaQwON","executionInfo":{"status":"ok","timestamp":1764614186742,"user_tz":480,"elapsed":199,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"aa8f2c11-bebc-490b-b750-dc3506f299c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------\n","reverse indexing\n","tensor([6.], dtype=torch.float16)\n","tensor([5., 6.], dtype=torch.float16)\n","tensor([4., 5., 6.], dtype=torch.float16)\n","-------------\n","foward indexing\n","tensor([1.], dtype=torch.float16)\n","tensor([1., 2.], dtype=torch.float16)\n","tensor([1., 2., 3.], dtype=torch.float16)\n","------------\n","easier to do forward indexing since we tokenize from l->r\n","the below is incorrect because of the first empty array\n","start from index: 0  previous tokens: tensor([], dtype=torch.float16) mean: tensor(nan, dtype=torch.float16)\n","start from index: 1  previous tokens: tensor([1.], dtype=torch.float16) mean: tensor(1., dtype=torch.float16)\n","start from index: 2  previous tokens: tensor([1., 2.], dtype=torch.float16) mean: tensor(1.5000, dtype=torch.float16)\n","start from index: 3  previous tokens: tensor([1., 2., 3.], dtype=torch.float16) mean: tensor(2., dtype=torch.float16)\n","start from index: 4  previous tokens: tensor([1., 2., 3., 4.], dtype=torch.float16) mean: tensor(2.5000, dtype=torch.float16)\n","start from index: 5  previous tokens: tensor([1., 2., 3., 4., 5.], dtype=torch.float16) mean: tensor(3., dtype=torch.float16)\n","\n","\n","\n","\n","since we index from 0, we need to start from 1\n","start from index+1: 1  previous tokens: tensor([1.], dtype=torch.float16) mean: tensor(1., dtype=torch.float16)\n","start from index+1: 2  previous tokens: tensor([1., 2.], dtype=torch.float16) mean: tensor(1.5000, dtype=torch.float16)\n","start from index+1: 3  previous tokens: tensor([1., 2., 3.], dtype=torch.float16) mean: tensor(2., dtype=torch.float16)\n","start from index+1: 4  previous tokens: tensor([1., 2., 3., 4.], dtype=torch.float16) mean: tensor(2.5000, dtype=torch.float16)\n","start from index+1: 5  previous tokens: tensor([1., 2., 3., 4., 5.], dtype=torch.float16) mean: tensor(3., dtype=torch.float16)\n","start from index+1: 6  previous tokens: tensor([1., 2., 3., 4., 5., 6.], dtype=torch.float16) mean: tensor(3.5000, dtype=torch.float16)\n","----------\n","tensor([[[0., 0.],\n","         [0., 1.],\n","         [0., 2.],\n","         [0., 3.],\n","         [0., 4.],\n","         [0., 5.],\n","         [0., 6.],\n","         [0., 7.]],\n","\n","        [[1., 0.],\n","         [1., 1.],\n","         [1., 2.],\n","         [1., 3.],\n","         [1., 4.],\n","         [1., 5.],\n","         [1., 6.],\n","         [1., 7.]],\n","\n","        [[2., 0.],\n","         [2., 1.],\n","         [2., 2.],\n","         [2., 3.],\n","         [2., 4.],\n","         [2., 5.],\n","         [2., 6.],\n","         [2., 7.]],\n","\n","        [[3., 0.],\n","         [3., 1.],\n","         [3., 2.],\n","         [3., 3.],\n","         [3., 4.],\n","         [3., 5.],\n","         [3., 6.],\n","         [3., 7.]]])\n","----------\n","b:0,t:0,xprev:tensor([[0., 0.]]), xbow:tensor([0., 0.])\n","b:0,t:1,xprev:tensor([[0., 0.],\n","        [0., 1.]]), xbow:tensor([0.0000, 0.5000])\n","b:0,t:2,xprev:tensor([[0., 0.],\n","        [0., 1.],\n","        [0., 2.]]), xbow:tensor([0., 1.])\n","b:0,t:3,xprev:tensor([[0., 0.],\n","        [0., 1.],\n","        [0., 2.],\n","        [0., 3.]]), xbow:tensor([0.0000, 1.5000])\n","b:0,t:4,xprev:tensor([[0., 0.],\n","        [0., 1.],\n","        [0., 2.],\n","        [0., 3.],\n","        [0., 4.]]), xbow:tensor([0., 2.])\n","b:0,t:5,xprev:tensor([[0., 0.],\n","        [0., 1.],\n","        [0., 2.],\n","        [0., 3.],\n","        [0., 4.],\n","        [0., 5.]]), xbow:tensor([0.0000, 2.5000])\n","b:0,t:6,xprev:tensor([[0., 0.],\n","        [0., 1.],\n","        [0., 2.],\n","        [0., 3.],\n","        [0., 4.],\n","        [0., 5.],\n","        [0., 6.]]), xbow:tensor([0., 3.])\n","b:0,t:7,xprev:tensor([[0., 0.],\n","        [0., 1.],\n","        [0., 2.],\n","        [0., 3.],\n","        [0., 4.],\n","        [0., 5.],\n","        [0., 6.],\n","        [0., 7.]]), xbow:tensor([0.0000, 3.5000])\n","b:1,t:0,xprev:tensor([[1., 0.]]), xbow:tensor([1., 0.])\n","b:1,t:1,xprev:tensor([[1., 0.],\n","        [1., 1.]]), xbow:tensor([1.0000, 0.5000])\n","b:1,t:2,xprev:tensor([[1., 0.],\n","        [1., 1.],\n","        [1., 2.]]), xbow:tensor([1., 1.])\n","b:1,t:3,xprev:tensor([[1., 0.],\n","        [1., 1.],\n","        [1., 2.],\n","        [1., 3.]]), xbow:tensor([1.0000, 1.5000])\n","b:1,t:4,xprev:tensor([[1., 0.],\n","        [1., 1.],\n","        [1., 2.],\n","        [1., 3.],\n","        [1., 4.]]), xbow:tensor([1., 2.])\n","b:1,t:5,xprev:tensor([[1., 0.],\n","        [1., 1.],\n","        [1., 2.],\n","        [1., 3.],\n","        [1., 4.],\n","        [1., 5.]]), xbow:tensor([1.0000, 2.5000])\n","b:1,t:6,xprev:tensor([[1., 0.],\n","        [1., 1.],\n","        [1., 2.],\n","        [1., 3.],\n","        [1., 4.],\n","        [1., 5.],\n","        [1., 6.]]), xbow:tensor([1., 3.])\n","b:1,t:7,xprev:tensor([[1., 0.],\n","        [1., 1.],\n","        [1., 2.],\n","        [1., 3.],\n","        [1., 4.],\n","        [1., 5.],\n","        [1., 6.],\n","        [1., 7.]]), xbow:tensor([1.0000, 3.5000])\n","b:2,t:0,xprev:tensor([[2., 0.]]), xbow:tensor([2., 0.])\n","b:2,t:1,xprev:tensor([[2., 0.],\n","        [2., 1.]]), xbow:tensor([2.0000, 0.5000])\n","b:2,t:2,xprev:tensor([[2., 0.],\n","        [2., 1.],\n","        [2., 2.]]), xbow:tensor([2., 1.])\n","b:2,t:3,xprev:tensor([[2., 0.],\n","        [2., 1.],\n","        [2., 2.],\n","        [2., 3.]]), xbow:tensor([2.0000, 1.5000])\n","b:2,t:4,xprev:tensor([[2., 0.],\n","        [2., 1.],\n","        [2., 2.],\n","        [2., 3.],\n","        [2., 4.]]), xbow:tensor([2., 2.])\n","b:2,t:5,xprev:tensor([[2., 0.],\n","        [2., 1.],\n","        [2., 2.],\n","        [2., 3.],\n","        [2., 4.],\n","        [2., 5.]]), xbow:tensor([2.0000, 2.5000])\n","b:2,t:6,xprev:tensor([[2., 0.],\n","        [2., 1.],\n","        [2., 2.],\n","        [2., 3.],\n","        [2., 4.],\n","        [2., 5.],\n","        [2., 6.]]), xbow:tensor([2., 3.])\n","b:2,t:7,xprev:tensor([[2., 0.],\n","        [2., 1.],\n","        [2., 2.],\n","        [2., 3.],\n","        [2., 4.],\n","        [2., 5.],\n","        [2., 6.],\n","        [2., 7.]]), xbow:tensor([2.0000, 3.5000])\n","b:3,t:0,xprev:tensor([[3., 0.]]), xbow:tensor([3., 0.])\n","b:3,t:1,xprev:tensor([[3., 0.],\n","        [3., 1.]]), xbow:tensor([3.0000, 0.5000])\n","b:3,t:2,xprev:tensor([[3., 0.],\n","        [3., 1.],\n","        [3., 2.]]), xbow:tensor([3., 1.])\n","b:3,t:3,xprev:tensor([[3., 0.],\n","        [3., 1.],\n","        [3., 2.],\n","        [3., 3.]]), xbow:tensor([3.0000, 1.5000])\n","b:3,t:4,xprev:tensor([[3., 0.],\n","        [3., 1.],\n","        [3., 2.],\n","        [3., 3.],\n","        [3., 4.]]), xbow:tensor([3., 2.])\n","b:3,t:5,xprev:tensor([[3., 0.],\n","        [3., 1.],\n","        [3., 2.],\n","        [3., 3.],\n","        [3., 4.],\n","        [3., 5.]]), xbow:tensor([3.0000, 2.5000])\n","b:3,t:6,xprev:tensor([[3., 0.],\n","        [3., 1.],\n","        [3., 2.],\n","        [3., 3.],\n","        [3., 4.],\n","        [3., 5.],\n","        [3., 6.]]), xbow:tensor([3., 3.])\n","b:3,t:7,xprev:tensor([[3., 0.],\n","        [3., 1.],\n","        [3., 2.],\n","        [3., 3.],\n","        [3., 4.],\n","        [3., 5.],\n","        [3., 6.],\n","        [3., 7.]]), xbow:tensor([3.0000, 3.5000])\n"]}]},{"cell_type":"markdown","source":["Attention definitions\n","\n","<ul>\n","<li>\n","given an embedding , 768 how does this become B,T,C, is B a collection of 768 vectorx, T is the row, C are the columns so C = 768. We have to create B,T\n","</li>\n","<li>\n","T = sequence length. and B is number of sequences processed in GPU memory at one time.\n","</li>\n","<li>\n","\n","attention splits C to num_heads.\n","</li>\n","<li>\n","num_heads is a predefined constant\n","num_heads=H;\n","number of attention heads is C/H. this isnt the same as num_heads\n","num_attention_heads = C/H\n","</li>\n","<li>\n","head_dim = C/H, 768/16\n","</li>\n","<li>\n","K_cache: (B, T, H, head_dim)\n","V_cache: (B, T, H, head_dim)\n","</li>\n","\n","<ul>"],"metadata":{"id":"OTmn1dQpjS6v"}},{"cell_type":"code","source":[],"metadata":{"id":"CpB_RdIT8pe_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Matrix Multiplies\n","\n","<ul>\n","<li>Matrix multiply by identity matrix proudces sums and averages. [2,7], [6,4], [6,5] produce column sums by matrix multiply with identity matrix</li>\n","</ul>"],"metadata":{"id":"yPPLS_RN8qNp"}},{"cell_type":"code","source":["# karpathy replicating medians in (B,T,C) format with triangular matrix multiply\n","import torch\n","torch.manual_seed(42)\n","a = torch.ones(3,3)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('----')\n","print('b=')\n","print(b)\n","print('----')\n","print('c=')\n","print(c)\n","print('---end multiply w identiy matrix---')\n","\n","# can see matrix multiply with Identiy matrix and data produces sums in the columns. Column sums 2+6+6=16, 7+4+5=16\n","# the matrix multiply is a sum when we take the dot product. First row [1,1,1] * first col [2,6,6] gives sum 2+6+6=14,\n","\n","# second step take the lower triangular matrix instead if Identity matrix. This adds 0s\n","print('---replace with lower triangular and make rows sum to 1---')\n","print('  ')\n","a = torch.tril(torch.ones(3,3))/torch.sum(a,1,keepdim=True)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('----')\n","print('b=')\n","print(b)\n","print('----')\n","print('c=')\n","print(c)\n","print('---end multiply w lower triagular rows normalzied to sum 1---')\n"],"metadata":{"id":"GipzwfmKhI-L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764651093562,"user_tz":480,"elapsed":9,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"81f0709d-0862-4683-9891-d5e5e96b3d48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","----\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","----\n","c=\n","tensor([[14., 16.],\n","        [14., 16.],\n","        [14., 16.]])\n","---end multiply w identiy matrix---\n","---replace with lower triangular and make rows sum to 1---\n","  \n","a=\n","tensor([[0.3333, 0.0000, 0.0000],\n","        [0.3333, 0.3333, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","----\n","b=\n","tensor([[0., 4.],\n","        [0., 3.],\n","        [8., 4.]])\n","----\n","c=\n","tensor([[0.0000, 1.3333],\n","        [0.0000, 2.3333],\n","        [2.6667, 3.6667]])\n","---end multiply w lower triagular rows normalzied to sum 1---\n"]}]},{"cell_type":"code","source":["x=torch.tensor([[1,2,3],[4,5,6]])\n","print(x, x.shape)\n","#y = x/torch.sum(x,dim=1)\n","print(torch.sum(x, dim=0, keepdim=True))\n","print(torch.sum(x, dim=0, keepdim=False))\n","print(torch.sum(x, dim=1, keepdim=True))\n","print(torch.sum(x, dim=1, keepdim=False))\n","#print(s)\n","print(torch.sum(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucxpn5RQZ6nM","executionInfo":{"status":"ok","timestamp":1764652076726,"user_tz":480,"elapsed":7,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"6de78ca2-5167-4744-c266-39feeb434cb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2, 3],\n","        [4, 5, 6]]) torch.Size([2, 3])\n","tensor([[5, 7, 9]])\n","tensor([5, 7, 9])\n","tensor([[ 6],\n","        [15]])\n","tensor([ 6, 15])\n","tensor([[3],\n","        [7]])\n","tensor(21)\n"]}]},{"cell_type":"markdown","source":["# **Matrix and Vector multiply for averages.**\n","\n","There are different versions of averages which are used for normalization. Karpathy develops the t-1 or the autoregresseive average where the average of the sequence of a,b,c,d has 4 averages; at t=0, avg=a, t=1 avg=(a+b)/2, t=2 avg=\n","(a+b+c)/3, etc...\n","\n","\n","<ul>\n","<li>$v = \\frac{1}{N}[1,1,1,...]$ where num ones = N</li>\n","<li>A cumulative average is the conventional average $\\frac{1}{N}\\sum_0^{N-1}x_i$. It is time invariant. Shifting the sequence produces the same average. $v=[1,1,1...len(x)]$ and the data is $x$. cumulative avg = $v^T@x$</li>\n","<li>A column average $v^T@X$</li>\n","<li>How to derive row and column avg. X=(B,D). Create a 2x3 test matrix\n","[[1,2,3],[4,5,6]]. Make sure the TM is not symmetric to reduce confusion. We have to options an identity matrix 1x2 if I@X or 3x1 if X@I. There are 2 rows so a row sum must have 2 rows so you know 1x2 is rows and there are 3 cols and you know you need 3 columns for a column sum. so I@X is row sum and X@I is column sum. Then add 1/N to get avg. N=num elements in row or col.\n","</li>\n","<li>A row average $X@v$</li>\n","<li>Weighted avg for softmax. W=(T), V=(T,D). $avg=W^T@V$ Output = (D,) or (D,1) if keepdims=True. Because avg collapases and removes dimensions by default</li>\n","<li>How to derive Weighted Softmax Avg</li>\n","<li>Sequence avg: <li>\n","<li>How to derive sequence avg. </li>\n","</ul>"],"metadata":{"id":"2VKZ-B-GGCGX"}},{"cell_type":"code","source":["import torch\n","\n","x = torch.tensor([1,2,3,4,5,6]).float()\n","\n","v = (1/6)*(torch.ones(6))\n","print(\"avg of sum of all elements v@x:\",v@x)\n","tri = torch.tril(torch.ones(6,6))\n","avg = (tri@x)/torch.arange(1,7)\n","\n","print(\"sums:\",tri@x)\n","print(\"torch arange:\",torch.arange(1,7))\n","print(\"rolling avg:\",avg) # 1/1, (1+2)/2, (1+2+3)/3,..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zRqW-1qzYSoh","executionInfo":{"status":"ok","timestamp":1764641233838,"user_tz":480,"elapsed":45,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"9c5e9486-b76b-4041-d6f7-5677cd7643df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["avg of sum of all elements v@x: tensor(3.5000)\n","sums: tensor([ 1.,  3.,  6., 10., 15., 21.])\n","torch arange: tensor([1, 2, 3, 4, 5, 6])\n","avg: tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000])\n"]}]},{"cell_type":"code","source":["import torch\n","\n","x = torch.tensor([1,2,3,4,5])\n","print(f'len(x):{len(x)}')\n","v = torch.ones(len(x))\n","lower_tri = torch.ones(len(x),len(x))\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TlgMKsHJGPhB","executionInfo":{"status":"ok","timestamp":1764629315963,"user_tz":480,"elapsed":7,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"97527ecb-eafb-4535-e3bc-81e32b315619"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len(x):5\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from typing import Optional, Union\n","\n","# avoid division by 0\n","def safe_torch_mean(x: torch.Tensor) -> torch.Tensor:\n","    return x.float().sum() / max(x.numel(), 1)\n","\n","def safe_mean(\n","    x: torch.Tensor,\n","    dim: Optional[int] = None,\n","    keepdim: bool = False,\n","    default: Union[float, int] = 0.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Mean that:\n","      - casts non-floating dtypes to float32\n","      - returns `default` when there are no elements along `dim`\n","    \"\"\"\n","    if not x.is_floating_point():\n","        x = x.to(torch.float32)\n","\n","    if dim is None:\n","        if x.numel() == 0:\n","            # scalar default\n","            return x.new_tensor(float(default))\n","        return x.mean()\n","\n","    # Mean along a dimension\n","    if x.size(dim) == 0:\n","        # build output shape manually\n","        out_shape = list(x.shape)\n","        if keepdim:\n","            out_shape[dim] = 1\n","        else:\n","            del out_shape[dim]\n","        return x.new_full(out_shape, float(default))\n","\n","    return x.mean(dim=dim, keepdim=keepdim)\n","\n","x = torch.tensor([], dtype=torch.float32)\n","print(safe_mean(x))  # tensor(0.)\n","\n","x = torch.randint(0, 10, (3,), dtype=torch.int8)\n","print(safe_mean(x))  # float32 mean, no error\n","\n","\n","def masked_mean(\n","    x: torch.Tensor,\n","    mask: torch.Tensor,\n","    dim: int,\n","    keepdim: bool = False,\n","    default: Union[float, int] = 0.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Mean over elements where mask == 1/True along `dim`.\n","\n","    x: (..., D, ...)\n","    mask: same shape as x or broadcastable to x\n","    \"\"\"\n","    if not x.is_floating_point():\n","        x = x.to(torch.float32)\n","\n","    # make mask float for multiplication\n","    m = mask.to(x.dtype)\n","    # broadcast OK: this relies on PyTorch broadcasting\n","    masked_x = x * m\n","\n","    # sum over dim\n","    num = masked_x.sum(dim=dim, keepdim=keepdim)\n","    den = m.sum(dim=dim, keepdim=keepdim)\n","\n","    # safe division: where den > 0, num / den; else default\n","    default_tensor = num.new_full(num.shape, float(default))\n","    mean = torch.where(den > 0, num / torch.clamp(den, min=1e-12), default_tensor)\n","    return mean\n","\n","x = torch.tensor([[1., 2., 3.],\n","                  [4., 5., 6.]])\n","mask = torch.tensor([[1, 0, 1],\n","                     [0, 0, 0]])  # second row all masked out\n","\n","print(masked_mean(x, mask, dim=1))\n","# tensor([2., 0.])  (last row default=0)\n","\n","print(masked_mean(x, mask, dim=1, default=-1.0))\n","# tensor([2., -1.])\n","\n","\n","def segment_mean(\n","    values: torch.Tensor,\n","    segment_ids: torch.Tensor,\n","    num_segments: Optional[int] = None,\n","    default: Union[float, int] = 0.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Compute mean over segments:\n","        segment_means[k] = mean(values[segment_ids == k])\n","\n","    values: (N, D) or (N,)\n","    segment_ids: (N,) ints in [0, num_segments-1]\n","    \"\"\"\n","    if not values.is_floating_point():\n","        values = values.to(torch.float32)\n","\n","    if values.dim() == 1:\n","        values = values.unsqueeze(-1)  # make it (N, 1)\n","\n","    N, D = values.shape\n","    segment_ids = segment_ids.to(torch.long)\n","\n","    if num_segments is None:\n","        num_segments = int(segment_ids.max().item()) + 1 if N > 0 else 0\n","\n","    device = values.device\n","    dtype = values.dtype\n","\n","    # sums for each segment\n","    sums = torch.zeros(num_segments, D, device=device, dtype=dtype)\n","    counts = torch.zeros(num_segments, 1, device=device, dtype=dtype)\n","\n","    # index_add along segment dimension\n","    sums.index_add_(0, segment_ids, values)\n","    counts.index_add_(0, segment_ids, torch.ones_like(values[:, :1]))\n","\n","    default_tensor = sums.new_full(sums.shape, float(default))\n","    means = torch.where(\n","        counts > 0,\n","        sums / torch.clamp(counts, min=1e-12),\n","        default_tensor,\n","    )\n","\n","    # squeeze if original was 1D\n","    if values.shape[1] == 1:\n","        means = means.squeeze(-1)\n","\n","    return means\n","\n","vals = torch.tensor([[1., 2.],\n","                     [3., 4.],\n","                     [10., 20.]], dtype=torch.float32)\n","seg = torch.tensor([0, 0, 2])   # segment 1 is empty\n","\n","print(segment_mean(vals, seg, num_segments=3, default=0.0))\n","# tensor([[2., 3.],      # mean of rows 0 and 1\n","#         [0., 0.],      # empty segment -> default\n","#         [10., 20.]])   # row 2\n","\n","def batch_safe_mean(\n","    x: torch.Tensor,\n","    mask: torch.Tensor,\n","    default: Union[float, int] = 0.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Per-batch masked mean over time dimension 1.\n","\n","    x: (B, T, D) or (B, T)\n","    mask: (B, T) with 1/True = valid entries\n","    \"\"\"\n","    if x.dim() == 2:\n","        # (B, T) -> (B, T, 1) so we reuse the same logic\n","        x = x.unsqueeze(-1)\n","        squeeze_back = True\n","    else:\n","        squeeze_back = False\n","\n","    # broadcast mask to (B, T, 1)\n","    mask_exp = mask.unsqueeze(-1)\n","\n","    means = masked_mean(\n","        x,\n","        mask_exp,\n","        dim=1,           # average over time\n","        keepdim=False,\n","        default=default,\n","    )\n","\n","    if squeeze_back:\n","        means = means.squeeze(-1)\n","\n","    return means\n","\n","B, T, D = 2, 5, 3\n","x = torch.randn(B, T, D)\n","mask = torch.tensor([\n","    [1, 1, 1, 0, 0],   # first sequence length 3\n","    [0, 0, 0, 0, 0],   # second is fully padded\n","])\n","\n","m = batch_safe_mean(x, mask, default=0.0)\n","print(m.shape)   # (2, 3)\n","# row 0: mean over first 3 time steps\n","# row 1: [0., 0., 0.] from default\n","\n","import numpy as np\n","\n","def np_safe_mean(x: np.ndarray, axis=None, keepdims=False):\n","  if np.size(x) == 0:\n","      # Empty → return 0 with requested shape\n","      if axis is None:\n","          return 0.0\n","      # build shape as if mean had been taken, but filled with 0\n","      return np.zeros(np.mean(x, axis=axis, keepdims=keepdims).shape, dtype=float)\n","\n","  # normal mean is fine when non-empty\n","  return np.mean(x, axis=axis, keepdims=keepdims)\n","\n","\n","\n","def np_safe_masked_mean(x: np.ndarray,\n","                        mask: np.ndarray,\n","                        axis=None,\n","                        keepdims=False):\n","    m = mask.astype(float)\n","    masked = x * m\n","\n","    num = masked.sum(axis=axis, keepdims=keepdims)\n","    count = m.sum(axis=axis, keepdims=keepdims)\n","    safe_count = np.clip(count, 1.0, None)\n","    return num / safe_count\n","\n","\n","\n","def np_safe_segment_mean(values: np.ndarray,\n","                         segment_ids: np.ndarray,\n","                         num_segments: int | None = None):\n","    if num_segments is None:\n","        num_segments = int(segment_ids.max()) + 1\n","\n","    rest_shape = values.shape[1:]\n","    sums = np.zeros((num_segments, *rest_shape), dtype=values.dtype)\n","    counts = np.zeros(num_segments, dtype=float)\n","\n","    np.add.at(sums, segment_ids, values)\n","    np.add.at(counts, segment_ids, 1.0)\n","\n","    counts = np.clip(counts, 1.0, None)\n","    # reshape for broadcast\n","    while counts.ndim < sums.ndim:\n","        counts = counts[..., None]\n","\n","    return sums / counts\n","\n","\n"],"metadata":{"id":"_NJPCrNE44OS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, time, math\n","import numpy as np\n","import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", device)\n","\n","# =============================\n","# 1. Safe mean helpers (PyTorch / NumPy)\n","# =============================\n","\n","def torch_safe_mean(x: torch.Tensor, dim: int = -1, keepdim: bool = False):\n","    \"\"\"\n","    Safe mean: if the reduction dim is empty, returns 0 (not NaN).\n","    Works on CPU or GPU.\n","    \"\"\"\n","    if x.numel() == 0 or x.size(dim) == 0:\n","        # Build an output shape consistent with keepdim\n","        out_shape = list(x.shape)\n","        if keepdim:\n","            out_shape[dim] = 1\n","        else:\n","            del out_shape[dim]\n","        if len(out_shape) == 0:\n","            return torch.zeros((), dtype=x.dtype, device=x.device)\n","        return torch.zeros(out_shape, dtype=x.dtype, device=x.device)\n","    return x.mean(dim=dim, keepdim=keepdim)\n","\n","def torch_masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int = -1, eps: float = 1e-8):\n","    \"\"\"\n","    Masked mean: mean over elements where mask==1.\n","    If all masked-out, returns 0.\n","    x: (..., D), mask: same shape as x or broadcastable.\n","    \"\"\"\n","    mask = mask.to(dtype=x.dtype)\n","    num = (x * mask).sum(dim=dim)\n","    denom = mask.sum(dim=dim)\n","    mean = num / torch.clamp(denom, min=eps)\n","    # Zero out where denom is zero\n","    mean = torch.where(denom > 0, mean, torch.zeros_like(mean))\n","    return mean\n","\n","def torch_segment_mean(x: torch.Tensor, segment_ids: torch.Tensor, num_segments: int):\n","    \"\"\"\n","    Segment mean on CPU/GPU using scatter_add:\n","    x: (N, D), segment_ids: (N,), num_segments=K\n","    returns: (K, D)\n","    \"\"\"\n","    N, D = x.shape\n","    out = torch.zeros(num_segments, D, dtype=x.dtype, device=x.device)\n","    count = torch.zeros(num_segments, 1, dtype=x.dtype, device=x.device)\n","\n","    out.scatter_add_(0,\n","                     segment_ids.view(-1, 1).expand(-1, D),\n","                     x)\n","    ones = torch.ones(N, 1, dtype=x.dtype, device=x.device)\n","    count.scatter_add_(0,\n","                       segment_ids.view(-1, 1),\n","                       ones)\n","\n","    denom = torch.clamp(count, min=1.0)\n","    mean = out / denom\n","    mean[count.squeeze(-1) == 0] = 0.0\n","    return mean\n","\n","# NumPy equivalents\n","def np_safe_mean(x: np.ndarray, axis: int = -1, keepdims: bool = False):\n","    if x.size == 0 or x.shape[axis] == 0:\n","        out_shape = list(x.shape)\n","        if keepdims:\n","            out_shape[axis] = 1\n","        else:\n","            del out_shape[axis]\n","        if len(out_shape) == 0:\n","            return np.array(0, dtype=x.dtype)\n","        return np.zeros(out_shape, dtype=x.dtype)\n","    return x.mean(axis=axis, keepdims=keepdims)\n","\n","def np_masked_mean(x: np.ndarray, mask: np.ndarray, axis: int = -1, eps: float = 1e-8):\n","    mask = mask.astype(x.dtype)\n","    num = (x * mask).sum(axis=axis)\n","    denom = mask.sum(axis=axis)\n","    mean = num / np.clip(denom, eps, None)\n","    mean = np.where(denom > 0, mean, 0.0)\n","    return mean\n","\n","def np_segment_mean(x: np.ndarray, segment_ids: np.ndarray, num_segments: int):\n","    \"\"\"\n","    Segment mean for NumPy:\n","    x: (N, D), segment_ids: (N,), num_segments=K\n","    returns: (K, D)\n","    \"\"\"\n","    if x.ndim == 1:\n","        x = x[:, None]\n","        squeeze = True\n","    else:\n","        squeeze = False\n","\n","    N, D = x.shape\n","    out = np.zeros((num_segments, D), dtype=x.dtype)\n","    count = np.zeros((num_segments, 1), dtype=x.dtype)\n","\n","    for i in range(N):\n","        seg = int(segment_ids[i])\n","        out[seg] += x[i]\n","        count[seg] += 1\n","\n","    denom = np.clip(count, 1.0, None)\n","    mean = out / denom\n","    # zero out segments with count=0\n","    mask_zero = (count == 0).reshape(num_segments, 1)\n","    mean[mask_zero[:, 0]] = 0.0\n","\n","    if squeeze:\n","        mean = mean[:, 0]\n","    return mean\n","\n","# ======================================\n","# 2. Triton kernels: safe mean / masked mean / segment mean\n","# ======================================\n","\n","try:\n","    import triton\n","    import triton.language as tl\n","    HAS_TRITON = True\n","except Exception as e:\n","    print(\"Triton not available:\", e)\n","    HAS_TRITON = False\n","\n","if HAS_TRITON:\n","    @triton.jit\n","    def triton_row_mean_kernel(\n","        x_ptr, out_ptr,\n","        B, D,\n","        BLOCK_SIZE: tl.constexpr,\n","    ):\n","        row_id = tl.program_id(0)\n","        # Each program computes mean over one row x[row_id, :]\n","        offs = tl.arange(0, BLOCK_SIZE)\n","        acc = tl.zeros((), dtype=tl.float32)\n","        # Loop over D in chunks of BLOCK_SIZE\n","        for start in range(0, D, BLOCK_SIZE):\n","            idx = start + offs\n","            mask = idx < D\n","            vals = tl.load(x_ptr + row_id * D + idx, mask=mask, other=0.0)\n","            acc += tl.sum(vals.to(tl.float32), axis=0)\n","        mean = acc / tl.max(tl.float32(D), 1.0)\n","        tl.store(out_ptr + row_id, mean)\n","\n","    @triton.jit\n","    def triton_row_masked_mean_kernel(\n","        x_ptr, mask_ptr, out_ptr,\n","        B, D,\n","        BLOCK_SIZE: tl.constexpr,\n","    ):\n","        row_id = tl.program_id(0)\n","        offs = tl.arange(0, BLOCK_SIZE)\n","        sum_acc = tl.zeros((), dtype=tl.float32)\n","        cnt_acc = tl.zeros((), dtype=tl.float32)\n","        for start in range(0, D, BLOCK_SIZE):\n","            idx = start + offs\n","            mask = idx < D\n","            vals = tl.load(x_ptr + row_id * D + idx, mask=mask, other=0.0)\n","            m = tl.load(mask_ptr + row_id * D + idx, mask=mask, other=0)\n","            vals = vals.to(tl.float32)\n","            m = m.to(tl.float32)\n","            sum_acc += tl.sum(vals * m, axis=0)\n","            cnt_acc += tl.sum(m, axis=0)\n","        denom = tl.where(cnt_acc > 0, cnt_acc, 1.0)\n","        mean = sum_acc / denom\n","        mean = tl.where(cnt_acc > 0, mean, 0.0)\n","        tl.store(out_ptr + row_id, mean)\n","\n","    @triton.jit\n","    def triton_segment_sum_kernel(\n","        x_ptr, seg_ptr, out_ptr, cnt_ptr,\n","        N, D, K,\n","        BLOCK_SIZE_N: tl.constexpr,\n","        BLOCK_SIZE_D: tl.constexpr,\n","    ):\n","        \"\"\"\n","        Atomically accumulate segment sums and counts:\n","        x: (N, D) -> out: (K, D), cnt: (K,)\n","        \"\"\"\n","        n_offsets = tl.program_id(0) * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n","        d_offsets = tl.program_id(1) * BLOCK_SIZE_D + tl.arange(0, BLOCK_SIZE_D)\n","\n","        mask_n = n_offsets < N\n","        # For each (n, d), accumulate into out[seg, d] and cnt[seg]\n","        for n in n_offsets:\n","            if n >= N:\n","                continue\n","            seg = tl.load(seg_ptr + n).to(tl.int32)\n","            # inner loop over d\n","            x_row_ptr = x_ptr + n * D\n","            for d in d_offsets:\n","                if d >= D:\n","                    continue\n","                val = tl.load(x_row_ptr + d)\n","                tl.atomic_add(out_ptr + seg * D + d, val.to(tl.float32))\n","            # count (only once per row)\n","            tl.atomic_add(cnt_ptr + seg, 1.0)\n","\n","# ======================================\n","# 3. CUDA kernels via torch.utils.cpp_extension\n","# ======================================\n","\n","from torch.utils.cpp_extension import load\n","\n","cuda_src = r\"\"\"\n","#include <torch/extension.h>\n","#include <cuda.h>\n","#include <cuda_runtime.h>\n","\n","template<typename scalar_t>\n","__global__ void row_mean_kernel(const scalar_t* __restrict__ x,\n","                                float* __restrict__ out,\n","                                int B, int D) {\n","  int row = blockIdx.x;\n","  if (row >= B) return;\n","  extern __shared__ float sdata[];\n","  int tid = threadIdx.x;\n","  sdata[tid] = 0.0f;\n","\n","  for (int col = tid; col < D; col += blockDim.x) {\n","    float v = static_cast<float>(x[row * D + col]);\n","    sdata[tid] += v;\n","  }\n","  __syncthreads();\n","\n","  // reduction in shared mem\n","  for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","    if (tid < s) {\n","      sdata[tid] += sdata[tid + s];\n","    }\n","    __syncthreads();\n","  }\n","\n","  if (tid == 0) {\n","    float denom = D > 0 ? static_cast<float>(D) : 1.0f;\n","    out[row] = (D > 0) ? (sdata[0] / denom) : 0.0f;\n","  }\n","}\n","\n","template<typename scalar_t>\n","__global__ void row_masked_mean_kernel(const scalar_t* __restrict__ x,\n","                                       const uint8_t* __restrict__ mask,\n","                                       float* __restrict__ out,\n","                                       int B, int D) {\n","  int row = blockIdx.x;\n","  if (row >= B) return;\n","  extern __shared__ float sdata[];\n","  float* s_sum = sdata;\n","  float* s_cnt = sdata + blockDim.x;\n","\n","  int tid = threadIdx.x;\n","  s_sum[tid] = 0.0f;\n","  s_cnt[tid] = 0.0f;\n","\n","  for (int col = tid; col < D; col += blockDim.x) {\n","    int idx = row * D + col;\n","    uint8_t m = mask[idx];\n","    if (m) {\n","      float v = static_cast<float>(x[idx]);\n","      s_sum[tid] += v;\n","      s_cnt[tid] += 1.0f;\n","    }\n","  }\n","  __syncthreads();\n","\n","  // reduce\n","  for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n","    if (tid < s) {\n","      s_sum[tid] += s_sum[tid + s];\n","      s_cnt[tid] += s_cnt[tid + s];\n","    }\n","    __syncthreads();\n","  }\n","\n","  if (tid == 0) {\n","    float denom = s_cnt[0] > 0.0f ? s_cnt[0] : 1.0f;\n","    float mean = (s_cnt[0] > 0.0f) ? (s_sum[0] / denom) : 0.0f;\n","    out[row] = mean;\n","  }\n","}\n","\n","template<typename scalar_t>\n","__global__ void segment_sum_kernel(const scalar_t* __restrict__ x,\n","                                   const int32_t* __restrict__ seg_ids,\n","                                   float* __restrict__ out,\n","                                   float* __restrict__ cnt,\n","                                   int N, int D, int K) {\n","  int n = blockIdx.x;\n","  if (n >= N) return;\n","  int tid = threadIdx.x;\n","  int seg = seg_ids[n];\n","  if (seg < 0 || seg >= K) return;\n","\n","  for (int d = tid; d < D; d += blockDim.x) {\n","    float v = static_cast<float>(x[n * D + d]);\n","    atomicAdd(out + seg * D + d, v);\n","  }\n","  // count once per row (thread 0)\n","  if (tid == 0) {\n","    atomicAdd(cnt + seg, 1.0f);\n","  }\n","}\n","\n","torch::Tensor cuda_row_mean(torch::Tensor x) {\n","  TORCH_CHECK(x.is_cuda(), \"x must be CUDA\");\n","  TORCH_CHECK(x.dim() == 2, \"x must be (B, D)\");\n","  const auto B = x.size(0);\n","  const auto D = x.size(1);\n","  auto out = torch::empty({B}, x.options().dtype(torch::kFloat32));\n","  const int threads = 256;\n","  const int blocks = B;\n","  const size_t shmem = threads * sizeof(float);\n","  AT_DISPATCH_ALL_TYPES_AND(torch::ScalarType::Half, x.scalar_type(), \"row_mean_kernel\", [&] {\n","    row_mean_kernel<scalar_t><<<blocks, threads, shmem>>>(\n","      x.data_ptr<scalar_t>(),\n","      out.data_ptr<float>(),\n","      B, D\n","    );\n","  });\n","  return out;\n","}\n","\n","torch::Tensor cuda_row_masked_mean(torch::Tensor x, torch::Tensor mask) {\n","  TORCH_CHECK(x.is_cuda(), \"x must be CUDA\");\n","  TORCH_CHECK(mask.is_cuda(), \"mask must be CUDA\");\n","  TORCH_CHECK(x.sizes() == mask.sizes(), \"x and mask shape mismatch\");\n","  TORCH_CHECK(x.dim() == 2, \"x must be (B, D)\");\n","  const auto B = x.size(0);\n","  const auto D = x.size(1);\n","  auto out = torch::empty({B}, x.options().dtype(torch::kFloat32));\n","  const int threads = 256;\n","  const int blocks = B;\n","  const size_t shmem = threads * sizeof(float) * 2;\n","  AT_DISPATCH_ALL_TYPES_AND(torch::ScalarType::Half, x.scalar_type(), \"row_masked_mean_kernel\", [&] {\n","    row_masked_mean_kernel<scalar_t><<<blocks, threads, shmem>>>(\n","      x.data_ptr<scalar_t>(),\n","      mask.data_ptr<uint8_t>(),\n","      out.data_ptr<float>(),\n","      B, D\n","    );\n","  });\n","  return out;\n","}\n","\n","std::vector<torch::Tensor> cuda_segment_mean(torch::Tensor x,\n","                                             torch::Tensor seg_ids,\n","                                             int64_t K) {\n","  TORCH_CHECK(x.is_cuda(), \"x must be CUDA\");\n","  TORCH_CHECK(seg_ids.is_cuda(), \"seg_ids must be CUDA\");\n","  TORCH_CHECK(x.dim() == 2, \"x must be (N, D)\");\n","  TORCH_CHECK(seg_ids.dim() == 1, \"seg_ids must be (N)\");\n","  const auto N = x.size(0);\n","  const auto D = x.size(1);\n","  auto out = torch::zeros({K, D}, x.options().dtype(torch::kFloat32));\n","  auto cnt = torch::zeros({K}, x.options().dtype(torch::kFloat32));\n","  const int threads = 256;\n","  const int blocks = N;\n","  AT_DISPATCH_ALL_TYPES_AND(torch::ScalarType::Half, x.scalar_type(), \"segment_sum_kernel\", [&] {\n","    segment_sum_kernel<scalar_t><<<blocks, threads>>>(\n","      x.data_ptr<scalar_t>(),\n","      seg_ids.data_ptr<int32_t>(),\n","      out.data_ptr<float>(),\n","      cnt.data_ptr<float>(),\n","      N, D, (int)K\n","    );\n","  });\n","  return {out, cnt};\n","}\n","\n","PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n","  m.def(\"row_mean\", &cuda_row_mean, \"Row-wise mean (safe) [CUDA]\");\n","  m.def(\"row_masked_mean\", &cuda_row_masked_mean, \"Row-wise masked mean (safe) [CUDA]\");\n","  m.def(\"segment_mean_raw\", &cuda_segment_mean, \"Segment sum+count (CUDA)\");\n","}\n","\"\"\"\n","\n","if device == \"cuda\":\n","    cuda_kernels = load(\n","        name=\"mean_kernels\",\n","        sources=[cuda_src],\n","        verbose=False,\n","    )\n","    print(\"Loaded custom CUDA kernels.\")\n","else:\n","    cuda_kernels = None\n","    print(\"CUDA not available, skipping custom kernels.\")\n","\n","# ======================================\n","# 4. Benchmark harness\n","# ======================================\n","\n","def bench(fn, iters=50, cuda_sync=True):\n","    # warmup\n","    for _ in range(5):\n","        fn()\n","    if cuda_sync and torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","    t0 = time.perf_counter()\n","    for _ in range(iters):\n","        out = fn()\n","    if cuda_sync and torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","    t1 = time.perf_counter()\n","    return (t1 - t0) * 1000.0 / iters  # ms\n","\n","# ======================================\n","# 5. Run benchmarks for various dtypes\n","# ======================================\n","\n","B, D = 4096, 1024      # row-wise mean shape\n","N, K = 8192, 128       # segment shape (N rows, K segments)\n","\n","dtypes = [torch.float32, torch.float16]  # you can add int8 etc. for CPU / cast\n","\n","for dtype in dtypes:\n","    print(f\"\\n=== dtype: {dtype} ===\")\n","\n","    # generate data\n","    x_cpu = torch.randn(B, D, dtype=dtype)\n","    mask_cpu = (torch.rand(B, D) > 0.5).to(torch.bool)\n","    seg_ids_cpu = torch.randint(low=0, high=K, size=(N,), dtype=torch.int64)\n","    x_seg_cpu = torch.randn(N, D, dtype=dtype)\n","\n","    # PyTorch CPU\n","    def fn_torch_cpu_batch_mean():\n","        return torch_safe_mean(x_cpu, dim=-1)\n","\n","    def fn_torch_cpu_masked_mean():\n","        return torch_masked_mean(x_cpu, mask_cpu, dim=-1)\n","\n","    def fn_torch_cpu_segment_mean():\n","        return torch_segment_mean(x_seg_cpu, seg_ids_cpu, num_segments=K)\n","\n","    print(\"=== PyTorch CPU ===\")\n","    print(\"torch_cpu_batch_mean:   %.3f ms\" % bench(fn_torch_cpu_batch_mean, cuda_sync=False))\n","    print(\"torch_cpu_masked_mean:  %.3f ms\" % bench(fn_torch_cpu_masked_mean, cuda_sync=False))\n","    print(\"torch_cpu_segment_mean: %.3f ms\" % bench(fn_torch_cpu_segment_mean, cuda_sync=False))\n","\n","    # NumPy CPU\n","    x_np = x_cpu.numpy().astype(np.float32)   # use float32 internally\n","    mask_np = mask_cpu.numpy()\n","    x_seg_np = x_seg_cpu.numpy().astype(np.float32)\n","    seg_ids_np = seg_ids_cpu.numpy()\n","\n","    def fn_numpy_batch_mean():\n","        return np_safe_mean(x_np, axis=-1)\n","\n","    def fn_numpy_masked_mean():\n","        return np_masked_mean(x_np, mask_np, axis=-1)\n","\n","    def fn_numpy_segment_mean():\n","        return np_segment_mean(x_seg_np, seg_ids_np, num_segments=K)\n","\n","    print(\"=== NumPy CPU ===\")\n","    print(\"numpy_batch_mean:   %.3f ms\" % bench(fn_numpy_batch_mean, cuda_sync=False))\n","    print(\"numpy_masked_mean:  %.3f ms\" % bench(fn_numpy_masked_mean, cuda_sync=False))\n","    print(\"numpy_segment_mean: %.3f ms\" % bench(fn_numpy_segment_mean, cuda_sync=False))\n","\n","    if device == \"cuda\":\n","        x_gpu = x_cpu.to(device)\n","        mask_gpu = mask_cpu.to(device)\n","        x_seg_gpu = x_seg_cpu.to(device)\n","        seg_ids_gpu = seg_ids_cpu.to(device).to(torch.int32)\n","\n","        def fn_torch_gpu_batch_mean():\n","            return torch_safe_mean(x_gpu, dim=-1)\n","\n","        def fn_torch_gpu_masked_mean():\n","            return torch_masked_mean(x_gpu, mask_gpu, dim=-1)\n","\n","        def fn_torch_gpu_segment_mean():\n","            return torch_segment_mean(x_seg_gpu, seg_ids_gpu.to(torch.long), num_segments=K)\n","\n","        print(\"=== PyTorch GPU ===\")\n","        print(\"torch_gpu_batch_mean:   %.3f ms\" % bench(fn_torch_gpu_batch_mean))\n","        print(\"torch_gpu_masked_mean:  %.3f ms\" % bench(fn_torch_gpu_masked_mean))\n","        print(\"torch_gpu_segment_mean: %.3f ms\" % bench(fn_torch_gpu_segment_mean))\n","\n","        # Triton\n","        if HAS_TRITON:\n","            B_, D_ = x_gpu.shape\n","\n","            def fn_triton_batch_mean():\n","                x32 = x_gpu.to(torch.float32)\n","                out = torch.empty(B_, device=device, dtype=torch.float32)\n","                grid = (B_,)\n","                triton_row_mean_kernel[grid](\n","                    x32, out, B_, D_,\n","                    BLOCK_SIZE=128,\n","                )\n","                return out\n","\n","            def fn_triton_masked_mean():\n","                x32 = x_gpu.to(torch.float32)\n","                m8 = mask_gpu.to(torch.uint8)\n","                out = torch.empty(B_, device=device, dtype=torch.float32)\n","                grid = (B_,)\n","                triton_row_masked_mean_kernel[grid](\n","                    x32, m8, out, B_, D_,\n","                    BLOCK_SIZE=128,\n","                )\n","                return out\n","\n","            def fn_triton_segment_mean():\n","                # segment_sum -> divide\n","                x32 = x_seg_gpu.to(torch.float32)\n","                out = torch.zeros(K, D, dtype=torch.float32, device=device)\n","                cnt = torch.zeros(K, dtype=torch.float32, device=device)\n","                grid = (triton.cdiv(N, 32), triton.cdiv(D, 32))\n","                triton_segment_sum_kernel[grid](\n","                    x32, seg_ids_gpu, out, cnt,\n","                    N, D, K,\n","                    BLOCK_SIZE_N=32,\n","                    BLOCK_SIZE_D=32,\n","                )\n","                denom = torch.clamp(cnt.view(K, 1), min=1.0)\n","                mean = out / denom\n","                mean[cnt == 0] = 0.0\n","                return mean\n","\n","            print(\"=== Triton GPU ===\")\n","            print(\"triton_batch_mean:   %.3f ms\" % bench(fn_triton_batch_mean))\n","            print(\"triton_masked_mean:  %.3f ms\" % bench(fn_triton_masked_mean))\n","            print(\"triton_segment_mean: %.3f ms\" % bench(fn_triton_segment_mean))\n","\n","        # CUDA custom\n","        if cuda_kernels is not None:\n","            def fn_cuda_batch_mean():\n","                # returns float32\n","                return cuda_kernels.row_mean(x_gpu)\n","\n","            def fn_cuda_masked_mean():\n","                return cuda_kernels.row_masked_mean(x_gpu, mask_gpu.to(torch.uint8))\n","\n","            def fn_cuda_segment_mean():\n","                out, cnt = cuda_kernels.segment_mean_raw(x_seg_gpu, seg_ids_gpu, K)\n","                denom = torch.clamp(cnt.view(K, 1), min=1.0)\n","                mean = out / denom\n","                mean[cnt == 0] = 0.0\n","                return mean\n","\n","            print(\"=== Custom CUDA ===\")\n","            print(\"cuda_batch_mean:   %.3f ms\" % bench(fn_cuda_batch_mean))\n","            print(\"cuda_masked_mean:  %.3f ms\" % bench(fn_cuda_masked_mean))\n","            print(\"cuda_segment_mean: %.3f ms\" % bench(fn_cuda_segment_mean))"],"metadata":{"id":"Ia134wDD-QLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# triton_safe_masked_mean.py\n","import torch\n","import triton\n","import triton.language as tl\n","\n","@triton.jit\n","def safe_masked_mean_rowwise_kernel(\n","    x_ptr,         # *f32\n","    mask_ptr,      # *i32 or *f32 (0/1); pass nullptr for unmasked\n","    out_ptr,       # *f32\n","    B, N,\n","    stride_xb, stride_xn,\n","    stride_mb, stride_mn,\n","    BLOCK_N: tl.constexpr,\n","):\n","    b = tl.program_id(0)  # batch index\n","\n","    offs_n = tl.arange(0, BLOCK_N)\n","    row_x_ptr = x_ptr + b * stride_xb + offs_n * stride_xn\n","\n","    has_mask = mask_ptr != 0\n","    if has_mask:\n","        row_m_ptr = mask_ptr + b * stride_mb + offs_n * stride_mn\n","\n","    acc_sum = tl.zeros((), dtype=tl.float32)\n","    acc_count = tl.zeros((), dtype=tl.float32)\n","\n","    for start_n in range(0, N, BLOCK_N):\n","        cur_mask = start_n + offs_n < N\n","\n","        x_vals = tl.load(row_x_ptr + start_n * stride_xn,\n","                         mask=cur_mask,\n","                         other=0.0)\n","\n","        if has_mask:\n","            m_vals = tl.load(row_m_ptr + start_n * stride_mn,\n","                             mask=cur_mask,\n","                             other=0)\n","            # assume mask is 0/1 or bool, cast to float\n","            m_vals_f = m_vals.to(tl.float32)\n","        else:\n","            m_vals_f = tl.where(cur_mask, 1.0, 0.0)\n","\n","        acc_sum += tl.sum(x_vals * m_vals_f, axis=0)\n","        acc_count += tl.sum(m_vals_f, axis=0)\n","\n","    # safe mean: if acc_count == 0, define mean = 0\n","    safe_count = tl.maximum(acc_count, 1.0)\n","    mean = acc_sum / safe_count\n","    mean = tl.where(acc_count > 0, mean, 0.0)\n","\n","    tl.store(out_ptr + b, mean)\n","\n","def safe_masked_mean_rowwise(x: torch.Tensor, mask: torch.Tensor | None = None):\n","    \"\"\"\n","    x: (B, N), float32\n","    mask: (B, N) or None; 0/1 or bool\n","    returns: (B,)\n","    \"\"\"\n","    assert x.dim() == 2\n","    B, N = x.shape\n","    x = x.contiguous()\n","\n","    if mask is not None:\n","        mask = mask.to(torch.int32).contiguous()\n","        mask_ptr = mask\n","    else:\n","        mask_ptr = torch.tensor([], device=x.device, dtype=torch.int32)  # dummy\n","        # We'll treat 'mask_ptr != 0' as 'has_mask', so ensure it's not literally None.\n","        # Instead: we pass mask_ptr.data_ptr()==0? Triton can't; hack: pass 0 below.\n","\n","    out = torch.empty(B, device=x.device, dtype=torch.float32)\n","\n","    BLOCK_N = 128\n","    grid = (B,)\n","\n","    # carefully pass 0 for mask_ptr when mask is None\n","    mask_arg = mask_ptr if mask is not None else 0\n","\n","    safe_masked_mean_rowwise_kernel[grid](\n","        x,\n","        mask_arg,\n","        out,\n","        B, N,\n","        x.stride(0), x.stride(1),\n","        mask.stride(0) if mask is not None else 0,\n","        mask.stride(1) if mask is not None else 0,\n","        BLOCK_N=BLOCK_N,\n","    )\n","    return out\n","\n","// safe_masked_mean.cu\n","#include <cuda_runtime.h>\n","#include <stdint.h>\n","\n","__global__ void safe_masked_mean_kernel(\n","    const float* __restrict__ x,\n","    const uint8_t* __restrict__ mask,  // 0 or 1; nullptr for unmasked\n","    int64_t N,\n","    float* __restrict__ out_sum,\n","    float* __restrict__ out_count\n",") {\n","    extern __shared__ float shmem[];\n","    float* sh_sum   = shmem;\n","    float* sh_count = shmem + blockDim.x;\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    float local_sum = 0.0f;\n","    float local_count = 0.0f;\n","\n","    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n","        uint8_t m = mask ? mask[i] : 1;\n","        if (m) {\n","            local_sum += x[i];\n","            local_count += 1.0f;\n","        }\n","    }\n","\n","    sh_sum[threadIdx.x] = local_sum;\n","    sh_count[threadIdx.x] = local_count;\n","    __syncthreads();\n","\n","    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n","        if (threadIdx.x < stride) {\n","            sh_sum[threadIdx.x] += sh_sum[threadIdx.x + stride];\n","            sh_count[threadIdx.x] += sh_count[threadIdx.x + stride];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (threadIdx.x == 0) {\n","        atomicAdd(out_sum, sh_sum[0]);\n","        atomicAdd(out_count, sh_count[0]);\n","    }\n","}\n","\n","\n","#include <torch/extension.h>  // or your own wrapper\n","#include <cuda_runtime.h>\n","\n","std::pair<float, float> safe_masked_mean_cuda(\n","    const float* d_x,\n","    const uint8_t* d_mask,\n","    int64_t N\n",") {\n","    float h_sum = 0.0f;\n","    float h_count = 0.0f;\n","\n","    float* d_sum;\n","    float* d_count;\n","    cudaMalloc(&d_sum, sizeof(float));\n","    cudaMalloc(&d_count, sizeof(float));\n","    cudaMemcpy(d_sum, &h_sum, sizeof(float), cudaMemcpyHostToDevice);\n","    cudaMemcpy(d_count, &h_count, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    int threads = 256;\n","    int blocks = (N + threads - 1) / threads;\n","    size_t shmem = 2 * threads * sizeof(float);\n","\n","    safe_masked_mean_kernel<<<blocks, threads, shmem>>>(d_x, d_mask, N, d_sum, d_count);\n","\n","    cudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n","    cudaMemcpy(&h_count, d_count, sizeof(float), cudaMemcpyDeviceToHost);\n","\n","    cudaFree(d_sum);\n","    cudaFree(d_count);\n","\n","    // safe mean\n","    float mean = (h_count > 0.0f) ? (h_sum / h_count) : 0.0f;\n","    return {mean, h_count};\n","}\n","\n","sum[seg] = Σ x[i] for i with segment_ids[i] == seg and mask[i]==1\n","count[seg] = Σ 1    for same\n","mean[seg] = sum[seg] / max(count[seg], 1)\n","\n","// segment_mean.cu\n","#include <cuda_runtime.h>\n","#include <stdint.h>\n","\n","__global__ void segment_sum_count_kernel(\n","    const float* __restrict__ x,\n","    const int32_t* __restrict__ segment_ids,\n","    const uint8_t* __restrict__ mask,    // 0/1 or nullptr\n","    int64_t N,\n","    float* __restrict__ seg_sums,\n","    float* __restrict__ seg_counts,\n","    int32_t num_segments\n",") {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n","        uint8_t m = mask ? mask[i] : 1;\n","        if (!m) {\n","            continue;\n","        }\n","\n","        int32_t seg = segment_ids[i];\n","        if (seg < 0 || seg >= num_segments) {\n","            continue;  // or assert\n","        }\n","\n","        float val = x[i];\n","        atomicAdd(&seg_sums[seg], val);\n","        atomicAdd(&seg_counts[seg], 1.0f);\n","    }\n","}\n","\n","__global__ void segment_safe_mean_kernel(\n","    const float* __restrict__ seg_sums,\n","    const float* __restrict__ seg_counts,\n","    float* __restrict__ seg_means,\n","    int32_t num_segments\n",") {\n","    int seg = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (seg >= num_segments) return;\n","\n","    float s = seg_sums[seg];\n","    float c = seg_counts[seg];\n","\n","    if (c > 0.0f) {\n","        seg_means[seg] = s / c;\n","    } else {\n","        seg_means[seg] = 0.0f;  // safe mean for empty segment\n","    }\n","}\n","\n","#include <torch/extension.h>  // or your own wrapper\n","#include <cuda_runtime.h>\n","\n","std::pair<float, float> safe_masked_mean_cuda(\n","    const float* d_x,\n","    const uint8_t* d_mask,\n","    int64_t N\n",") {\n","    float h_sum = 0.0f;\n","    float h_count = 0.0f;\n","\n","    float* d_sum;\n","    float* d_count;\n","    cudaMalloc(&d_sum, sizeof(float));\n","    cudaMalloc(&d_count, sizeof(float));\n","    cudaMemcpy(d_sum, &h_sum, sizeof(float), cudaMemcpyHostToDevice);\n","    cudaMemcpy(d_count, &h_count, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    int threads = 256;\n","    int blocks = (N + threads - 1) / threads;\n","    size_t shmem = 2 * threads * sizeof(float);\n","\n","    safe_masked_mean_kernel<<<blocks, threads, shmem>>>(d_x, d_mask, N, d_sum, d_count);\n","\n","    cudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n","    cudaMemcpy(&h_count, d_count, sizeof(float), cudaMemcpyDeviceToHost);\n","\n","    cudaFree(d_sum);\n","    cudaFree(d_count);\n","\n","    // safe mean\n","    float mean = (h_count > 0.0f) ? (h_sum / h_count) : 0.0f;\n","    return {mean, h_count};\n","}\n","\n","#sum[seg] = Σ x[i] for i with segment_ids[i] == seg and mask[i]==1\n","#count[seg] = Σ 1    for same\n","#mean[seg] = sum[seg] / max(count[seg], 1)\n","\n","// segment_mean.cu\n","#include <cuda_runtime.h>\n","#include <stdint.h>\n","\n","__global__ void segment_sum_count_kernel(\n","    const float* __restrict__ x,\n","    const int32_t* __restrict__ segment_ids,\n","    const uint8_t* __restrict__ mask,    // 0/1 or nullptr\n","    int64_t N,\n","    float* __restrict__ seg_sums,\n","    float* __restrict__ seg_counts,\n","    int32_t num_segments\n",") {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n","        uint8_t m = mask ? mask[i] : 1;\n","        if (!m) {\n","            continue;\n","        }\n","\n","        int32_t seg = segment_ids[i];\n","        if (seg < 0 || seg >= num_segments) {\n","            continue;  // or assert\n","        }\n","\n","        float val = x[i];\n","        atomicAdd(&seg_sums[seg], val);\n","        atomicAdd(&seg_counts[seg], 1.0f);\n","    }\n","}\n","\n","__global__ void segment_safe_mean_kernel(\n","    const float* __restrict__ seg_sums,\n","    const float* __restrict__ seg_counts,\n","    float* __restrict__ seg_means,\n","    int32_t num_segments\n",") {\n","    int seg = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (seg >= num_segments) return;\n","\n","    float s = seg_sums[seg];\n","    float c = seg_counts[seg];\n","\n","    if (c > 0.0f) {\n","        seg_means[seg] = s / c;\n","    } else {\n","        seg_means[seg] = 0.0f;  // safe mean for empty segment\n","    }\n","}\n","\n","void segment_mean_cuda(\n","    const float* d_x,\n","    const int32_t* d_segment_ids,\n","    const uint8_t* d_mask,      // may be nullptr\n","    int64_t N,\n","    int32_t num_segments,\n","    float* d_out_means\n",") {\n","    float* d_sums;\n","    float* d_counts;\n","    cudaMalloc(&d_sums,   num_segments * sizeof(float));\n","    cudaMalloc(&d_counts, num_segments * sizeof(float));\n","    cudaMemset(d_sums,   0, num_segments * sizeof(float));\n","    cudaMemset(d_counts, 0, num_segments * sizeof(float));\n","\n","    int threads = 256;\n","    int blocks = (N + threads - 1) / threads;\n","\n","    segment_sum_count_kernel<<<blocks, threads>>>(\n","        d_x, d_segment_ids, d_mask, N, d_sums, d_counts, num_segments\n","    );\n","\n","    int blocks_seg = (num_segments + threads - 1) / threads;\n","    segment_safe_mean_kernel<<<blocks_seg, threads>>>(\n","        d_sums, d_counts, d_out_means, num_segments\n","    );\n","\n","    cudaFree(d_sums);\n","    cudaFree(d_counts);\n","}\n","\n","\n","#triton segment mean sketch\n","@triton.jit\n","def segment_sum_count_kernel_triton(\n","    x_ptr, seg_id_ptr, mask_ptr,\n","    seg_sums_ptr, seg_counts_ptr,\n","    N, NUM_SEGMENTS: tl.constexpr,\n","    BLOCK_SIZE: tl.constexpr,\n","):\n","    pid = tl.program_id(0)\n","    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n","    mask = offs < N\n","\n","    x = tl.load(x_ptr + offs, mask=mask, other=0.0)\n","    seg = tl.load(seg_id_ptr + offs, mask=mask, other=0)\n","    has_mask = mask_ptr != 0\n","    if has_mask:\n","        m = tl.load(mask_ptr + offs, mask=mask, other=0)\n","        valid = mask & (m != 0)\n","    else:\n","        valid = mask\n","\n","    x = tl.where(valid, x, 0.0)\n","    seg = tl.where(valid, seg, 0)\n","\n","    # atomic adds\n","    tl.atomic_add(seg_sums_ptr + seg, x, mask=valid)\n","    tl.atomic_add(seg_counts_ptr + seg,\n","                  tl.where(valid, 1.0, 0.0),\n","                  mask=valid)"],"metadata":{"id":"TDCfqhM7pyEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Colab-ready benchmark: safe mean, masked mean, segment mean, batch mean\n","# Across: PyTorch CPU, PyTorch GPU, NumPy CPU, Triton kernel, CUDA (CuPy) kernel\n","import torch\n","!pip install -q triton==3.0.0 cupy-cuda12x\n","\n","import time\n","import math\n","from dataclasses import dataclass\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","\n","import triton\n","import triton.language as tl\n","\n","import cupy as cp\n","\n","# ------------------------------------------------------------\n","# 0. Config\n","# ------------------------------------------------------------\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", DEVICE)\n","\n","# Problem sizes\n","BATCH = 4096     # number of rows\n","N      = 1024    # row length\n","SEG_K  = 128     # number of segments for segment mean\n","\n","WARMUP_ITERS = 5\n","BENCH_ITERS  = 20\n","\n","# ------------------------------------------------------------\n","# 1. Safe mean helpers (PyTorch & NumPy)\n","# ------------------------------------------------------------\n","\n","def torch_safe_mean(x: torch.Tensor, dim=None, keepdim=False) -> torch.Tensor:\n","    \"\"\"\n","    Safe mean: if count == 0, returns 0 (no NaN).\n","    Works with arbitrary dimension, keeps gradients.\n","    \"\"\"\n","    if dim is None:\n","        # flatten\n","        x_flat = x.view(-1)\n","        count = x_flat.numel()\n","        if count == 0:\n","            return x_flat.new_zeros(())\n","        return x_flat.sum() / max(count, 1)\n","    else:\n","        # general dim\n","        x = x.float()\n","        ones = torch.ones_like(x, dtype=x.dtype)\n","        count = ones.sum(dim=dim, keepdim=keepdim)\n","        s = x.sum(dim=dim, keepdim=keepdim)\n","        # clamp denominator to at least 1 to avoid NaNs; where count==0, we force 0\n","        denom = count.clamp_min(1.0)\n","        mean = s / denom\n","        mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n","        return mean\n","\n","def torch_masked_mean(x: torch.Tensor, mask: torch.Tensor, dim=-1, keepdim=False):\n","    \"\"\"\n","    mask: bool or 0/1, same shape as x.\n","    \"\"\"\n","    x = x.float()\n","    mask = mask.to(dtype=x.dtype)\n","    s = (x * mask).sum(dim=dim, keepdim=keepdim)\n","    count = mask.sum(dim=dim, keepdim=keepdim)\n","    denom = count.clamp_min(1.0)\n","    mean = s / denom\n","    mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n","    return mean\n","\n","def torch_segment_mean(x: torch.Tensor, segment_ids: torch.Tensor, num_segments: int):\n","    \"\"\"\n","    x: [N, D] or [N]; segment_ids: [N] in [0, num_segments-1]\n","    Returns [num_segments, D] or [num_segments]\n","    \"\"\"\n","    if x.dim() == 1:\n","        x = x[:, None]\n","        squeeze = True\n","    else:\n","        squeeze = False\n","\n","    N, D = x.shape\n","    device = x.device\n","\n","    segment_ids = segment_ids.to(device=device, dtype=torch.long)\n","    out = torch.zeros(num_segments, D, device=device, dtype=x.dtype)\n","    count = torch.zeros(num_segments, 1, device=device, dtype=x.dtype)\n","\n","    out.index_add_(0, segment_ids, x)\n","    ones = torch.ones(N, 1, device=device, dtype=x.dtype)\n","    count.index_add_(0, segment_ids, ones)\n","\n","    denom = count.clamp_min(1.0)\n","    mean = out / denom\n","    mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n","\n","    if squeeze:\n","        mean = mean[:, 0]\n","    return mean\n","\n","# NumPy equivalents (no gradient)\n","def np_safe_mean(x: np.ndarray, axis=None, keepdims=False):\n","    if x.size == 0:\n","        return np.zeros((), dtype=x.dtype)\n","    count = x.shape[axis] if axis is not None else x.size\n","    s = x.sum(axis=axis, keepdims=keepdims)\n","    return s / max(count, 1)\n","\n","def np_masked_mean(x: np.ndarray, mask: np.ndarray, axis=-1, keepdims=False):\n","    x = x.astype(np.float32)\n","    mask = mask.astype(np.float32)\n","    s = (x * mask).sum(axis=axis, keepdims=keepdims)\n","    count = mask.sum(axis=axis, keepdims=keepdims)\n","    denom = np.clip(count, 1.0, None)\n","    out = s / denom\n","    out = np.where(count > 0, out, np.zeros_like(out))\n","    return out\n","\n","import numpy as np\n","\n","def np_segment_mean(x: np.ndarray,\n","                    segment_ids: np.ndarray,\n","                    num_segments: int):\n","    # Handle 1D x by temporarily promoting to 2D\n","    if x.ndim == 1:\n","        x = x[:, None]   # (N,) -> (N, 1)\n","        squeeze = True\n","    else:\n","        squeeze = False\n","\n","    N, D = x.shape\n","    out = np.zeros((num_segments, D), dtype=x.dtype)      # sum per segment\n","    count = np.zeros((num_segments,), dtype=np.int64)     # count per segment\n","\n","    # Accumulate per segment\n","    for i in range(N):\n","        seg = int(segment_ids[i])\n","        if 0 <= seg < num_segments:\n","            out[seg] += x[i]\n","            count[seg] += 1\n","\n","    # Avoid div-by-zero by clamping denominator to at least 1\n","    denom = np.maximum(count, 1).reshape(num_segments, 1)  # (K, 1)\n","    mean = out / denom                                    # (K, D)\n","\n","    # For segments where count == 0, explicitly set mean to 0\n","    zero_mask = (count == 0)  # (K,)\n","    mean[zero_mask] = 0.0\n","\n","    if squeeze:\n","        mean = mean[:, 0]  # (K,)\n","    return mean\n","\n","# ------------------------------------------------------------\n","# 2. Data setup\n","# ------------------------------------------------------------\n","torch.manual_seed(0)\n","np.random.seed(0)\n","\n","x_torch_cpu = torch.randn(BATCH, N, dtype=torch.float32)\n","mask_torch_cpu = (torch.rand(BATCH, N) > 0.3).to(torch.bool)\n","\n","seg_ids_cpu = torch.randint(0, SEG_K, (BATCH,), dtype=torch.long)\n","\n","x_np = x_torch_cpu.numpy()\n","mask_np = mask_torch_cpu.numpy().astype(np.bool_)\n","seg_ids_np = seg_ids_cpu.numpy()\n","\n","if DEVICE == \"cuda\":\n","    x_torch_gpu = x_torch_cpu.to(\"cuda\")\n","    mask_torch_gpu = mask_torch_cpu.to(\"cuda\")\n","    seg_ids_gpu = seg_ids_cpu.to(\"cuda\")\n","else:\n","    x_torch_gpu = None\n","    mask_torch_gpu = None\n","    seg_ids_gpu = None\n","\n","# ------------------------------------------------------------\n","# 3. Triton kernel: row-wise mean (batch mean) for 2D tensor\n","# ------------------------------------------------------------\n","\n","@triton.jit\n","def row_mean_kernel(X_ptr, Y_ptr, BATCH, N, BLOCK_SIZE: tl.constexpr):\n","    row_id = tl.program_id(0)\n","    offs = row_id * N + tl.arange(0, BLOCK_SIZE)\n","    mask = offs < (row_id * N + N)\n","\n","    x = tl.load(X_ptr + offs, mask=mask, other=0.0)\n","    # parallel reduction in block\n","    # here we just sum and rely on BLOCK_SIZE == N for simplicity\n","    # (you can generalize to partial tiles if needed)\n","    s = tl.sum(x, axis=0)\n","    # each program handles a whole row\n","    denom = N\n","    mean = s / denom\n","    tl.store(Y_ptr + row_id, mean)\n","\n","def triton_row_mean(x: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    x: [B, N] on CUDA\n","    returns: [B]\n","    \"\"\"\n","    assert x.is_cuda\n","    B, N = x.shape\n","    y = torch.empty(B, device=x.device, dtype=x.dtype)\n","\n","    BLOCK_SIZE = N  # simple case: one block per row\n","    grid = (B,)\n","\n","    row_mean_kernel[grid](\n","        x, y,\n","        BATCH=B,\n","        N=N,\n","        BLOCK_SIZE=BLOCK_SIZE,\n","        num_warps=4,\n","    )\n","    return y\n","\n","# ------------------------------------------------------------\n","# 4. CUDA kernel with CuPy: row-wise mean\n","# ------------------------------------------------------------\n","\n","cuda_row_mean_src = r\"\"\"\n","extern \"C\" __global__\n","void row_mean(const float* __restrict__ x,\n","              float* __restrict__ y,\n","              int B, int N) {\n","    int row = blockIdx.x;\n","    if (row >= B) return;\n","\n","    float sum = 0.0f;\n","    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n","        sum += x[row * N + i];\n","    }\n","\n","    __shared__ float smem[256]; // up to 256 threads\n","    int tid = threadIdx.x;\n","    smem[tid] = sum;\n","    __syncthreads();\n","\n","    // simple reduction in shared memory\n","    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n","        if (tid < stride) {\n","            smem[tid] += smem[tid + stride];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        y[row] = smem[0] / (float)N;\n","    }\n","}\n","\"\"\"\n","\n","row_mean_kernel_cuda = cp.RawKernel(cuda_row_mean_src, \"row_mean\")\n","\n","def cuda_row_mean(x_torch: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Takes a CUDA torch tensor [B, N], uses CuPy to run custom kernel,\n","    returns torch tensor [B].\n","    \"\"\"\n","    assert x_torch.is_cuda\n","    B, N = x_torch.shape\n","    # zero-copy via DLPack\n","    x_cu = cp.fromDlpack(torch.utils.dlpack.to_dlpack(x_torch))\n","    y_cu = cp.empty((B,), dtype=cp.float32)\n","\n","    threads_per_block = 256\n","    blocks = (B,)\n","\n","    row_mean_kernel_cuda(blocks, (threads_per_block,),\n","                         (x_cu, y_cu, B, N))\n","\n","    # back to torch\n","    y_torch = torch.utils.dlpack.from_dlpack(y_cu.toDlpack())\n","    return y_torch\n","\n","# ------------------------------------------------------------\n","# 5. Benchmark helpers\n","# ------------------------------------------------------------\n","\n","@dataclass\n","class BenchResult:\n","    name: str\n","    time_ms: float\n","\n","def bench(fn, iters=BENCH_ITERS, warmup=WARMUP_ITERS):\n","    # Warmup\n","    for _ in range(warmup):\n","        out = fn()\n","        if isinstance(out, torch.Tensor) and out.is_cuda:\n","            torch.cuda.synchronize()\n","\n","    t0 = time.perf_counter()\n","    for _ in range(iters):\n","        out = fn()\n","        if isinstance(out, torch.Tensor) and out.is_cuda:\n","            torch.cuda.synchronize()\n","    t1 = time.perf_counter()\n","    return (t1 - t0) * 1000.0 / iters\n","\n","results = []\n","\n","# ------------------------------------------------------------\n","# 6. PyTorch CPU benchmarks\n","# ------------------------------------------------------------\n","print(\"\\n=== PyTorch CPU ===\")\n","\n","# mean (batch-level: row-wise)\n","def fn_torch_cpu_batch_mean():\n","    return torch_safe_mean(x_torch_cpu, dim=1)  # [B]\n","\n","t = bench(fn_torch_cpu_batch_mean)\n","results.append(BenchResult(\"torch_cpu_batch_mean\", t))\n","print(\"torch_cpu_batch_mean: %.3f ms\" % t)\n","\n","# masked mean (row-wise)\n","def fn_torch_cpu_masked_mean():\n","    return torch_masked_mean(x_torch_cpu, mask_torch_cpu, dim=1)\n","\n","t = bench(fn_torch_cpu_masked_mean)\n","results.append(BenchResult(\"torch_cpu_masked_mean\", t))\n","print(\"torch_cpu_masked_mean: %.3f ms\" % t)\n","\n","# segment mean (over batch dimension)\n","def fn_torch_cpu_segment_mean():\n","    return torch_segment_mean(x_torch_cpu, seg_ids_cpu, num_segments=SEG_K)\n","\n","t = bench(fn_torch_cpu_segment_mean)\n","results.append(BenchResult(\"torch_cpu_segment_mean\", t))\n","print(\"torch_cpu_segment_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 7. PyTorch GPU benchmarks (if available)\n","# ------------------------------------------------------------\n","if DEVICE == \"cuda\":\n","    print(\"\\n=== PyTorch GPU ===\")\n","\n","    def fn_torch_gpu_batch_mean():\n","        return torch_safe_mean(x_torch_gpu, dim=1)\n","\n","    t = bench(fn_torch_gpu_batch_mean)\n","    results.append(BenchResult(\"torch_gpu_batch_mean\", t))\n","    print(\"torch_gpu_batch_mean: %.3f ms\" % t)\n","\n","    def fn_torch_gpu_masked_mean():\n","        return torch_masked_mean(x_torch_gpu, mask_torch_gpu, dim=1)\n","\n","    t = bench(fn_torch_gpu_masked_mean)\n","    results.append(BenchResult(\"torch_gpu_masked_mean\", t))\n","    print(\"torch_gpu_masked_mean: %.3f ms\" % t)\n","\n","    def fn_torch_gpu_segment_mean():\n","        return torch_segment_mean(x_torch_gpu, seg_ids_gpu, num_segments=SEG_K)\n","\n","    t = bench(fn_torch_gpu_segment_mean)\n","    results.append(BenchResult(\"torch_gpu_segment_mean\", t))\n","    print(\"torch_gpu_segment_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 8. NumPy CPU benchmarks\n","# ------------------------------------------------------------\n","print(\"\\n=== NumPy CPU ===\")\n","\n","def fn_numpy_batch_mean():\n","    return np_safe_mean(x_np, axis=1)\n","\n","t = bench(fn_numpy_batch_mean)\n","results.append(BenchResult(\"numpy_batch_mean\", t))\n","print(\"numpy_batch_mean: %.3f ms\" % t)\n","\n","def fn_numpy_masked_mean():\n","    return np_masked_mean(x_np, mask_np, axis=1)\n","\n","t = bench(fn_numpy_masked_mean)\n","results.append(BenchResult(\"numpy_masked_mean\", t))\n","print(\"numpy_masked_mean: %.3f ms\" % t)\n","\n","def fn_numpy_segment_mean():\n","    return np_segment_mean(x_np, seg_ids_np, num_segments=SEG_K)\n","\n","t = bench(fn_numpy_segment_mean)\n","results.append(BenchResult(\"numpy_segment_mean\", t))\n","print(\"numpy_segment_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 9. Triton benchmark (row-wise mean only)\n","# ------------------------------------------------------------\n","if DEVICE == \"cuda\":\n","    print(\"\\n=== Triton row-wise mean (CUDA) ===\")\n","\n","    def fn_triton_row_mean():\n","        return triton_row_mean(x_torch_gpu)\n","\n","    t = bench(fn_triton_row_mean)\n","    results.append(BenchResult(\"triton_row_mean\", t))\n","    print(\"triton_row_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 10. CUDA (CuPy) kernel benchmark (row-wise mean only)\n","# ------------------------------------------------------------\n","if DEVICE == \"cuda\":\n","    print(\"\\n=== CUDA (CuPy) row-wise mean ===\")\n","\n","    def fn_cuda_row_mean():\n","        return cuda_row_mean(x_torch_gpu)\n","\n","    t = bench(fn_cuda_row_mean)\n","    results.append(BenchResult(\"cuda_row_mean\", t))\n","    print(\"cuda_row_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 11. Summary\n","# ------------------------------------------------------------\n","print(\"\\n=== Summary (ms per call) ===\")\n","for r in results:\n","    print(f\"{r.name:30s}: {r.time_ms:8.3f} ms\")"],"metadata":{"id":"cF_qp5KNuyyz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Math trick in self attention\n","Karpathy YT https://www.youtube.com/watch?v=kCc8FmEb1nY 42:27\n"],"metadata":{"id":"wUPZFtNNE40E"}},{"cell_type":"code","source":["\n","torch.manual_seed(1337)\n","B, T, C = 4,8,2\n","x = torch.randn(B,T,C)\n","x.shape\n","#we want to look at past tokens from currnt positio\n","# Batch, Time, Channels\n","#simplest way to communicate with past tokens is to take average of tokens before\n","# current token. This vector of 5 past tokens with an average becomes\n","#\n","xbow = torch.zeros(B,T,C)\n","for batch in range(B):\n","  for time in range(T):\n","    xprev = x[batch, time+1, ] #t,C\n","    xbow[b,t] = torch.mean()\n"],"metadata":{"id":"aoQwFMnrAw1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ohiYhXzqFUzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVvvy3F3SkZg"},"outputs":[],"source":["#https://github.com/facebookresearch/MobileLLM-R1\n","# is this really true? Doesnt match Karpathy's progression with nanogpt, ie faster tokenizer needed, etc..\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# vim editor to prevent unaligned tabs when pasting into vim from chatGPT\n","# :set paste"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/quora"],"metadata":{"id":"d3fmWmq8bdX5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Simple pytorch dataset"],"metadata":{"id":"QosdDZOeN2kF"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","PATH = \"/content/drive/MyDrive/quora/questions 3.csv\"\n","df = pd.read_csv(PATH)\n","\n","df = df.dropna(subset=[\"question1\", \"question2\", \"is_duplicate\"])\n","df = df.sample(50000, random_state=0)  # subsample for speed\n","\n","df.head()"],"metadata":{"id":"FPTYZJ1lbdaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$"],"metadata":{"id":"rPLpWeZYl3is"}},{"cell_type":"code","source":["# attention\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","\n","def get_embeddings(text):\n","  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","  model = AutoModel.from_pretrained(\"bert-base-uncased\")\n","  inputs = tokenizer(text, return_tensors=\"pt\")\n","  print(inputs)\n","  outputs = model(**inputs)\n","  return outputs.last_hidden_state\n","\n","print(get_embeddings(\"hello\").shape)\n","print(get_embeddings(\"hello\"))\n","#the first dim = batch size, how many sentences\n","#second dim = sequence len or token count. add start/stop tokens to hello, CLS, SEP, classification token and separator token\n","# one row for each token."],"metadata":{"id":"3PNqlz6Jlzb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sdpa(Wq, Wk, Wv, text):\n","  embed = get_embeddings(text)\n","  Q = torch.matmul(embed, Wq)\n","  K = torch.matmul(embed, Wk)\n","  V = torch.matmul(embed, Wv)\n","  print(Q.shape,K.shape,embed.shape)\n","  attn_scores = torch.matmul(Q, torch.transpose(K,1,2)) / torch.sqrt(torch.tensor(embed.shape[2]))\n","  #d_k is the embedding dimension\n","  #do we need -1?\n","  print(\"attn_scores shape\",attn_scores.shape)\n","  scaled = torch.softmax(attn_scores,dim=-1)\n","  attn = torch.matmul(scaled, V)\n","\n","  return attn\n","\n","Wq = torch.rand(768,768)\n","Wk = torch.rand(768,768)\n","Wv = torch.rand(768,768)\n","\n","print(sdpa(Wq, Wk, Wv, \"hello\").shape)\n","\n"],"metadata":{"id":"2EDTjVFylzd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)\\mathbf{W}^O$$"],"metadata":{"id":"T9_MSkVDBYcM"}},{"cell_type":"code","source":["#karpathy lets build gpt 1:02:00\n","torch.manual_seed(1337)\n","B,T,C = 4, 8, 32\n","x = torch.randn(B,T,C)\n","\n","tril = torch.tril(torch.ones(T,T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril==0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","\n","out = wei @ x\n"],"metadata":{"id":"f7bK3fYtlzgg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Triton SDPA\n","\n","When $\\text{softmax}$ is applied:$$\\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\text{ with causal mask}\\right)$$The $e^{-\\infty}$ terms become zero, effectively giving zero attention weight to all future tokens."],"metadata":{"id":"7XTjyLd_DTE4"}},{"cell_type":"code","source":["import torch\n","import triton\n","import triton.language as tl\n","import math\n","\n","# Define the Triton Kernel\n","@triton.jit\n","def causal_attention_kernel(\n","    Q, K, V, O,  # Data Pointers for Query, Key, Value, Output\n","    sm_scale,  # Scaling factor: 1/sqrt(d_k)\n","    # Tensors dimensions\n","    Lq, Lk, Lv, Lo,  # Strides for Q, K, V, O\n","    N_CTX,  # Sequence Length (Max tokens)\n","    D_HEAD,  # Head Dimension (d_k)\n","    # Block dimensions (These are fixed by the user when launching)\n","    BLOCK_M: tl.constexpr,\n","    BLOCK_N: tl.constexpr,\n","    BLOCK_DMODEL: tl.constexpr,\n","):\n","    \"\"\"\n","    Computes Causal Masked Scaled Dot-Product Attention.\n","\n","    This kernel implements the online softmax trick to avoid materializing the\n","    full N x N attention matrix in global memory, which is the core principle\n","    of FlashAttention.\n","    \"\"\"\n","    # 1. Block Indexing for parallelization\n","    # Program ID (PID) maps to a Query block M (rows in the attention matrix)\n","    pid_m = tl.program_id(0)\n","\n","    # Initialize a pointer to the output block (O_ptr)\n","    # O is indexed by [pid_m * BLOCK_M, :], scaled by the strides.\n","    offs_om = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n","    offs_n = tl.arange(0, BLOCK_DMODEL)\n","    O_ptr = O + offs_om[:, None] * Lo + offs_n[None, :] * Lk\n","\n","    # Initialize the accumulators for the output (O_i), running maximum (m_i),\n","    # and normalization factor (l_i). These are the core variables for online softmax.\n","    m_i = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n","    l_i = tl.full([BLOCK_M], 0.0, dtype=tl.float32)\n","    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n","\n","    # 2. Load Query block Q_i\n","    # Q is indexed by [pid_m * BLOCK_M, :]\n","    offs_qm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n","    offs_d = tl.arange(0, BLOCK_DMODEL)\n","    Q_ptr = Q + offs_qm[:, None] * Lq + offs_d[None, :] * Lk\n","\n","    # Load Q block and multiply by the scaling factor\n","    q = tl.load(Q_ptr) * sm_scale\n","\n","    # 3. Iterate over blocks of K_j and V_j (tiling along the sequence dimension N)\n","    for start_n in range(0, N_CTX, BLOCK_N):\n","        # Create pointer offsets for the current block K_j and V_j\n","        offs_n_load = start_n + tl.arange(0, BLOCK_N)\n","        offs_k = offs_n_load[None, :] * Lk + offs_d[:, None] * Lq\n","        K_ptr = K + offs_k\n","        V_ptr = V + offs_n_load[:, None] * Lv + offs_d[None, :] * Lk\n","\n","        # Load K_j and V_j block\n","        k = tl.load(K_ptr)\n","        v = tl.load(V_ptr)\n","\n","        # 4. Compute attention scores S_ij = Q_i * K_j^T\n","        # s has shape [BLOCK_M, BLOCK_N]\n","        s = tl.dot(q, k, allow_tf32=True)\n","\n","        # 5. Apply Causal Masking (Prevent attending to future tokens)\n","        # s must be masked where query index > key index.\n","        # This is where the causal constraint (i <= j) is enforced.\n","        mask = offs_qm[:, None] >= offs_n_load[None, :]\n","        s = tl.where(mask, s, float(\"-inf\"))\n","\n","        # 6. Online Softmax Update (Row-wise max and sum)\n","        # This is the core trick for numerical stability and memory efficiency.\n","\n","        # 6a. Compute the new row-wise maximum m_j\n","        m_j = tl.max(s, 1)\n","\n","        # 6b. Update the running maximum m_i\n","        m_new = tl.maximum(m_i, m_j)\n","\n","        # 6c. Compute the exponential terms e_i and e_j\n","        alpha = tl.exp(m_i - m_new)\n","        beta = tl.exp(m_j - m_new)\n","\n","        # 6d. Update the running normalization factor l_i\n","        l_new = alpha * l_i + beta\n","        l_i = l_new\n","\n","        # 6e. Re-scale the previous accumulator acc\n","        acc_scale = alpha / l_i\n","        acc = acc * acc_scale[:, None]\n","\n","        # 6f. Compute the attention weights and update the accumulator\n","        s = s - m_new[:, None]\n","        p = tl.exp(s) * (beta / l_i)[:, None]\n","\n","        # 6g. Update the accumulator: acc_new = acc_old + P_ij * V_j\n","        acc = acc + tl.dot(p, v, allow_tf32=True)\n","\n","        # 6h. Update the running maximum m_i for the next iteration\n","        m_i = m_new\n","\n","    # 7. Write the final result to the output tensor O\n","    # The final output is acc (weighted sum of V) divided by the final normalization l_i\n","    tl.store(O_ptr, acc / l_i[:, None])\n","\n","# ----------------------------------------------------------------------\n","# Python Host Wrapper\n","# ----------------------------------------------------------------------\n","\n","def run_attention_kernel(q, k, v, is_causal=True):\n","    \"\"\"\n","    Runs the Triton attention kernel with torch tensors.\n","\n","    Args:\n","        q (torch.Tensor): Query tensor (L, D).\n","        k (torch.Tensor): Key tensor (L, D).\n","        v (torch.Tensor): Value tensor (L, D).\n","        is_causal (bool): Whether to apply causal masking.\n","    \"\"\"\n","    assert q.shape == k.shape == v.shape, \"Q, K, V must have the same shape (L, D) for self-attention.\"\n","    assert q.is_cuda and k.is_cuda and v.is_cuda, \"Inputs must be on the GPU.\"\n","\n","    N_CTX, D_HEAD = q.shape[0], q.shape[1]\n","\n","    # Hyperparameters: Tune these for performance\n","    BLOCK_M = 64  # Block size for the Query dimension (rows)\n","    BLOCK_N = 64  # Block size for the Key dimension (columns)\n","    BLOCK_DMODEL = D_HEAD # Block size for the Head dimension\n","\n","    # Define the scaling factor: 1/sqrt(d_k)\n","    sm_scale = 1.0 / math.sqrt(D_HEAD)\n","\n","    # Output tensor initialization (N_CTX, D_HEAD)\n","    o = torch.empty_like(q)\n","\n","    # 1D launch grid: we launch one program for each block M (rows) in the sequence\n","    grid = lambda META: (triton.cdiv(N_CTX, META['BLOCK_M']),)\n","\n","    # Kernel call\n","    causal_attention_kernel[grid](\n","        q, k, v, o,  # Pointers\n","        sm_scale,  # Scaling factor\n","        q.stride(0), k.stride(0), v.stride(0), o.stride(0),  # Strides\n","        N_CTX, D_HEAD,\n","        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=BLOCK_DMODEL,\n","        num_warps=4,\n","        num_stages=3\n","    )\n","    return o\n","\n","# ----------------------------------------------------------------------\n","# Example Usage\n","# ----------------------------------------------------------------------\n","if __name__ == '__main__':\n","    # Ensure inputs are in a format compatible with Triton (float16 is typical for perf)\n","    dtype = torch.float16\n","    device = 'cuda'\n","\n","    # Set Sequence Length (N_CTX) and Head Dimension (D_HEAD)\n","    N_CTX = 256\n","    D_HEAD = 64\n","\n","    # Create dummy tensors for Q, K, V (all derived from the same input for self-attention)\n","    # Shape: [Sequence Length, Head Dimension]\n","    Q_data = torch.randn(N_CTX, D_HEAD, dtype=dtype, device=device)\n","    K_data = torch.randn(N_CTX, D_HEAD, dtype=dtype, device=device)\n","    V_data = torch.randn(N_CTX, D_HEAD, dtype=dtype, device=device)\n","\n","    # Run the Triton kernel\n","    output_triton = run_attention_kernel(Q_data, K_data, V_data)\n","\n","    print(f\"Input Shape (Q, K, V): {Q_data.shape}\")\n","    print(f\"Output Shape (O):      {output_triton.shape}\")\n","    print(f\"\\nExample Output (First 5 elements of first row):\\n{output_triton[0, :5]}\")\n","\n","    # For comparison, you would compare this output against a known, verified\n","    # PyTorch implementation (like F.scaled_dot_product_attention with causal=True).\n","    # The numerical results should be close, demonstrating the kernel works."],"metadata":{"id":"7EFU1dQGlzjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Kbvk4DMwlzld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts1 = df[\"question1\"].tolist()\n","texts2 = df[\"question2\"].tolist()\n","y = df[\"is_duplicate\"].values"],"metadata":{"id":"15ofxLH1bde6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q sentence-transformers"],"metadata":{"id":"4FalCyi7bdj4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","st_model = SentenceTransformer(model_name)\n","\n","# For speed, we encode q1 and q2 separately and reuse\n","emb1 = st_model.encode(texts1, batch_size=128, convert_to_numpy=True)\n","emb2 = st_model.encode(texts2, batch_size=128, convert_to_numpy=True)"],"metadata":{"id":"wAcct-SXdgPH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(texts1), type(texts2))\n","print(len(texts1), len(texts2))\n","print(texts1[0][:100])\n","print(texts2[0][:100])"],"metadata":{"id":"9WOQuPPc6CGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(texts1),len(texts2)"],"metadata":{"id":"gbvuzr975WIr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Basic lexical features\n","tfidf = TfidfVectorizer(dtype=np.float64, min_df=5, max_features=20000, stop_words=\"english\")\n","\n","tfidf.fit(texts1 + texts2)\n","\n","\n","import numpy as np\n","\n","def lexical_features(q1_list, q2_list):\n","    assert len(q1_list) == len(q2_list), \"q1_list and q2_list must have same length\"\n","\n","    # TF-IDF vectors\n","    v1 = tfidf.transform(q1_list)   # csr_matrix [N, V]\n","    v2 = tfidf.transform(q2_list)   # csr_matrix [N, V]\n","\n","    # L2 norms: sum(...) returns np.matrix, so convert to 1D ndarray with .A1\n","    v1_sq = v1.multiply(v1).sum(axis=1).A1   # [N]\n","    v2_sq = v2.multiply(v2).sum(axis=1).A1   # [N]\n","\n","    v1_norm = np.sqrt(v1_sq) + 1e-8          # [N]\n","    v2_norm = np.sqrt(v2_sq) + 1e-8          # [N]\n","\n","    # Cosine similarity: again, sum(...) → matrix, so .A1\n","    num = v1.multiply(v2).sum(axis=1).A1     # [N]\n","    cos = num / (v1_norm * v2_norm)          # [N], elementwise\n","\n","    # Length-based features\n","    len1 = np.array([len(t.split()) for t in q1_list])   # [N]\n","    len2 = np.array([len(t.split()) for t in q2_list])   # [N]\n","    len_diff = np.abs(len1 - len2)                       # [N]\n","\n","    # Crude token overlap (Jaccard)\n","    overlap = []\n","    for a, b in zip(q1_list, q2_list):\n","        s1 = set(a.lower().split())\n","        s2 = set(b.lower().split())\n","        if not s1 or not s2:\n","            overlap.append(0.0)\n","        else:\n","            overlap.append(len(s1 & s2) / len(s1 | s2))\n","    overlap = np.array(overlap)                          # [N]\n","\n","    # Stack into feature matrix [N, 5]\n","    feats = np.stack([cos, len1, len2, len_diff, overlap], axis=1)\n","    return feats\n","\n","# import numpy as np\n","\n","# def lexical_features(q1_list, q2_list):\n","#     assert len(q1_list) == len(q2_list), \"q1_list and q2_list must have same length\"\n","\n","#     # TF-IDF vectors for each side\n","#     v1 = tfidf.transform(q1_list)   # sparse [N, V]\n","#     v2 = tfidf.transform(q2_list)   # sparse [N, V]\n","\n","#     # L2 norms\n","#     v1_norm = np.sqrt(v1.multiply(v1).sum(axis=1)) + 1e-8   # [N, 1]\n","#     v2_norm = np.sqrt(v2.multiply(v2).sum(axis=1)) + 1e-8   # [N, 1]\n","\n","#     # Cosine similarity for each pair\n","#     cos = (v1.multiply(v2).sum(axis=1) / (v1_norm * v2_norm)).A1  # -> [N]\n","\n","#     # Length-based features\n","#     len1 = np.array([len(t.split()) for t in q1_list])           # [N]\n","#     len2 = np.array([len(t.split()) for t in q2_list])           # [N]\n","#     len_diff = np.abs(len1 - len2)                               # [N]\n","\n","#     # Crude token overlap (Jaccard)\n","#     overlap = []\n","#     for a, b in zip(q1_list, q2_list):\n","#         s1 = set(a.lower().split())\n","#         s2 = set(b.lower().split())\n","#         if not s1 or not s2:\n","#             overlap.append(0.0)\n","#         else:\n","#             overlap.append(len(s1 & s2) / len(s1 | s2))\n","#     overlap = np.array(overlap)                                  # [N]\n","\n","#     # Stack into feature matrix [N, 5]\n","#     feats = np.stack([cos, len1, len2, len_diff, overlap], axis=1)\n","#     return feats\n","\n","\n","# def lexical_features(q1_list, q2_list):\n","#     # naive TF-IDF cosine similarity & lengths & token overlap\n","#     v1 = tfidf.transform(q1_list)\n","#     v2 = tfidf.transform(q2_list)\n","\n","#     v1_norm = np.sqrt(v1.multiply(v1).sum(axis=1)) + 1e-8\n","#     v2_norm = np.sqrt(v2.multiply(v2).sum(axis=1)) + 1e-8\n","#     cos = (v1.multiply(v2).sum(axis=1) / (v1_norm * v2_norm)).A1  # cosine similarity\n","\n","#     len1 = np.array([len(t.split()) for t in q1_list])\n","#     len2 = np.array([len(t.split()) for t in q2_list])\n","#     len_diff = np.abs(len1 - len2)\n","\n","#     # crude token overlap\n","#     overlap = []\n","#     for a, b in zip(q1_list, q2_list):\n","#         s1 = set(a.lower().split())\n","#         s2 = set(b.lower().split())\n","#         if not s1 or not s2:\n","#             overlap.append(0.0)\n","#         else:\n","#             overlap.append(len(s1 & s2) / len(s1 | s2))\n","#     overlap = np.array(overlap)\n","\n","#     return np.stack([cos, len1, len2, len_diff, overlap], axis=1)\n","\n","lex_feats = lexical_features(texts1, texts2)\n","print(lex_feats.shape)"],"metadata":{"id":"wO4eKR6-dgRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pairwise embedding features\n","diff = np.abs(emb1 - emb2)\n","prod = emb1 * emb2\n","pair_emb = np.concatenate([emb1, emb2, diff, prod], axis=1)\n","\n","X_all = np.concatenate([pair_emb, lex_feats], axis=1)\n","\n","scaler = StandardScaler()\n","X_all = scaler.fit_transform(X_all)\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_all, y, test_size=0.2, random_state=0, stratify=y\n",")"],"metadata":{"id":"m8KqF-5CdgUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score\n","\n","def train_model(X, y, sample_weight=None):\n","    clf = LogisticRegression(\n","        max_iter=200,\n","        class_weight=None,\n","        n_jobs=-1,\n","    )\n","    clf.fit(X, y, sample_weight=sample_weight)\n","    return clf\n","\n","def eval_model(clf, X_val, y_val):\n","    y_pred = clf.predict(X_val)\n","    return f1_score(y_val, y_pred)\n","\n","base_clf = train_model(X_train, y_train)\n","base_f1 = eval_model(base_clf, X_val, y_val)\n","print(\"Base F1:\", base_f1)\n","\n","p_train = base_clf.predict_proba(X_train)[:, 1]\n","conf = np.abs(p_train - 0.5)  # small = uncertain → more likely noisy"],"metadata":{"id":"ADtptpUjdgXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["candidate_policies = [\n","    {\"theta\": 0.05, \"noisy_weight\": 0.5},\n","    {\"theta\": 0.10, \"noisy_weight\": 0.5},\n","    {\"theta\": 0.10, \"noisy_weight\": 0.2},\n","    {\"theta\": 0.15, \"noisy_weight\": 0.2},\n","    {\"theta\": 0.20, \"noisy_weight\": 0.0},  # drop most uncertain\n","    {\"theta\": 0.30, \"noisy_weight\": 0.0},\n","]\n","\n","def apply_policy(policy, conf):\n","    theta = policy[\"theta\"]\n","    noisy_weight = policy[\"noisy_weight\"]\n","    w = np.ones_like(conf, dtype=float)\n","    w[conf < theta] = noisy_weight\n","    return w\n","\n","n_trials = 20\n","epsilon = 0.3\n","K = len(candidate_policies)\n","Q = np.zeros(K)\n","N = np.zeros(K)\n","\n","for t in range(n_trials):\n","    if np.random.rand() < epsilon:\n","        k = np.random.randint(K)\n","    else:\n","        k = np.argmax(Q)\n","\n","    policy = candidate_policies[k]\n","    sample_weight = apply_policy(policy, conf)\n","\n","    clf = train_model(X_train, y_train, sample_weight=sample_weight)\n","    reward = eval_model(clf, X_val, y_val)\n","\n","    N[k] += 1\n","    Q[k] += (reward - Q[k]) / N[k]\n","\n","    print(f\"Iter {t:02d} | trial policy {k} {policy} | F1={reward:.4f}\")\n","\n","best_k = np.argmax(Q)\n","best_policy = candidate_policies[best_k]\n","print(\"\\nBase F1:\", base_f1)\n","print(\"Best policy from bandit:\", best_policy, \"Estimated F1:\", Q[best_k])"],"metadata":{"id":"ZwGGw2nTdgai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q cleanlab"],"metadata":{"id":"HC-i-aMsJ3f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Literal, Dict, Any, Tuple, Callable\n","\n","class NoisyDatasetCleaner:\n","    \"\"\"\n","    A generic wrapper for noisy supervised datasets.\n","    Strategies:\n","      - 'bandit_weight'\n","      - 'cleanlab'\n","      - 'none'\n","    You can extend it with active-relabelling logic.\n","    \"\"\"\n","    def __init__(self,\n","                 strategy: Literal[\"none\", \"bandit_weight\", \"cleanlab\"] = \"none\",\n","                 clf_factory: Callable[[], Any] = None,\n","                 strategy_kwargs: Optional[Dict[str, Any]] = None):\n","        self.strategy = strategy\n","        self.clf_factory = clf_factory or (lambda: LogisticRegression(max_iter=300, n_jobs=-1))\n","        self.strategy_kwargs = strategy_kwargs or {}\n","        self.sample_weight_ = None\n","        self._cleanlab_model = None\n","\n","    def fit(self, X, y) -> \"NoisyDatasetCleaner\":\n","        if self.strategy == \"none\":\n","            self.sample_weight_ = np.ones(len(y), dtype=float)\n","\n","        elif self.strategy == \"bandit_weight\":\n","            self._fit_bandit_weight(X, y)\n","\n","        elif self.strategy == \"cleanlab\":\n","            self._fit_cleanlab(X, y)\n","\n","        else:\n","            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n","\n","        return self\n","\n","    def _fit_bandit_weight(self, X, y):\n","        # train base model\n","        base_clf = self.clf_factory()\n","        base_clf.fit(X, y)\n","        p = base_clf.predict_proba(X)[:, 1]\n","        conf = np.abs(p - 0.5)\n","\n","        # simple discrete bandit over thresholds, like earlier\n","        candidate_policies = self.strategy_kwargs.get(\"candidate_policies\") or [\n","            {\"theta\": 0.05, \"noisy_weight\": 0.5},\n","            {\"theta\": 0.10, \"noisy_weight\": 0.5},\n","            {\"theta\": 0.10, \"noisy_weight\": 0.2},\n","            {\"theta\": 0.15, \"noisy_weight\": 0.2},\n","            {\"theta\": 0.20, \"noisy_weight\": 0.0},\n","            {\"theta\": 0.30, \"noisy_weight\": 0.0},\n","        ]\n","        n_trials = self.strategy_kwargs.get(\"n_trials\", 10)\n","        epsilon  = self.strategy_kwargs.get(\"epsilon\", 0.3)\n","        X_train, X_val, y_train, y_val, conf_train = train_test_split(\n","            X, y, conf, test_size=0.2, random_state=0, stratify=y\n","        )\n","\n","        def apply_policy(policy, conf_vec):\n","            w = np.ones_like(conf_vec, dtype=float)\n","            w[conf_vec < policy[\"theta\"]] = policy[\"noisy_weight\"]\n","            return w\n","\n","        K = len(candidate_policies)\n","        Q = np.zeros(K)\n","        N = np.zeros(K)\n","\n","        for _ in range(n_trials):\n","            if np.random.rand() < epsilon:\n","                k = np.random.randint(K)\n","            else:\n","                k = np.argmax(Q)\n","\n","            policy = candidate_policies[k]\n","            weights_train = apply_policy(policy, conf_train)\n","\n","            clf = self.clf_factory()\n","            clf.fit(X_train, y_train, sample_weight=weights_train)\n","            y_pred_val = clf.predict(X_val)\n","            reward = f1_score(y_val, y_pred_val)\n","\n","            N[k] += 1\n","            Q[k] += (reward - Q[k]) / N[k]\n","\n","        best_k = np.argmax(Q)\n","        best_policy = candidate_policies[best_k]\n","\n","        # final weights on full data\n","        self.sample_weight_ = apply_policy(best_policy, conf)\n","\n","    def _fit_cleanlab(self, X, y):\n","        from cleanlab.classification import CleanLearning\n","\n","        base_clf = self.clf_factory()\n","        cl = CleanLearning(clf=base_clf)\n","        cl.fit(X, y)\n","        self._cleanlab_model = cl\n","\n","        # label issues summary -> create weights\n","        issues = cl.get_label_issues()\n","        is_issue = issues[\"is_label_issue\"].values\n","        # simple scheme: 0.3 weight for suspected issues\n","        w = np.ones(len(y), dtype=float)\n","        w[is_issue] = 0.3\n","        self.sample_weight_ = w\n","\n","    def get_weights(self) -> np.ndarray:\n","        if self.sample_weight_ is None:\n","            raise RuntimeError(\"Call fit() first\")\n","        return self.sample_weight_\n","\n","    def fit_clean_model(self, X, y):\n","        \"\"\"Train a final classifier using the learned sample weights.\"\"\"\n","        w = self.get_weights()\n","        clf = self.clf_factory()\n","        clf.fit(X, y, sample_weight=w)\n","        return clf"],"metadata":{"id":"Mm12LAlMKVO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleaner = NoisyDatasetCleaner(strategy=\"bandit_weight\")\n","cleaner.fit(X_train, y_train)\n","weights = cleaner.get_weights()\n","\n","clf_clean = cleaner.fit_clean_model(X_train, y_train)\n","f1_clean = eval_model(clf_clean, X_val, y_val)\n","print(\"F1 with NoisyDatasetCleaner (bandit_weight):\", f1_clean)\n","\n","# Or:\n","cleaner_cl = NoisyDatasetCleaner(strategy=\"cleanlab\")\n","cleaner_cl.fit(X_train, y_train)\n","clf_cleanlab = cleaner_cl.fit_clean_model(X_train, y_train)\n","print(\"F1 with NoisyDatasetCleaner (cleanlab):\", eval_model(clf_cleanlab, X_val, y_val))"],"metadata":{"id":"pSbwmU1Cvtc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# old dont run\n","from typing import Optional, Literal, Dict, Any, Tuple, Callable\n","\n","class NoisyDatasetCleaner:\n","    \"\"\"\n","    A generic wrapper for noisy supervised datasets.\n","    Strategies:\n","      - 'bandit_weight'\n","      - 'cleanlab'\n","      - 'none'\n","    You can extend it with active-relabelling logic.\n","    \"\"\"\n","    def __init__(self,\n","                 strategy: Literal[\"none\", \"bandit_weight\", \"cleanlab\"] = \"none\",\n","                 clf_factory: Callable[[], Any] = None,\n","                 strategy_kwargs: Optional[Dict[str, Any]] = None):\n","        self.strategy = strategy\n","        self.clf_factory = clf_factory or (lambda: LogisticRegression(max_iter=300, n_jobs=-1))\n","        self.strategy_kwargs = strategy_kwargs or {}\n","        self.sample_weight_ = None\n","        self._cleanlab_model = None\n","\n","    def fit(self, X, y) -> \"NoisyDatasetCleaner\":\n","        if self.strategy == \"none\":\n","            self.sample_weight_ = np.ones(len(y), dtype=float)\n","\n","        elif self.strategy == \"bandit_weight\":\n","            self._fit_bandit_weight(X, y)\n","\n","        elif self.strategy == \"cleanlab\":\n","            self._fit_cleanlab(X, y)\n","\n","        else:\n","            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n","\n","        return self\n","\n","    def _fit_bandit_weight(self, X, y):\n","        # train base model\n","        base_clf = self.clf_factory()\n","        base_clf.fit(X, y)\n","        p = base_clf.predict_proba(X)[:, 1]\n","        conf = np.abs(p - 0.5)\n","\n","        # simple discrete bandit over thresholds, like earlier\n","        candidate_policies = self.strategy_kwargs.get(\"candidate_policies\") or [\n","            {\"theta\": 0.05, \"noisy_weight\": 0.5},\n","            {\"theta\": 0.10, \"noisy_weight\": 0.5},\n","            {\"theta\": 0.10, \"noisy_weight\": 0.2},\n","            {\"theta\": 0.15, \"noisy_weight\": 0.2},\n","            {\"theta\": 0.20, \"noisy_weight\": 0.0},\n","            {\"theta\": 0.30, \"noisy_weight\": 0.0},\n","        ]\n","        n_trials = self.strategy_kwargs.get(\"n_trials\", 10)\n","        epsilon  = self.strategy_kwargs.get(\"epsilon\", 0.3)\n","        X_train, X_val, y_train, y_val, conf_train = train_test_split(\n","            X, y, conf, test_size=0.2, random_state=0, stratify=y\n","        )\n","\n","        def apply_policy(policy, conf_vec):\n","            w = np.ones_like(conf_vec, dtype=float)\n","            w[conf_vec < policy[\"theta\"]] = policy[\"noisy_weight\"]\n","            return w\n","\n","        K = len(candidate_policies)\n","        Q = np.zeros(K)\n","        N = np.zeros(K)\n","\n","        for _ in range(n_trials):\n","            if np.random.rand() < epsilon:\n","                k = np.random.randint(K)\n","            else:\n","                k = np.argmax(Q)\n","\n","            policy = candidate_policies[k]\n","            weights_train = apply_policy(policy, conf_train)\n","\n","            clf = self.clf_factory()\n","            clf.fit(X_train, y_train, sample_weight=weights_train)\n","            y_pred_val = clf.predict(X_val)\n","            reward = f1_score(y_val, y_pred_val)\n","\n","            N[k] += 1\n","            Q[k] += (reward - Q[k]) / N[k]\n","\n","        best_k = np.argmax(Q)\n","        best_policy = candidate_policies[best_k]\n","\n","        # final weights on full data\n","        self.sample_weight_ = apply_policy(best_policy, conf)\n","\n","    def _fit_cleanlab(self, X, y):\n","        from cleanlab.classification import CleanLearning\n","\n","        base_clf = self.clf_factory()\n","        cl = CleanLearning(clf=base_clf)\n","        cl.fit(X, y)\n","        self._cleanlab_model = cl\n","\n","        # label issues summary -> create weights\n","        issues = cl.get_label_issues()\n","        is_issue = issues[\"is_label_issue\"].values\n","        # simple scheme: 0.3 weight for suspected issues\n","        w = np.ones(len(y), dtype=float)\n","        w[is_issue] = 0.3\n","        self.sample_weight_ = w\n","\n","    def get_weights(self) -> np.ndarray:\n","        if self.sample_weight_ is None:\n","            raise RuntimeError(\"Call fit() first\")\n","        return self.sample_weight_\n","\n","    def fit_clean_model(self, X, y):\n","        \"\"\"Train a final classifier using the learned sample weights.\"\"\"\n","        w = self.get_weights()\n","        clf = self.clf_factory()\n","        clf.fit(X, y, sample_weight=w)\n","        return clf"],"metadata":{"id":"tdfIWEuqKVRk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IPhU0xHiKVUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tinyllama_server_kv_stream.py\n","import asyncio\n","import json\n","import time\n","import uuid\n","from typing import List, Optional\n","\n","import torch\n","from fastapi import FastAPI\n","from fastapi.responses import StreamingResponse\n","from pydantic import BaseModel\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# ---------------------------------------------------------\n","# Model load\n","# ---------------------------------------------------------\n","MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","\n","print(f\"Loading model: {MODEL_NAME}\")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","dtype = torch.float16 if device == \"cuda\" else torch.float32\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    torch_dtype=dtype,\n","    device_map=\"auto\" if device == \"cuda\" else None,\n",")\n","model.to(device)\n","model.eval()\n","\n","# ---------------------------------------------------------\n","# FastAPI + schemas\n","# ---------------------------------------------------------\n","app = FastAPI(title=\"TinyLlama KV streaming demo\")\n","\n","class ChatMessage(BaseModel):\n","    role: str\n","    content: str\n","\n","class ChatRequest(BaseModel):\n","    model: Optional[str] = None\n","    messages: List[ChatMessage]\n","    max_tokens: int = 128\n","    temperature: float = 0.7\n","    top_p: float = 1.0\n","    n: int = 1\n","    stream: bool = False\n","    stop: Optional[List[str]] = None\n","\n","# ---------------------------------------------------------\n","# Prompt formatting\n","# ---------------------------------------------------------\n","def build_prompt(messages: List[ChatMessage]) -> str:\n","    parts = []\n","    for m in messages:\n","        if m.role == \"user\":\n","            parts.append(f\"User: {m.content}\")\n","        elif m.role == \"assistant\":\n","            parts.append(f\"Assistant: {m.content}\")\n","        else:\n","            parts.append(f\"{m.role.capitalize()}: {m.content}\")\n","    parts.append(\"Assistant:\")\n","    return \"\\n\".join(parts)\n","\n","# ---------------------------------------------------------\n","# Sampling helpers\n","# ---------------------------------------------------------\n","\n","def sample_next_token(\n","    logits: torch.Tensor,\n","    temperature: float,\n","    top_p: float,\n",") -> int:\n","    \"\"\"\n","    logits: [vocab_size] (for a single position)\n","    returns: int token id\n","    \"\"\"\n","    if temperature <= 0:\n","        # greedy\n","        return int(torch.argmax(logits, dim=-1).item())\n","\n","    # temperature scaling\n","    logits = logits / temperature\n","\n","    # softmax to probs\n","    probs = torch.softmax(logits, dim=-1)\n","\n","    if top_p < 1.0:\n","        # nucleus sampling\n","        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n","        cumulative = torch.cumsum(sorted_probs, dim=-1)\n","\n","        # keep minimal set that sums to >= top_p\n","        mask = cumulative - sorted_probs > top_p\n","        sorted_probs[mask] = 0\n","        sorted_probs = sorted_probs / sorted_probs.sum()\n","        idx = torch.multinomial(sorted_probs, 1)\n","        token_id = sorted_indices[idx]\n","        return int(token_id.item())\n","    else:\n","        # plain multinomial over full vocab\n","        token_id = torch.multinomial(probs, 1)\n","        return int(token_id.item())\n","\n","# ---------------------------------------------------------\n","# Non-streaming (simple single-call generate)\n","# ---------------------------------------------------------\n","\n","async def handle_non_stream(req: ChatRequest):\n","    prompt = build_prompt(req.messages)\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        gen_ids = model.generate(\n","            **inputs,\n","            max_new_tokens=req.max_tokens,\n","            temperature=req.temperature,\n","            top_p=req.top_p,\n","            do_sample=req.temperature > 0,\n","            pad_token_id=tokenizer.pad_token_id,\n","            eos_token_id=tokenizer.eos_token_id,\n","        )\n","\n","    input_len = inputs[\"input_ids\"].shape[1]\n","    new_tokens = gen_ids[0, input_len:]\n","    text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n","\n","    prompt_tokens = int(inputs[\"input_ids\"].numel())\n","    completion_tokens = int(new_tokens.numel())\n","    total_tokens = prompt_tokens + completion_tokens\n","\n","    resp = {\n","        \"id\": f\"chatcmpl-{uuid.uuid4().hex}\",\n","        \"object\": \"chat.completion\",\n","        \"created\": int(time.time()),\n","        \"model\": req.model or MODEL_NAME,\n","        \"choices\": [\n","            {\n","                \"index\": 0,\n","                \"message\": {\"role\": \"assistant\", \"content\": text},\n","                \"finish_reason\": \"stop\",\n","            }\n","        ],\n","        \"usage\": {\n","            \"prompt_tokens\": prompt_tokens,\n","            \"completion_tokens\": completion_tokens,\n","            \"total_tokens\": total_tokens,\n","        },\n","    }\n","    return resp\n","\n","# ---------------------------------------------------------\n","# Real per-token streaming with KV cache\n","# ---------------------------------------------------------\n","\n","async def handle_stream(req: ChatRequest):\n","    prompt = build_prompt(req.messages)\n","    request_id = f\"chatcmpl-{uuid.uuid4().hex}\"\n","    model_name = req.model or MODEL_NAME\n","    created = int(time.time())\n","\n","    async def event_stream():\n","        # 1) initial input: full prompt\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","        input_ids = inputs[\"input_ids\"]  # [1, seq_len]\n","        attention_mask = inputs[\"attention_mask\"]\n","\n","        max_new_tokens = req.max_tokens\n","        eos_id = tokenizer.eos_token_id\n","\n","        # Send initial chunk with role (OpenAI-style)\n","        first_chunk = {\n","            \"id\": request_id,\n","            \"object\": \"chat.completion.chunk\",\n","            \"created\": created,\n","            \"model\": model_name,\n","            \"choices\": [\n","                {\n","                    \"index\": 0,\n","                    \"delta\": {\"role\": \"assistant\"},\n","                    \"finish_reason\": None,\n","                }\n","            ],\n","        }\n","        yield f\"data: {json.dumps(first_chunk)}\\n\\n\"\n","\n","        past_key_values = None\n","        generated = []\n","        finish_reason = None\n","\n","        for step in range(max_new_tokens):\n","            # 2) forward pass: either full prompt (first step) or just last token (subsequent steps)\n","            with torch.no_grad():\n","                outputs = model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    past_key_values=past_key_values,\n","                    use_cache=True,\n","                )\n","\n","            logits = outputs.logits[:, -1, :]  # [1, vocab]\n","            past_key_values = outputs.past_key_values\n","\n","            # 3) sample next token\n","            next_token_id = sample_next_token(\n","                logits[0], req.temperature, req.top_p\n","            )\n","            generated.append(next_token_id)\n","\n","            if next_token_id == eos_id:\n","                finish_reason = \"stop\"\n","                break\n","\n","            # 4) decode just this token to text piece\n","            token_text = tokenizer.decode([next_token_id], skip_special_tokens=True)\n","\n","            if token_text:\n","                chunk = {\n","                    \"id\": request_id,\n","                    \"object\": \"chat.completion.chunk\",\n","                    \"created\": created,\n","                    \"model\": model_name,\n","                    \"choices\": [\n","                        {\n","                            \"index\": 0,\n","                            \"delta\": {\"content\": token_text},\n","                            \"finish_reason\": None,\n","                        }\n","                    ],\n","                }\n","                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n","\n","            # 5) prepare for next step: feed only this token, no need to resend the whole prompt\n","            input_ids = torch.tensor([[next_token_id]], device=device)\n","            attention_mask = None  # not strictly needed when using past with single token\n","\n","            # let event loop breathe a bit\n","            await asyncio.sleep(0)\n","\n","        if finish_reason is None:\n","            finish_reason = \"length\"\n","\n","        # final empty delta with finish_reason\n","        done_chunk = {\n","            \"id\": request_id,\n","            \"object\": \"chat.completion.chunk\",\n","            \"created\": created,\n","            \"model\": model_name,\n","            \"choices\": [\n","                {\n","                    \"index\": 0,\n","                    \"delta\": {},\n","                    \"finish_reason\": finish_reason,\n","                }\n","            ],\n","        }\n","        yield f\"data: {json.dumps(done_chunk)}\\n\\n\"\n","        yield \"data: [DONE]\\n\\n\"\n","\n","    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n","\n","# ---------------------------------------------------------\n","# Main endpoint\n","# ---------------------------------------------------------\n","\n","@app.post(\"/v1/chat/completions\")\n","async def chat_completions(req: ChatRequest):\n","    if req.stream:\n","        return await handle_stream(req)\n","    else:\n","        return await handle_non_stream(req)"],"metadata":{"id":"O1TEubioKVYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install \"fastapi[all]\" uvicorn transformers torch\n","uvicorn tinyllama_server_kv_stream:app --host 0.0.0.0 --port 9000"],"metadata":{"id":"zPSNw6Z-KVa8"},"execution_count":null,"outputs":[]}]}