{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMrxmMg44IfO15WjKtG94nY"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dougc333/Colab-Notebooks/blob/main/broadcast_2_cuda.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Broadcasting CUDA**"
   ],
   "metadata": {
    "id": "A_XOWlW0SlxZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oz8OAXqgtK9d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767135390526,
     "user_tz": 480,
     "elapsed": 16463,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "3eec5c70-f270-4cc1-e121-58c17ce6fbf5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/drive/MyDrive/'Colab Notebooks'\n",
    "# this is saved in /content/drive/MyDrive/Colab Notebooks\n",
    "# need to move it to Colab-Notebooks which is git checked in and make sure you\n",
    "# are editing this copy"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJBcQ8_6xjW3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1767135390920,
     "user_tz": 480,
     "elapsed": 387,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "17b891e7-b7ee-44b7-d5b8-94595d8be957"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "def broadcast_shape(shape_a: Tuple[int, ...],\n",
    "                    shape_b: Tuple[int, ...]) -> Tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Compute the broadcasted shape of two tensors, or raise ValueError\n",
    "    if they are not broadcastable (NumPy/PyTorch rules).\n",
    "    \"\"\"\n",
    "    # Align from the right by prepending 1s on the shorter shape\n",
    "    a = list(shape_a)\n",
    "    b = list(shape_b)\n",
    "\n",
    "    max_len = max(len(a), len(b))\n",
    "    a = [1] * (max_len - len(a)) + a\n",
    "    b = [1] * (max_len - len(b)) + b\n",
    "\n",
    "    out: List[int] = []\n",
    "    for dim_a, dim_b in zip(a, b):\n",
    "        if dim_a == dim_b or dim_a == 1 or dim_b == 1:\n",
    "            out.append(max(dim_a, dim_b))\n",
    "        else:\n",
    "            raise ValueError(f\"Shapes {shape_a} and {shape_b} are not \"\n",
    "                             f\"broadcastable: conflict {dim_a} vs {dim_b}\")\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "def explain_broadcast(shape_a: Tuple[int, ...],\n",
    "                      shape_b: Tuple[int, ...]) -> None:\n",
    "    \"\"\"\n",
    "    Print a step-by-step explanation of broadcasting between two shapes.\n",
    "    \"\"\"\n",
    "    print(f\"A shape: {shape_a}\")\n",
    "    print(f\"B shape: {shape_b}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Align with leading 1s\n",
    "    a = list(shape_a)\n",
    "    b = list(shape_b)\n",
    "    max_len = max(len(a), len(b))\n",
    "    a = [1] * (max_len - len(a)) + a\n",
    "    b = [1] * (max_len - len(b)) + b\n",
    "\n",
    "    print(\"Aligned (with leading 1s):\")\n",
    "    print(f\"A aligned: {tuple(a)}\")\n",
    "    print(f\"B aligned: {tuple(b)}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    out: List[int] = []\n",
    "    print(\"Compare dimensions (left \u2192 right):\")\n",
    "    for i, (dim_a, dim_b) in enumerate(zip(a, b)):\n",
    "        pos = i - max_len  # relative from the right: -1 is last dim\n",
    "        msg = f\"  dim {i} (from left, pos {pos:+} from right): {dim_a} vs {dim_b} -> \"\n",
    "        if dim_a == dim_b:\n",
    "            out_dim = dim_a\n",
    "            reason = \"same, keep\"\n",
    "        elif dim_a == 1:\n",
    "            out_dim = dim_b\n",
    "            reason = \"A expands to match B\"\n",
    "        elif dim_b == 1:\n",
    "            out_dim = dim_a\n",
    "            reason = \"B expands to match A\"\n",
    "        else:\n",
    "            print(msg + \"\u274c conflict (not broadcastable)\")\n",
    "            print()\n",
    "            print(f\"Result: shapes {shape_a} and {shape_b} are NOT broadcastable.\")\n",
    "            return\n",
    "        out.append(out_dim)\n",
    "        print(msg + f\"\u2714 {out_dim} ({reason})\")\n",
    "\n",
    "    out_shape = tuple(out)\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Broadcasted shape: {out_shape}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# --------- quick tests / examples ---------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Your classic (B,T,1) \u00d7 (B,1,D)\n",
    "    B, T, D = 2, 4, 8\n",
    "    explain_broadcast((B, T, 1), (B, 1, D))\n",
    "\n",
    "    # 2) (B,T,D) + (D,)\n",
    "    explain_broadcast((B, T, D), (D,))\n",
    "\n",
    "    # 3) (B,T) \u00d7 (D,) \u2013 usually invalid unless T == D\n",
    "    try:\n",
    "        explain_broadcast((B, T), (D,))\n",
    "    except Exception as e:\n",
    "        print(\"Caught error:\", e)\n",
    "\n",
    "    # 4) Quick programmatic use:\n",
    "    print(\"broadcast_shape((3, 1), (1, 4)) ->\",\n",
    "          broadcast_shape((3, 1), (1, 4)))\n"
   ],
   "metadata": {
    "id": "BBwPGRCcE73k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764529894656,
     "user_tz": 480,
     "elapsed": 14,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "c9f3de19-76f2-4ded-b3a7-59da0b74b3b8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A shape: (2, 4, 1)\n",
      "B shape: (2, 1, 8)\n",
      "----------------------------------------\n",
      "Aligned (with leading 1s):\n",
      "A aligned: (2, 4, 1)\n",
      "B aligned: (2, 1, 8)\n",
      "----------------------------------------\n",
      "Compare dimensions (left \u2192 right):\n",
      "  dim 0 (from left, pos -3 from right): 2 vs 2 -> \u2714 2 (same, keep)\n",
      "  dim 1 (from left, pos -2 from right): 4 vs 1 -> \u2714 4 (B expands to match A)\n",
      "  dim 2 (from left, pos -1 from right): 1 vs 8 -> \u2714 8 (A expands to match B)\n",
      "----------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n",
      "A shape: (2, 4, 8)\n",
      "B shape: (8,)\n",
      "----------------------------------------\n",
      "Aligned (with leading 1s):\n",
      "A aligned: (2, 4, 8)\n",
      "B aligned: (1, 1, 8)\n",
      "----------------------------------------\n",
      "Compare dimensions (left \u2192 right):\n",
      "  dim 0 (from left, pos -3 from right): 2 vs 1 -> \u2714 2 (B expands to match A)\n",
      "  dim 1 (from left, pos -2 from right): 4 vs 1 -> \u2714 4 (B expands to match A)\n",
      "  dim 2 (from left, pos -1 from right): 8 vs 8 -> \u2714 8 (same, keep)\n",
      "----------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n",
      "A shape: (2, 4)\n",
      "B shape: (8,)\n",
      "----------------------------------------\n",
      "Aligned (with leading 1s):\n",
      "A aligned: (2, 4)\n",
      "B aligned: (1, 8)\n",
      "----------------------------------------\n",
      "Compare dimensions (left \u2192 right):\n",
      "  dim 0 (from left, pos -2 from right): 2 vs 1 -> \u2714 2 (B expands to match A)\n",
      "  dim 1 (from left, pos -1 from right): 4 vs 8 -> \u274c conflict (not broadcastable)\n",
      "\n",
      "Result: shapes (2, 4) and (8,) are NOT broadcastable.\n",
      "broadcast_shape((3, 1), (1, 4)) -> (3, 4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "B, T, D = 2, 4, 8\n",
    "explain_broadcast((B, T, 1), (B, 1, D))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ijNHOiBLXYA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764529961278,
     "user_tz": 480,
     "elapsed": 9,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "54237c22-0c86-4914-a929-7c56b5ecd489"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A shape: (2, 4, 1)\n",
      "B shape: (2, 1, 8)\n",
      "----------------------------------------\n",
      "Aligned (with leading 1s):\n",
      "A aligned: (2, 4, 1)\n",
      "B aligned: (2, 1, 8)\n",
      "----------------------------------------\n",
      "Compare dimensions (left \u2192 right):\n",
      "  dim 0 (from left, pos -3 from right): 2 vs 2 -> \u2714 2 (same, keep)\n",
      "  dim 1 (from left, pos -2 from right): 4 vs 1 -> \u2714 4 (B expands to match A)\n",
      "  dim 2 (from left, pos -1 from right): 1 vs 8 -> \u2714 8 (A expands to match B)\n",
      "----------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Sequence\n",
    "\n",
    "Shape = Tuple[int, ...]\n",
    "\n",
    "\n",
    "def broadcast_shape_many(shapes: Sequence[Shape]) -> Shape:\n",
    "    \"\"\"\n",
    "    Compute the broadcasted shape of N tensors (NumPy/PyTorch rules).\n",
    "\n",
    "    shapes: iterable of shapes, e.g. [(B,T,1), (B,1,D), (1,T,1)]\n",
    "    returns: single broadcasted shape, or raises ValueError if incompatible.\n",
    "    \"\"\"\n",
    "    if not shapes:\n",
    "        return ()\n",
    "\n",
    "    # Step 1: compute max rank and left-pad each shape with 1s\n",
    "    max_len = max(len(s) for s in shapes)\n",
    "    aligned: List[List[int]] = []\n",
    "    for s in shapes:\n",
    "        padded = [1] * (max_len - len(s)) + list(s)\n",
    "        aligned.append(padded)\n",
    "\n",
    "    out: List[int] = []\n",
    "    # Step 2: for each dimension (left \u2192 right, but rules are symmetric)\n",
    "    for dim_idx in range(max_len):\n",
    "        dims_here = [a[dim_idx] for a in aligned]\n",
    "        non_ones = sorted({d for d in dims_here if d != 1})\n",
    "\n",
    "        if len(non_ones) == 0:\n",
    "            # all 1s \u2192 result is 1\n",
    "            out_dim = 1\n",
    "        elif len(non_ones) == 1:\n",
    "            # all non-1 dims agree \u2192 that is the result dim\n",
    "            out_dim = non_ones[0]\n",
    "        else:\n",
    "            # more than one distinct non-1 size \u2192 conflict\n",
    "            raise ValueError(\n",
    "                f\"Incompatible shapes at dim {dim_idx}: {dims_here} \"\n",
    "                f\"(non-1 dims {non_ones})\"\n",
    "            )\n",
    "        out.append(out_dim)\n",
    "\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "def explain_broadcast_many(shapes: Sequence[Shape]) -> None:\n",
    "    \"\"\"\n",
    "    Verbose explanation of N-way broadcasting.\n",
    "    \"\"\"\n",
    "    print(\"Input shapes:\")\n",
    "    for i, s in enumerate(shapes):\n",
    "        print(f\"  Tensor {i}: {s}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    if not shapes:\n",
    "        print(\"No shapes given \u2192 result shape is ().\")\n",
    "        return\n",
    "\n",
    "    max_len = max(len(s) for s in shapes)\n",
    "    aligned: List[List[int]] = []\n",
    "    for s in shapes:\n",
    "        padded = [1] * (max_len - len(s)) + list(s)\n",
    "        aligned.append(padded)\n",
    "\n",
    "    print(\"Aligned with leading 1s (so all have same rank):\")\n",
    "    for i, a in enumerate(aligned):\n",
    "        print(f\"  Tensor {i} aligned: {tuple(a)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    out: List[int] = []\n",
    "    print(\"Per-dimension analysis (left \u2192 right):\")\n",
    "    for dim_idx in range(max_len):\n",
    "        dims_here = [a[dim_idx] for a in aligned]\n",
    "        non_ones = sorted({d for d in dims_here if d != 1})\n",
    "\n",
    "        print(f\"\\nDim {dim_idx} (from left):\")\n",
    "        for i, d in enumerate(dims_here):\n",
    "            print(f\"  - Tensor {i}: {d}\")\n",
    "\n",
    "        if len(non_ones) == 0:\n",
    "            out_dim = 1\n",
    "            print(\"  -> All dims are 1 \u2192 result dim = 1\")\n",
    "        elif len(non_ones) == 1:\n",
    "            out_dim = non_ones[0]\n",
    "            expanders = [\n",
    "                i for i, d in enumerate(dims_here) if d == 1 and out_dim != 1\n",
    "            ]\n",
    "            print(f\"  -> Non-1 dims agree on {out_dim} \u2192 result dim = {out_dim}\")\n",
    "            if expanders:\n",
    "                print(f\"     Tensors {expanders} broadcast (their dim 1 expands)\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"  -> \u274c Conflict: multiple distinct non-1 dims {non_ones}. \"\n",
    "                f\"Shapes are NOT broadcastable.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        out.append(out_dim)\n",
    "\n",
    "    out_shape = tuple(out)\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"Broadcasted shape: {out_shape}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Example usage / quick tests\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    B, T, D = 2, 4, 8\n",
    "\n",
    "    # 1) Classic attention-style shapes: (B,T,1) * (B,1,D) * (1,T,D)\n",
    "    shapes1 = [(B, T, 1), (B, 1, D), (1, T, D)]\n",
    "    explain_broadcast_many(shapes1)\n",
    "    print(\"broadcast_shape_many:\", broadcast_shape_many(shapes1))\n",
    "    print()\n",
    "\n",
    "    # 2) (B,T,D) + (D,) + (1,1,D)\n",
    "    # (D,) is the same as\n",
    "    shapes2 = [(B, T, D), (D,), (1, 1, D)]\n",
    "    explain_broadcast_many(shapes2)\n",
    "    print(\"broadcast_shape_many:\", broadcast_shape_many(shapes2))\n",
    "    print()\n",
    "\n",
    "    # 3) Incompatible case: (B,T) and (D,2)\n",
    "    shapes3 = [(B, T), (D, 2)]\n",
    "    try:\n",
    "        explain_broadcast_many(shapes3)\n",
    "        print(\"broadcast_shape_many:\", broadcast_shape_many(shapes3))\n",
    "    except ValueError as e:\n",
    "        print(\"Caught error:\", e)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-wVKIOFL5YD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764530037754,
     "user_tz": 480,
     "elapsed": 14,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "98d43d65-3810-46dc-aaef-cd8257d719f0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input shapes:\n",
      "  Tensor 0: (2, 4, 1)\n",
      "  Tensor 1: (2, 1, 8)\n",
      "  Tensor 2: (1, 4, 8)\n",
      "------------------------------------------------------------\n",
      "Aligned with leading 1s (so all have same rank):\n",
      "  Tensor 0 aligned: (2, 4, 1)\n",
      "  Tensor 1 aligned: (2, 1, 8)\n",
      "  Tensor 2 aligned: (1, 4, 8)\n",
      "------------------------------------------------------------\n",
      "Per-dimension analysis (left \u2192 right):\n",
      "\n",
      "Dim 0 (from left):\n",
      "  - Tensor 0: 2\n",
      "  - Tensor 1: 2\n",
      "  - Tensor 2: 1\n",
      "  -> Non-1 dims agree on 2 \u2192 result dim = 2\n",
      "     Tensors [2] broadcast (their dim 1 expands)\n",
      "\n",
      "Dim 1 (from left):\n",
      "  - Tensor 0: 4\n",
      "  - Tensor 1: 1\n",
      "  - Tensor 2: 4\n",
      "  -> Non-1 dims agree on 4 \u2192 result dim = 4\n",
      "     Tensors [1] broadcast (their dim 1 expands)\n",
      "\n",
      "Dim 2 (from left):\n",
      "  - Tensor 0: 1\n",
      "  - Tensor 1: 8\n",
      "  - Tensor 2: 8\n",
      "  -> Non-1 dims agree on 8 \u2192 result dim = 8\n",
      "     Tensors [0] broadcast (their dim 1 expands)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n",
      "broadcast_shape_many: (2, 4, 8)\n",
      "\n",
      "Input shapes:\n",
      "  Tensor 0: (2, 4, 8)\n",
      "  Tensor 1: (8,)\n",
      "  Tensor 2: (1, 1, 8)\n",
      "------------------------------------------------------------\n",
      "Aligned with leading 1s (so all have same rank):\n",
      "  Tensor 0 aligned: (2, 4, 8)\n",
      "  Tensor 1 aligned: (1, 1, 8)\n",
      "  Tensor 2 aligned: (1, 1, 8)\n",
      "------------------------------------------------------------\n",
      "Per-dimension analysis (left \u2192 right):\n",
      "\n",
      "Dim 0 (from left):\n",
      "  - Tensor 0: 2\n",
      "  - Tensor 1: 1\n",
      "  - Tensor 2: 1\n",
      "  -> Non-1 dims agree on 2 \u2192 result dim = 2\n",
      "     Tensors [1, 2] broadcast (their dim 1 expands)\n",
      "\n",
      "Dim 1 (from left):\n",
      "  - Tensor 0: 4\n",
      "  - Tensor 1: 1\n",
      "  - Tensor 2: 1\n",
      "  -> Non-1 dims agree on 4 \u2192 result dim = 4\n",
      "     Tensors [1, 2] broadcast (their dim 1 expands)\n",
      "\n",
      "Dim 2 (from left):\n",
      "  - Tensor 0: 8\n",
      "  - Tensor 1: 8\n",
      "  - Tensor 2: 8\n",
      "  -> Non-1 dims agree on 8 \u2192 result dim = 8\n",
      "\n",
      "------------------------------------------------------------\n",
      "Broadcasted shape: (2, 4, 8)\n",
      "\n",
      "broadcast_shape_many: (2, 4, 8)\n",
      "\n",
      "Input shapes:\n",
      "  Tensor 0: (2, 4)\n",
      "  Tensor 1: (8, 2)\n",
      "------------------------------------------------------------\n",
      "Aligned with leading 1s (so all have same rank):\n",
      "  Tensor 0 aligned: (2, 4)\n",
      "  Tensor 1 aligned: (8, 2)\n",
      "------------------------------------------------------------\n",
      "Per-dimension analysis (left \u2192 right):\n",
      "\n",
      "Dim 0 (from left):\n",
      "  - Tensor 0: 2\n",
      "  - Tensor 1: 8\n",
      "  -> \u274c Conflict: multiple distinct non-1 dims [2, 8]. Shapes are NOT broadcastable.\n",
      "Caught error: Incompatible shapes at dim 0: [2, 8] (non-1 dims [2, 8])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Computing mean\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "Karpathy Trick behind attention\n",
    "\n",
    "44:00\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li>Summary: A mean reduces the dims of a shape like [10,10] to [10] because one of the dims becomes the mean. Use keepdims=True to get [1,10] instead of [10]. This helps to make clear it is ready for broadcasting</li>\n",
    "<li>create data pattern for debugging. Show the batch, time, Column dims</li>\n",
    "<li>We are calculating the mean per vertical column. 4,8,2 has 2 colmns so the mean is [mean col0, mean col1]</li>\n",
    "<li>Broadcasting requires a 1 as one of the dimensions or pytorch uses the shift trick to add a 1 to the arg with the smaller number of dims.  </li>\n",
    "<li>Pytorch first aligns left then adds ones until num dims match the other arg. [4,8,2] amnd [2] become [4,8,2] and [1,1,2].</li>\n",
    "<li>We use the phrase left aligned but pytorch calls it right aligned</li>\n",
    "<li>[4,8,2]</li>\n",
    "<li>[6]</li>\n",
    "<li>Unsqueeze(1) adds a 1 to the front of the shape tuple,  (6)->(1,6) and unsqueeze(-1) adds a 1 to the end of the shape vector (1,6)->(1,6,1). A unsqueeze(2) adds to the 2 index, (1,6,1)->(1,6,1,1)</li>\n",
    "<li>the 6 is in the same column as the 4. I call this left aligned but pytorch calls it right aligned.</li>\n",
    "<li>Some operations like mean, sum eliminate one of the dimensions, to keep this dim=1, use keepdims=True</li>\n",
    "<ul>"
   ],
   "metadata": {
    "id": "v8oNMPk8Hm-t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "x = torch.tensor([6])\n",
    "print(x.shape)\n",
    "x=x.unsqueeze(1)\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(-1)\n",
    "print(x.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZ6bY9CADq2f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764614186523,
     "user_tz": 480,
     "elapsed": 16,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "19f71328-3d46-476d-d5be-ec2128f12a47"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1, 1])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "x = torch.tensor([1,2,3,4,5,6], dtype=torch.float16)\n",
    "last_5=x[0:5] #this is not really right should index fro 6\n",
    "print('-------------')\n",
    "print('reverse indexing')\n",
    "print(x[-1:])\n",
    "print(x[-2:])\n",
    "print(x[-3:])\n",
    "print('-------------')\n",
    "print('foward indexing')\n",
    "print(x[:(0+1)])\n",
    "print(x[:(1+1):])\n",
    "print(x[:(2+1):])\n",
    "\n",
    "\n",
    "#print(last_5)\n",
    "#torch.mean(last_5)\n",
    "print('------------')\n",
    "print(\"easier to do forward indexing since we tokenize from l->r\")\n",
    "print(\"the below is incorrect because of the first empty array\")\n",
    "for idx in range(len(x)):\n",
    "  print(\"start from index:\",idx,\" previous tokens:\", x[:idx], \"mean:\",torch.mean(x[:idx]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"since we index from 0, we need to start from 1\")\n",
    "\n",
    "for idx in range(len(x)):\n",
    "  print(\"start from index+1:\",idx+1,\" previous tokens:\", x[:idx+1], \"mean:\",torch.mean(x[:idx+1]))\n",
    "\n",
    "#convert to tensor x(B,T,C) convention\n",
    "B,T,C = 4,8,2\n",
    "\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x[b, t, 0] = b     # channel 0 shows batch index\n",
    "        x[b, t, 1] = t     # channel 1 shows timestep index\n",
    "\n",
    "print('----------')\n",
    "print(x)\n",
    "print('----------')\n",
    "\n",
    "for b in range(B):\n",
    "  for t in range(T):\n",
    "    xprev = x[b,:t+1] #(t,C)\n",
    "    xbow = torch.mean(xprev,0) #the 0 means 0 dimension, which is t because (t,C)\n",
    "    print(f'b:{b},t:{t},xprev:{xprev}, xbow:{xbow}')\n",
    "\n",
    "#c isnt incremented [b,t] so we are stepping through rows [0,0],[0,1],[0,2],[0,3],,...\n",
    "# xprev is the previous row."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UOP5ppaQwON",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764614186742,
     "user_tz": 480,
     "elapsed": 199,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "aa8f2c11-bebc-490b-b750-dc3506f299c9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------\n",
      "reverse indexing\n",
      "tensor([6.], dtype=torch.float16)\n",
      "tensor([5., 6.], dtype=torch.float16)\n",
      "tensor([4., 5., 6.], dtype=torch.float16)\n",
      "-------------\n",
      "foward indexing\n",
      "tensor([1.], dtype=torch.float16)\n",
      "tensor([1., 2.], dtype=torch.float16)\n",
      "tensor([1., 2., 3.], dtype=torch.float16)\n",
      "------------\n",
      "easier to do forward indexing since we tokenize from l->r\n",
      "the below is incorrect because of the first empty array\n",
      "start from index: 0  previous tokens: tensor([], dtype=torch.float16) mean: tensor(nan, dtype=torch.float16)\n",
      "start from index: 1  previous tokens: tensor([1.], dtype=torch.float16) mean: tensor(1., dtype=torch.float16)\n",
      "start from index: 2  previous tokens: tensor([1., 2.], dtype=torch.float16) mean: tensor(1.5000, dtype=torch.float16)\n",
      "start from index: 3  previous tokens: tensor([1., 2., 3.], dtype=torch.float16) mean: tensor(2., dtype=torch.float16)\n",
      "start from index: 4  previous tokens: tensor([1., 2., 3., 4.], dtype=torch.float16) mean: tensor(2.5000, dtype=torch.float16)\n",
      "start from index: 5  previous tokens: tensor([1., 2., 3., 4., 5.], dtype=torch.float16) mean: tensor(3., dtype=torch.float16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "since we index from 0, we need to start from 1\n",
      "start from index+1: 1  previous tokens: tensor([1.], dtype=torch.float16) mean: tensor(1., dtype=torch.float16)\n",
      "start from index+1: 2  previous tokens: tensor([1., 2.], dtype=torch.float16) mean: tensor(1.5000, dtype=torch.float16)\n",
      "start from index+1: 3  previous tokens: tensor([1., 2., 3.], dtype=torch.float16) mean: tensor(2., dtype=torch.float16)\n",
      "start from index+1: 4  previous tokens: tensor([1., 2., 3., 4.], dtype=torch.float16) mean: tensor(2.5000, dtype=torch.float16)\n",
      "start from index+1: 5  previous tokens: tensor([1., 2., 3., 4., 5.], dtype=torch.float16) mean: tensor(3., dtype=torch.float16)\n",
      "start from index+1: 6  previous tokens: tensor([1., 2., 3., 4., 5., 6.], dtype=torch.float16) mean: tensor(3.5000, dtype=torch.float16)\n",
      "----------\n",
      "tensor([[[0., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 2.],\n",
      "         [0., 3.],\n",
      "         [0., 4.],\n",
      "         [0., 5.],\n",
      "         [0., 6.],\n",
      "         [0., 7.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [1., 1.],\n",
      "         [1., 2.],\n",
      "         [1., 3.],\n",
      "         [1., 4.],\n",
      "         [1., 5.],\n",
      "         [1., 6.],\n",
      "         [1., 7.]],\n",
      "\n",
      "        [[2., 0.],\n",
      "         [2., 1.],\n",
      "         [2., 2.],\n",
      "         [2., 3.],\n",
      "         [2., 4.],\n",
      "         [2., 5.],\n",
      "         [2., 6.],\n",
      "         [2., 7.]],\n",
      "\n",
      "        [[3., 0.],\n",
      "         [3., 1.],\n",
      "         [3., 2.],\n",
      "         [3., 3.],\n",
      "         [3., 4.],\n",
      "         [3., 5.],\n",
      "         [3., 6.],\n",
      "         [3., 7.]]])\n",
      "----------\n",
      "b:0,t:0,xprev:tensor([[0., 0.]]), xbow:tensor([0., 0.])\n",
      "b:0,t:1,xprev:tensor([[0., 0.],\n",
      "        [0., 1.]]), xbow:tensor([0.0000, 0.5000])\n",
      "b:0,t:2,xprev:tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.]]), xbow:tensor([0., 1.])\n",
      "b:0,t:3,xprev:tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.]]), xbow:tensor([0.0000, 1.5000])\n",
      "b:0,t:4,xprev:tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.]]), xbow:tensor([0., 2.])\n",
      "b:0,t:5,xprev:tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.]]), xbow:tensor([0.0000, 2.5000])\n",
      "b:0,t:6,xprev:tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.]]), xbow:tensor([0., 3.])\n",
      "b:0,t:7,xprev:tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 2.],\n",
      "        [0., 3.],\n",
      "        [0., 4.],\n",
      "        [0., 5.],\n",
      "        [0., 6.],\n",
      "        [0., 7.]]), xbow:tensor([0.0000, 3.5000])\n",
      "b:1,t:0,xprev:tensor([[1., 0.]]), xbow:tensor([1., 0.])\n",
      "b:1,t:1,xprev:tensor([[1., 0.],\n",
      "        [1., 1.]]), xbow:tensor([1.0000, 0.5000])\n",
      "b:1,t:2,xprev:tensor([[1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 2.]]), xbow:tensor([1., 1.])\n",
      "b:1,t:3,xprev:tensor([[1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 2.],\n",
      "        [1., 3.]]), xbow:tensor([1.0000, 1.5000])\n",
      "b:1,t:4,xprev:tensor([[1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 2.],\n",
      "        [1., 3.],\n",
      "        [1., 4.]]), xbow:tensor([1., 2.])\n",
      "b:1,t:5,xprev:tensor([[1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 2.],\n",
      "        [1., 3.],\n",
      "        [1., 4.],\n",
      "        [1., 5.]]), xbow:tensor([1.0000, 2.5000])\n",
      "b:1,t:6,xprev:tensor([[1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 2.],\n",
      "        [1., 3.],\n",
      "        [1., 4.],\n",
      "        [1., 5.],\n",
      "        [1., 6.]]), xbow:tensor([1., 3.])\n",
      "b:1,t:7,xprev:tensor([[1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 2.],\n",
      "        [1., 3.],\n",
      "        [1., 4.],\n",
      "        [1., 5.],\n",
      "        [1., 6.],\n",
      "        [1., 7.]]), xbow:tensor([1.0000, 3.5000])\n",
      "b:2,t:0,xprev:tensor([[2., 0.]]), xbow:tensor([2., 0.])\n",
      "b:2,t:1,xprev:tensor([[2., 0.],\n",
      "        [2., 1.]]), xbow:tensor([2.0000, 0.5000])\n",
      "b:2,t:2,xprev:tensor([[2., 0.],\n",
      "        [2., 1.],\n",
      "        [2., 2.]]), xbow:tensor([2., 1.])\n",
      "b:2,t:3,xprev:tensor([[2., 0.],\n",
      "        [2., 1.],\n",
      "        [2., 2.],\n",
      "        [2., 3.]]), xbow:tensor([2.0000, 1.5000])\n",
      "b:2,t:4,xprev:tensor([[2., 0.],\n",
      "        [2., 1.],\n",
      "        [2., 2.],\n",
      "        [2., 3.],\n",
      "        [2., 4.]]), xbow:tensor([2., 2.])\n",
      "b:2,t:5,xprev:tensor([[2., 0.],\n",
      "        [2., 1.],\n",
      "        [2., 2.],\n",
      "        [2., 3.],\n",
      "        [2., 4.],\n",
      "        [2., 5.]]), xbow:tensor([2.0000, 2.5000])\n",
      "b:2,t:6,xprev:tensor([[2., 0.],\n",
      "        [2., 1.],\n",
      "        [2., 2.],\n",
      "        [2., 3.],\n",
      "        [2., 4.],\n",
      "        [2., 5.],\n",
      "        [2., 6.]]), xbow:tensor([2., 3.])\n",
      "b:2,t:7,xprev:tensor([[2., 0.],\n",
      "        [2., 1.],\n",
      "        [2., 2.],\n",
      "        [2., 3.],\n",
      "        [2., 4.],\n",
      "        [2., 5.],\n",
      "        [2., 6.],\n",
      "        [2., 7.]]), xbow:tensor([2.0000, 3.5000])\n",
      "b:3,t:0,xprev:tensor([[3., 0.]]), xbow:tensor([3., 0.])\n",
      "b:3,t:1,xprev:tensor([[3., 0.],\n",
      "        [3., 1.]]), xbow:tensor([3.0000, 0.5000])\n",
      "b:3,t:2,xprev:tensor([[3., 0.],\n",
      "        [3., 1.],\n",
      "        [3., 2.]]), xbow:tensor([3., 1.])\n",
      "b:3,t:3,xprev:tensor([[3., 0.],\n",
      "        [3., 1.],\n",
      "        [3., 2.],\n",
      "        [3., 3.]]), xbow:tensor([3.0000, 1.5000])\n",
      "b:3,t:4,xprev:tensor([[3., 0.],\n",
      "        [3., 1.],\n",
      "        [3., 2.],\n",
      "        [3., 3.],\n",
      "        [3., 4.]]), xbow:tensor([3., 2.])\n",
      "b:3,t:5,xprev:tensor([[3., 0.],\n",
      "        [3., 1.],\n",
      "        [3., 2.],\n",
      "        [3., 3.],\n",
      "        [3., 4.],\n",
      "        [3., 5.]]), xbow:tensor([3.0000, 2.5000])\n",
      "b:3,t:6,xprev:tensor([[3., 0.],\n",
      "        [3., 1.],\n",
      "        [3., 2.],\n",
      "        [3., 3.],\n",
      "        [3., 4.],\n",
      "        [3., 5.],\n",
      "        [3., 6.]]), xbow:tensor([3., 3.])\n",
      "b:3,t:7,xprev:tensor([[3., 0.],\n",
      "        [3., 1.],\n",
      "        [3., 2.],\n",
      "        [3., 3.],\n",
      "        [3., 4.],\n",
      "        [3., 5.],\n",
      "        [3., 6.],\n",
      "        [3., 7.]]), xbow:tensor([3.0000, 3.5000])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Embeddng, B,T,C definitions\n",
    "\n",
    "<ul>\n",
    "<li>\n",
    "given an embedding , 768 how does this become B,T,C, is B a collection of 768 vectorx, T is the row, C are the columns so C = 768. We have to create B,T\n",
    "</li>\n",
    "<li>\n",
    "T = sequence length. and B is number of sequences processed in GPU memory at one time.\n",
    "</li>\n",
    "<li>\n",
    "\n",
    "attention splits C to num_heads.\n",
    "</li>\n",
    "<li>\n",
    "num_heads is a predefined constant\n",
    "num_heads=H;\n",
    "number of attention heads is C/H. this isnt the same as num_heads\n",
    "num_attention_heads = C/H\n",
    "</li>\n",
    "<li>\n",
    "head_dim = C/H, 768/16\n",
    "</li>\n",
    "<li>\n",
    "K_cache: (B, T, H, head_dim)\n",
    "V_cache: (B, T, H, head_dim)\n",
    "</li>\n",
    "\n",
    "<ul>"
   ],
   "metadata": {
    "id": "OTmn1dQpjS6v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Matrix Multiplies\n",
    "\n",
    "<ul>\n",
    "<li>Matrix multiply by identity matrix proudces sums and averages. [2,7], [6,4], [6,5] produce column sums by matrix multiply with identity matrix</li>\n",
    "</ul>"
   ],
   "metadata": {
    "id": "yPPLS_RN8qNp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# karpathy replicating medians in (B,T,C) format with triangular matrix multiply\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('----')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('----')\n",
    "print('c=')\n",
    "print(c)\n",
    "print('---end multiply w identiy matrix---')\n",
    "\n",
    "# can see matrix multiply with Identiy matrix and data produces sums in the columns. Column sums 2+6+6=16, 7+4+5=16\n",
    "# the matrix multiply is a sum when we take the dot product. First row [1,1,1] * first col [2,6,6] gives sum 2+6+6=14,\n",
    "\n",
    "# second step take the lower triangular matrix instead if Identity matrix. This adds 0s\n",
    "print('---replace with lower triangular and make rows sum to 1---')\n",
    "print('  ')\n",
    "a = torch.tril(torch.ones(3,3))/torch.sum(a,1,keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('----')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('----')\n",
    "print('c=')\n",
    "print(c)\n",
    "print('---end multiply w lower triagular rows normalzied to sum 1---')\n"
   ],
   "metadata": {
    "id": "GipzwfmKhI-L",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764651093562,
     "user_tz": 480,
     "elapsed": 9,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "81f0709d-0862-4683-9891-d5e5e96b3d48"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "----\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "---end multiply w identiy matrix---\n",
      "---replace with lower triangular and make rows sum to 1---\n",
      "  \n",
      "a=\n",
      "tensor([[0.3333, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "----\n",
      "b=\n",
      "tensor([[0., 4.],\n",
      "        [0., 3.],\n",
      "        [8., 4.]])\n",
      "----\n",
      "c=\n",
      "tensor([[0.0000, 1.3333],\n",
      "        [0.0000, 2.3333],\n",
      "        [2.6667, 3.6667]])\n",
      "---end multiply w lower triagular rows normalzied to sum 1---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x=torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x, x.shape)\n",
    "#y = x/torch.sum(x,dim=1)\n",
    "print(torch.sum(x, dim=0, keepdim=True))\n",
    "print(torch.sum(x, dim=0, keepdim=False))\n",
    "print(torch.sum(x, dim=1, keepdim=True))\n",
    "print(torch.sum(x, dim=1, keepdim=False))\n",
    "#print(s)\n",
    "print(torch.sum(x))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucxpn5RQZ6nM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764652076726,
     "user_tz": 480,
     "elapsed": 7,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "6de78ca2-5167-4744-c266-39feeb434cb3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]) torch.Size([2, 3])\n",
      "tensor([[5, 7, 9]])\n",
      "tensor([5, 7, 9])\n",
      "tensor([[ 6],\n",
      "        [15]])\n",
      "tensor([ 6, 15])\n",
      "tensor([[3],\n",
      "        [7]])\n",
      "tensor(21)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Matrix and Vector multiply for averages.**\n",
    "\n",
    "There are different versions of averages which are used for normalization. Karpathy develops the t-1 or the autoregresseive average where the average of the sequence of a,b,c,d has 4 averages; at t=0, avg=a, t=1 avg=(a+b)/2, t=2 avg=\n",
    "(a+b+c)/3, etc...\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li>$v = \\frac{1}{N}[1,1,1,...]$ where num ones = N</li>\n",
    "<li>A cumulative average is the conventional average $\\frac{1}{N}\\sum_0^{N-1}x_i$. It is time invariant. Shifting the sequence produces the same average. $v=[1,1,1...len(x)]$ and the data is $x$. cumulative avg = $v^T@x$</li>\n",
    "<li>A column average $v^T@X$</li>\n",
    "<li>How to derive row and column avg. X=(B,D). Create a 2x3 test matrix\n",
    "[[1,2,3],[4,5,6]]. Make sure the TM is not symmetric to reduce confusion. We have to options an identity matrix 1x2 if I@X or 3x1 if X@I. There are 2 rows so a row sum must have 2 rows so you know 1x2 is rows and there are 3 cols and you know you need 3 columns for a column sum. so I@X is row sum and X@I is column sum. Then add 1/N to get avg. N=num elements in row or col.\n",
    "</li>\n",
    "<li>A row average $X@v$</li>\n",
    "<li>Weighted avg for softmax. W=(T), V=(T,D). $avg=W^T@V$ Output = (D,) or (D,1) if keepdims=True. Because avg collapases and removes dimensions by default</li>\n",
    "<li>How to derive Weighted Softmax Avg</li>\n",
    "<li>Sequence avg: <li>\n",
    "<li>How to derive sequence avg. </li>\n",
    "</ul>"
   ],
   "metadata": {
    "id": "2VKZ-B-GGCGX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1,2,3,4,5,6]).float()\n",
    "\n",
    "v = (1/6)*(torch.ones(6))\n",
    "print(\"avg of sum of all elements v@x:\",v@x)\n",
    "tri = torch.tril(torch.ones(6,6))\n",
    "avg = (tri@x)/torch.arange(1,7)\n",
    "\n",
    "print(\"sums:\",tri@x)\n",
    "print(\"torch arange:\",torch.arange(1,7))\n",
    "print(\"rolling avg:\",avg) # 1/1, (1+2)/2, (1+2+3)/3,..."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRqW-1qzYSoh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764641233838,
     "user_tz": 480,
     "elapsed": 45,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "9c5e9486-b76b-4041-d6f7-5677cd7643df"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "avg of sum of all elements v@x: tensor(3.5000)\n",
      "sums: tensor([ 1.,  3.,  6., 10., 15., 21.])\n",
      "torch arange: tensor([1, 2, 3, 4, 5, 6])\n",
      "avg: tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1,2,3,4,5])\n",
    "print(f'len(x):{len(x)}')\n",
    "v = torch.ones(len(x))\n",
    "lower_tri = torch.ones(len(x),len(x))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlgMKsHJGPhB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764629315963,
     "user_tz": 480,
     "elapsed": 7,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "97527ecb-eafb-4535-e3bc-81e32b315619"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len(x):5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Optional, Union\n",
    "\n",
    "# avoid division by 0\n",
    "def safe_torch_mean(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.float().sum() / max(x.numel(), 1)\n",
    "\n",
    "def safe_mean(\n",
    "    x: torch.Tensor,\n",
    "    dim: Optional[int] = None,\n",
    "    keepdim: bool = False,\n",
    "    default: Union[float, int] = 0.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Mean that:\n",
    "      - casts non-floating dtypes to float32\n",
    "      - returns `default` when there are no elements along `dim`\n",
    "    \"\"\"\n",
    "    if not x.is_floating_point():\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "    if dim is None:\n",
    "        if x.numel() == 0:\n",
    "            # scalar default\n",
    "            return x.new_tensor(float(default))\n",
    "        return x.mean()\n",
    "\n",
    "    # Mean along a dimension\n",
    "    if x.size(dim) == 0:\n",
    "        # build output shape manually\n",
    "        out_shape = list(x.shape)\n",
    "        if keepdim:\n",
    "            out_shape[dim] = 1\n",
    "        else:\n",
    "            del out_shape[dim]\n",
    "        return x.new_full(out_shape, float(default))\n",
    "\n",
    "    return x.mean(dim=dim, keepdim=keepdim)\n",
    "\n",
    "x = torch.tensor([], dtype=torch.float32)\n",
    "print(safe_mean(x))  # tensor(0.)\n",
    "\n",
    "x = torch.randint(0, 10, (3,), dtype=torch.int8)\n",
    "print(safe_mean(x))  # float32 mean, no error\n",
    "\n",
    "\n",
    "def masked_mean(\n",
    "    x: torch.Tensor,\n",
    "    mask: torch.Tensor,\n",
    "    dim: int,\n",
    "    keepdim: bool = False,\n",
    "    default: Union[float, int] = 0.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Mean over elements where mask == 1/True along `dim`.\n",
    "\n",
    "    x: (..., D, ...)\n",
    "    mask: same shape as x or broadcastable to x\n",
    "    \"\"\"\n",
    "    if not x.is_floating_point():\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "    # make mask float for multiplication\n",
    "    m = mask.to(x.dtype)\n",
    "    # broadcast OK: this relies on PyTorch broadcasting\n",
    "    masked_x = x * m\n",
    "\n",
    "    # sum over dim\n",
    "    num = masked_x.sum(dim=dim, keepdim=keepdim)\n",
    "    den = m.sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "    # safe division: where den > 0, num / den; else default\n",
    "    default_tensor = num.new_full(num.shape, float(default))\n",
    "    mean = torch.where(den > 0, num / torch.clamp(den, min=1e-12), default_tensor)\n",
    "    return mean\n",
    "\n",
    "x = torch.tensor([[1., 2., 3.],\n",
    "                  [4., 5., 6.]])\n",
    "mask = torch.tensor([[1, 0, 1],\n",
    "                     [0, 0, 0]])  # second row all masked out\n",
    "\n",
    "print(masked_mean(x, mask, dim=1))\n",
    "# tensor([2., 0.])  (last row default=0)\n",
    "\n",
    "print(masked_mean(x, mask, dim=1, default=-1.0))\n",
    "# tensor([2., -1.])\n",
    "\n",
    "\n",
    "def segment_mean(\n",
    "    values: torch.Tensor,\n",
    "    segment_ids: torch.Tensor,\n",
    "    num_segments: Optional[int] = None,\n",
    "    default: Union[float, int] = 0.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute mean over segments:\n",
    "        segment_means[k] = mean(values[segment_ids == k])\n",
    "\n",
    "    values: (N, D) or (N,)\n",
    "    segment_ids: (N,) ints in [0, num_segments-1]\n",
    "    \"\"\"\n",
    "    if not values.is_floating_point():\n",
    "        values = values.to(torch.float32)\n",
    "\n",
    "    if values.dim() == 1:\n",
    "        values = values.unsqueeze(-1)  # make it (N, 1)\n",
    "\n",
    "    N, D = values.shape\n",
    "    segment_ids = segment_ids.to(torch.long)\n",
    "\n",
    "    if num_segments is None:\n",
    "        num_segments = int(segment_ids.max().item()) + 1 if N > 0 else 0\n",
    "\n",
    "    device = values.device\n",
    "    dtype = values.dtype\n",
    "\n",
    "    # sums for each segment\n",
    "    sums = torch.zeros(num_segments, D, device=device, dtype=dtype)\n",
    "    counts = torch.zeros(num_segments, 1, device=device, dtype=dtype)\n",
    "\n",
    "    # index_add along segment dimension\n",
    "    sums.index_add_(0, segment_ids, values)\n",
    "    counts.index_add_(0, segment_ids, torch.ones_like(values[:, :1]))\n",
    "\n",
    "    default_tensor = sums.new_full(sums.shape, float(default))\n",
    "    means = torch.where(\n",
    "        counts > 0,\n",
    "        sums / torch.clamp(counts, min=1e-12),\n",
    "        default_tensor,\n",
    "    )\n",
    "\n",
    "    # squeeze if original was 1D\n",
    "    if values.shape[1] == 1:\n",
    "        means = means.squeeze(-1)\n",
    "\n",
    "    return means\n",
    "\n",
    "vals = torch.tensor([[1., 2.],\n",
    "                     [3., 4.],\n",
    "                     [10., 20.]], dtype=torch.float32)\n",
    "seg = torch.tensor([0, 0, 2])   # segment 1 is empty\n",
    "\n",
    "print(segment_mean(vals, seg, num_segments=3, default=0.0))\n",
    "# tensor([[2., 3.],      # mean of rows 0 and 1\n",
    "#         [0., 0.],      # empty segment -> default\n",
    "#         [10., 20.]])   # row 2\n",
    "\n",
    "def batch_safe_mean(\n",
    "    x: torch.Tensor,\n",
    "    mask: torch.Tensor,\n",
    "    default: Union[float, int] = 0.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Per-batch masked mean over time dimension 1.\n",
    "\n",
    "    x: (B, T, D) or (B, T)\n",
    "    mask: (B, T) with 1/True = valid entries\n",
    "    \"\"\"\n",
    "    if x.dim() == 2:\n",
    "        # (B, T) -> (B, T, 1) so we reuse the same logic\n",
    "        x = x.unsqueeze(-1)\n",
    "        squeeze_back = True\n",
    "    else:\n",
    "        squeeze_back = False\n",
    "\n",
    "    # broadcast mask to (B, T, 1)\n",
    "    mask_exp = mask.unsqueeze(-1)\n",
    "\n",
    "    means = masked_mean(\n",
    "        x,\n",
    "        mask_exp,\n",
    "        dim=1,           # average over time\n",
    "        keepdim=False,\n",
    "        default=default,\n",
    "    )\n",
    "\n",
    "    if squeeze_back:\n",
    "        means = means.squeeze(-1)\n",
    "\n",
    "    return means\n",
    "\n",
    "B, T, D = 2, 5, 3\n",
    "x = torch.randn(B, T, D)\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0],   # first sequence length 3\n",
    "    [0, 0, 0, 0, 0],   # second is fully padded\n",
    "])\n",
    "\n",
    "m = batch_safe_mean(x, mask, default=0.0)\n",
    "print(m.shape)   # (2, 3)\n",
    "# row 0: mean over first 3 time steps\n",
    "# row 1: [0., 0., 0.] from default\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def np_safe_mean(x: np.ndarray, axis=None, keepdims=False):\n",
    "  if np.size(x) == 0:\n",
    "      # Empty \u2192 return 0 with requested shape\n",
    "      if axis is None:\n",
    "          return 0.0\n",
    "      # build shape as if mean had been taken, but filled with 0\n",
    "      return np.zeros(np.mean(x, axis=axis, keepdims=keepdims).shape, dtype=float)\n",
    "\n",
    "  # normal mean is fine when non-empty\n",
    "  return np.mean(x, axis=axis, keepdims=keepdims)\n",
    "\n",
    "\n",
    "\n",
    "def np_safe_masked_mean(x: np.ndarray,\n",
    "                        mask: np.ndarray,\n",
    "                        axis=None,\n",
    "                        keepdims=False):\n",
    "    m = mask.astype(float)\n",
    "    masked = x * m\n",
    "\n",
    "    num = masked.sum(axis=axis, keepdims=keepdims)\n",
    "    count = m.sum(axis=axis, keepdims=keepdims)\n",
    "    safe_count = np.clip(count, 1.0, None)\n",
    "    return num / safe_count\n",
    "\n",
    "\n",
    "\n",
    "def np_safe_segment_mean(values: np.ndarray,\n",
    "                         segment_ids: np.ndarray,\n",
    "                         num_segments: int | None = None):\n",
    "    if num_segments is None:\n",
    "        num_segments = int(segment_ids.max()) + 1\n",
    "\n",
    "    rest_shape = values.shape[1:]\n",
    "    sums = np.zeros((num_segments, *rest_shape), dtype=values.dtype)\n",
    "    counts = np.zeros(num_segments, dtype=float)\n",
    "\n",
    "    np.add.at(sums, segment_ids, values)\n",
    "    np.add.at(counts, segment_ids, 1.0)\n",
    "\n",
    "    counts = np.clip(counts, 1.0, None)\n",
    "    # reshape for broadcast\n",
    "    while counts.ndim < sums.ndim:\n",
    "        counts = counts[..., None]\n",
    "\n",
    "    return sums / counts\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "_NJPCrNE44OS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os, time, math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =============================\n",
    "# 1. Safe mean helpers (PyTorch / NumPy)\n",
    "# =============================\n",
    "\n",
    "def torch_safe_mean(x: torch.Tensor, dim: int = -1, keepdim: bool = False):\n",
    "    \"\"\"\n",
    "    Safe mean: if the reduction dim is empty, returns 0 (not NaN).\n",
    "    Works on CPU or GPU.\n",
    "    \"\"\"\n",
    "    if x.numel() == 0 or x.size(dim) == 0:\n",
    "        # Build an output shape consistent with keepdim\n",
    "        out_shape = list(x.shape)\n",
    "        if keepdim:\n",
    "            out_shape[dim] = 1\n",
    "        else:\n",
    "            del out_shape[dim]\n",
    "        if len(out_shape) == 0:\n",
    "            return torch.zeros((), dtype=x.dtype, device=x.device)\n",
    "        return torch.zeros(out_shape, dtype=x.dtype, device=x.device)\n",
    "    return x.mean(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def torch_masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int = -1, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Masked mean: mean over elements where mask==1.\n",
    "    If all masked-out, returns 0.\n",
    "    x: (..., D), mask: same shape as x or broadcastable.\n",
    "    \"\"\"\n",
    "    mask = mask.to(dtype=x.dtype)\n",
    "    num = (x * mask).sum(dim=dim)\n",
    "    denom = mask.sum(dim=dim)\n",
    "    mean = num / torch.clamp(denom, min=eps)\n",
    "    # Zero out where denom is zero\n",
    "    mean = torch.where(denom > 0, mean, torch.zeros_like(mean))\n",
    "    return mean\n",
    "\n",
    "def torch_segment_mean(x: torch.Tensor, segment_ids: torch.Tensor, num_segments: int):\n",
    "    \"\"\"\n",
    "    Segment mean on CPU/GPU using scatter_add:\n",
    "    x: (N, D), segment_ids: (N,), num_segments=K\n",
    "    returns: (K, D)\n",
    "    \"\"\"\n",
    "    N, D = x.shape\n",
    "    out = torch.zeros(num_segments, D, dtype=x.dtype, device=x.device)\n",
    "    count = torch.zeros(num_segments, 1, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    out.scatter_add_(0,\n",
    "                     segment_ids.view(-1, 1).expand(-1, D),\n",
    "                     x)\n",
    "    ones = torch.ones(N, 1, dtype=x.dtype, device=x.device)\n",
    "    count.scatter_add_(0,\n",
    "                       segment_ids.view(-1, 1),\n",
    "                       ones)\n",
    "\n",
    "    denom = torch.clamp(count, min=1.0)\n",
    "    mean = out / denom\n",
    "    mean[count.squeeze(-1) == 0] = 0.0\n",
    "    return mean\n",
    "\n",
    "# NumPy equivalents\n",
    "def np_safe_mean(x: np.ndarray, axis: int = -1, keepdims: bool = False):\n",
    "    if x.size == 0 or x.shape[axis] == 0:\n",
    "        out_shape = list(x.shape)\n",
    "        if keepdims:\n",
    "            out_shape[axis] = 1\n",
    "        else:\n",
    "            del out_shape[axis]\n",
    "        if len(out_shape) == 0:\n",
    "            return np.array(0, dtype=x.dtype)\n",
    "        return np.zeros(out_shape, dtype=x.dtype)\n",
    "    return x.mean(axis=axis, keepdims=keepdims)\n",
    "\n",
    "def np_masked_mean(x: np.ndarray, mask: np.ndarray, axis: int = -1, eps: float = 1e-8):\n",
    "    mask = mask.astype(x.dtype)\n",
    "    num = (x * mask).sum(axis=axis)\n",
    "    denom = mask.sum(axis=axis)\n",
    "    mean = num / np.clip(denom, eps, None)\n",
    "    mean = np.where(denom > 0, mean, 0.0)\n",
    "    return mean\n",
    "\n",
    "def np_segment_mean(x: np.ndarray, segment_ids: np.ndarray, num_segments: int):\n",
    "    \"\"\"\n",
    "    Segment mean for NumPy:\n",
    "    x: (N, D), segment_ids: (N,), num_segments=K\n",
    "    returns: (K, D)\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "        squeeze = True\n",
    "    else:\n",
    "        squeeze = False\n",
    "\n",
    "    N, D = x.shape\n",
    "    out = np.zeros((num_segments, D), dtype=x.dtype)\n",
    "    count = np.zeros((num_segments, 1), dtype=x.dtype)\n",
    "\n",
    "    for i in range(N):\n",
    "        seg = int(segment_ids[i])\n",
    "        out[seg] += x[i]\n",
    "        count[seg] += 1\n",
    "\n",
    "    denom = np.clip(count, 1.0, None)\n",
    "    mean = out / denom\n",
    "    # zero out segments with count=0\n",
    "    mask_zero = (count == 0).reshape(num_segments, 1)\n",
    "    mean[mask_zero[:, 0]] = 0.0\n",
    "\n",
    "    if squeeze:\n",
    "        mean = mean[:, 0]\n",
    "    return mean\n",
    "\n",
    "# ======================================\n",
    "# 2. Triton kernels: safe mean / masked mean / segment mean\n",
    "# ======================================\n",
    "\n",
    "try:\n",
    "    import triton\n",
    "    import triton.language as tl\n",
    "    HAS_TRITON = True\n",
    "except Exception as e:\n",
    "    print(\"Triton not available:\", e)\n",
    "    HAS_TRITON = False\n",
    "\n",
    "if HAS_TRITON:\n",
    "    @triton.jit\n",
    "    def triton_row_mean_kernel(\n",
    "        x_ptr, out_ptr,\n",
    "        B, D,\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "    ):\n",
    "        row_id = tl.program_id(0)\n",
    "        # Each program computes mean over one row x[row_id, :]\n",
    "        offs = tl.arange(0, BLOCK_SIZE)\n",
    "        acc = tl.zeros((), dtype=tl.float32)\n",
    "        # Loop over D in chunks of BLOCK_SIZE\n",
    "        for start in range(0, D, BLOCK_SIZE):\n",
    "            idx = start + offs\n",
    "            mask = idx < D\n",
    "            vals = tl.load(x_ptr + row_id * D + idx, mask=mask, other=0.0)\n",
    "            acc += tl.sum(vals.to(tl.float32), axis=0)\n",
    "        mean = acc / tl.max(tl.float32(D), 1.0)\n",
    "        tl.store(out_ptr + row_id, mean)\n",
    "\n",
    "    @triton.jit\n",
    "    def triton_row_masked_mean_kernel(\n",
    "        x_ptr, mask_ptr, out_ptr,\n",
    "        B, D,\n",
    "        BLOCK_SIZE: tl.constexpr,\n",
    "    ):\n",
    "        row_id = tl.program_id(0)\n",
    "        offs = tl.arange(0, BLOCK_SIZE)\n",
    "        sum_acc = tl.zeros((), dtype=tl.float32)\n",
    "        cnt_acc = tl.zeros((), dtype=tl.float32)\n",
    "        for start in range(0, D, BLOCK_SIZE):\n",
    "            idx = start + offs\n",
    "            mask = idx < D\n",
    "            vals = tl.load(x_ptr + row_id * D + idx, mask=mask, other=0.0)\n",
    "            m = tl.load(mask_ptr + row_id * D + idx, mask=mask, other=0)\n",
    "            vals = vals.to(tl.float32)\n",
    "            m = m.to(tl.float32)\n",
    "            sum_acc += tl.sum(vals * m, axis=0)\n",
    "            cnt_acc += tl.sum(m, axis=0)\n",
    "        denom = tl.where(cnt_acc > 0, cnt_acc, 1.0)\n",
    "        mean = sum_acc / denom\n",
    "        mean = tl.where(cnt_acc > 0, mean, 0.0)\n",
    "        tl.store(out_ptr + row_id, mean)\n",
    "\n",
    "    @triton.jit\n",
    "    def triton_segment_sum_kernel(\n",
    "        x_ptr, seg_ptr, out_ptr, cnt_ptr,\n",
    "        N, D, K,\n",
    "        BLOCK_SIZE_N: tl.constexpr,\n",
    "        BLOCK_SIZE_D: tl.constexpr,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Atomically accumulate segment sums and counts:\n",
    "        x: (N, D) -> out: (K, D), cnt: (K,)\n",
    "        \"\"\"\n",
    "        n_offsets = tl.program_id(0) * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "        d_offsets = tl.program_id(1) * BLOCK_SIZE_D + tl.arange(0, BLOCK_SIZE_D)\n",
    "\n",
    "        mask_n = n_offsets < N\n",
    "        # For each (n, d), accumulate into out[seg, d] and cnt[seg]\n",
    "        for n in n_offsets:\n",
    "            if n >= N:\n",
    "                continue\n",
    "            seg = tl.load(seg_ptr + n).to(tl.int32)\n",
    "            # inner loop over d\n",
    "            x_row_ptr = x_ptr + n * D\n",
    "            for d in d_offsets:\n",
    "                if d >= D:\n",
    "                    continue\n",
    "                val = tl.load(x_row_ptr + d)\n",
    "                tl.atomic_add(out_ptr + seg * D + d, val.to(tl.float32))\n",
    "            # count (only once per row)\n",
    "            tl.atomic_add(cnt_ptr + seg, 1.0)\n",
    "\n",
    "# ======================================\n",
    "# 3. CUDA kernels via torch.utils.cpp_extension\n",
    "# ======================================\n",
    "\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "cuda_src = r\"\"\"\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "template<typename scalar_t>\n",
    "__global__ void row_mean_kernel(const scalar_t* __restrict__ x,\n",
    "                                float* __restrict__ out,\n",
    "                                int B, int D) {\n",
    "  int row = blockIdx.x;\n",
    "  if (row >= B) return;\n",
    "  extern __shared__ float sdata[];\n",
    "  int tid = threadIdx.x;\n",
    "  sdata[tid] = 0.0f;\n",
    "\n",
    "  for (int col = tid; col < D; col += blockDim.x) {\n",
    "    float v = static_cast<float>(x[row * D + col]);\n",
    "    sdata[tid] += v;\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  // reduction in shared mem\n",
    "  for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "    if (tid < s) {\n",
    "      sdata[tid] += sdata[tid + s];\n",
    "    }\n",
    "    __syncthreads();\n",
    "  }\n",
    "\n",
    "  if (tid == 0) {\n",
    "    float denom = D > 0 ? static_cast<float>(D) : 1.0f;\n",
    "    out[row] = (D > 0) ? (sdata[0] / denom) : 0.0f;\n",
    "  }\n",
    "}\n",
    "\n",
    "template<typename scalar_t>\n",
    "__global__ void row_masked_mean_kernel(const scalar_t* __restrict__ x,\n",
    "                                       const uint8_t* __restrict__ mask,\n",
    "                                       float* __restrict__ out,\n",
    "                                       int B, int D) {\n",
    "  int row = blockIdx.x;\n",
    "  if (row >= B) return;\n",
    "  extern __shared__ float sdata[];\n",
    "  float* s_sum = sdata;\n",
    "  float* s_cnt = sdata + blockDim.x;\n",
    "\n",
    "  int tid = threadIdx.x;\n",
    "  s_sum[tid] = 0.0f;\n",
    "  s_cnt[tid] = 0.0f;\n",
    "\n",
    "  for (int col = tid; col < D; col += blockDim.x) {\n",
    "    int idx = row * D + col;\n",
    "    uint8_t m = mask[idx];\n",
    "    if (m) {\n",
    "      float v = static_cast<float>(x[idx]);\n",
    "      s_sum[tid] += v;\n",
    "      s_cnt[tid] += 1.0f;\n",
    "    }\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  // reduce\n",
    "  for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "    if (tid < s) {\n",
    "      s_sum[tid] += s_sum[tid + s];\n",
    "      s_cnt[tid] += s_cnt[tid + s];\n",
    "    }\n",
    "    __syncthreads();\n",
    "  }\n",
    "\n",
    "  if (tid == 0) {\n",
    "    float denom = s_cnt[0] > 0.0f ? s_cnt[0] : 1.0f;\n",
    "    float mean = (s_cnt[0] > 0.0f) ? (s_sum[0] / denom) : 0.0f;\n",
    "    out[row] = mean;\n",
    "  }\n",
    "}\n",
    "\n",
    "template<typename scalar_t>\n",
    "__global__ void segment_sum_kernel(const scalar_t* __restrict__ x,\n",
    "                                   const int32_t* __restrict__ seg_ids,\n",
    "                                   float* __restrict__ out,\n",
    "                                   float* __restrict__ cnt,\n",
    "                                   int N, int D, int K) {\n",
    "  int n = blockIdx.x;\n",
    "  if (n >= N) return;\n",
    "  int tid = threadIdx.x;\n",
    "  int seg = seg_ids[n];\n",
    "  if (seg < 0 || seg >= K) return;\n",
    "\n",
    "  for (int d = tid; d < D; d += blockDim.x) {\n",
    "    float v = static_cast<float>(x[n * D + d]);\n",
    "    atomicAdd(out + seg * D + d, v);\n",
    "  }\n",
    "  // count once per row (thread 0)\n",
    "  if (tid == 0) {\n",
    "    atomicAdd(cnt + seg, 1.0f);\n",
    "  }\n",
    "}\n",
    "\n",
    "torch::Tensor cuda_row_mean(torch::Tensor x) {\n",
    "  TORCH_CHECK(x.is_cuda(), \"x must be CUDA\");\n",
    "  TORCH_CHECK(x.dim() == 2, \"x must be (B, D)\");\n",
    "  const auto B = x.size(0);\n",
    "  const auto D = x.size(1);\n",
    "  auto out = torch::empty({B}, x.options().dtype(torch::kFloat32));\n",
    "  const int threads = 256;\n",
    "  const int blocks = B;\n",
    "  const size_t shmem = threads * sizeof(float);\n",
    "  AT_DISPATCH_ALL_TYPES_AND(torch::ScalarType::Half, x.scalar_type(), \"row_mean_kernel\", [&] {\n",
    "    row_mean_kernel<scalar_t><<<blocks, threads, shmem>>>(\n",
    "      x.data_ptr<scalar_t>(),\n",
    "      out.data_ptr<float>(),\n",
    "      B, D\n",
    "    );\n",
    "  });\n",
    "  return out;\n",
    "}\n",
    "\n",
    "torch::Tensor cuda_row_masked_mean(torch::Tensor x, torch::Tensor mask) {\n",
    "  TORCH_CHECK(x.is_cuda(), \"x must be CUDA\");\n",
    "  TORCH_CHECK(mask.is_cuda(), \"mask must be CUDA\");\n",
    "  TORCH_CHECK(x.sizes() == mask.sizes(), \"x and mask shape mismatch\");\n",
    "  TORCH_CHECK(x.dim() == 2, \"x must be (B, D)\");\n",
    "  const auto B = x.size(0);\n",
    "  const auto D = x.size(1);\n",
    "  auto out = torch::empty({B}, x.options().dtype(torch::kFloat32));\n",
    "  const int threads = 256;\n",
    "  const int blocks = B;\n",
    "  const size_t shmem = threads * sizeof(float) * 2;\n",
    "  AT_DISPATCH_ALL_TYPES_AND(torch::ScalarType::Half, x.scalar_type(), \"row_masked_mean_kernel\", [&] {\n",
    "    row_masked_mean_kernel<scalar_t><<<blocks, threads, shmem>>>(\n",
    "      x.data_ptr<scalar_t>(),\n",
    "      mask.data_ptr<uint8_t>(),\n",
    "      out.data_ptr<float>(),\n",
    "      B, D\n",
    "    );\n",
    "  });\n",
    "  return out;\n",
    "}\n",
    "\n",
    "std::vector<torch::Tensor> cuda_segment_mean(torch::Tensor x,\n",
    "                                             torch::Tensor seg_ids,\n",
    "                                             int64_t K) {\n",
    "  TORCH_CHECK(x.is_cuda(), \"x must be CUDA\");\n",
    "  TORCH_CHECK(seg_ids.is_cuda(), \"seg_ids must be CUDA\");\n",
    "  TORCH_CHECK(x.dim() == 2, \"x must be (N, D)\");\n",
    "  TORCH_CHECK(seg_ids.dim() == 1, \"seg_ids must be (N)\");\n",
    "  const auto N = x.size(0);\n",
    "  const auto D = x.size(1);\n",
    "  auto out = torch::zeros({K, D}, x.options().dtype(torch::kFloat32));\n",
    "  auto cnt = torch::zeros({K}, x.options().dtype(torch::kFloat32));\n",
    "  const int threads = 256;\n",
    "  const int blocks = N;\n",
    "  AT_DISPATCH_ALL_TYPES_AND(torch::ScalarType::Half, x.scalar_type(), \"segment_sum_kernel\", [&] {\n",
    "    segment_sum_kernel<scalar_t><<<blocks, threads>>>(\n",
    "      x.data_ptr<scalar_t>(),\n",
    "      seg_ids.data_ptr<int32_t>(),\n",
    "      out.data_ptr<float>(),\n",
    "      cnt.data_ptr<float>(),\n",
    "      N, D, (int)K\n",
    "    );\n",
    "  });\n",
    "  return {out, cnt};\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "  m.def(\"row_mean\", &cuda_row_mean, \"Row-wise mean (safe) [CUDA]\");\n",
    "  m.def(\"row_masked_mean\", &cuda_row_masked_mean, \"Row-wise masked mean (safe) [CUDA]\");\n",
    "  m.def(\"segment_mean_raw\", &cuda_segment_mean, \"Segment sum+count (CUDA)\");\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    cuda_kernels = load(\n",
    "        name=\"mean_kernels\",\n",
    "        sources=[cuda_src],\n",
    "        verbose=False,\n",
    "    )\n",
    "    print(\"Loaded custom CUDA kernels.\")\n",
    "else:\n",
    "    cuda_kernels = None\n",
    "    print(\"CUDA not available, skipping custom kernels.\")\n",
    "\n",
    "# ======================================\n",
    "# 4. Benchmark harness\n",
    "# ======================================\n",
    "\n",
    "def bench(fn, iters=50, cuda_sync=True):\n",
    "    # warmup\n",
    "    for _ in range(5):\n",
    "        fn()\n",
    "    if cuda_sync and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        out = fn()\n",
    "    if cuda_sync and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) * 1000.0 / iters  # ms\n",
    "\n",
    "# ======================================\n",
    "# 5. Run benchmarks for various dtypes\n",
    "# ======================================\n",
    "\n",
    "B, D = 4096, 1024      # row-wise mean shape\n",
    "N, K = 8192, 128       # segment shape (N rows, K segments)\n",
    "\n",
    "dtypes = [torch.float32, torch.float16]  # you can add int8 etc. for CPU / cast\n",
    "\n",
    "for dtype in dtypes:\n",
    "    print(f\"\\n=== dtype: {dtype} ===\")\n",
    "\n",
    "    # generate data\n",
    "    x_cpu = torch.randn(B, D, dtype=dtype)\n",
    "    mask_cpu = (torch.rand(B, D) > 0.5).to(torch.bool)\n",
    "    seg_ids_cpu = torch.randint(low=0, high=K, size=(N,), dtype=torch.int64)\n",
    "    x_seg_cpu = torch.randn(N, D, dtype=dtype)\n",
    "\n",
    "    # PyTorch CPU\n",
    "    def fn_torch_cpu_batch_mean():\n",
    "        return torch_safe_mean(x_cpu, dim=-1)\n",
    "\n",
    "    def fn_torch_cpu_masked_mean():\n",
    "        return torch_masked_mean(x_cpu, mask_cpu, dim=-1)\n",
    "\n",
    "    def fn_torch_cpu_segment_mean():\n",
    "        return torch_segment_mean(x_seg_cpu, seg_ids_cpu, num_segments=K)\n",
    "\n",
    "    print(\"=== PyTorch CPU ===\")\n",
    "    print(\"torch_cpu_batch_mean:   %.3f ms\" % bench(fn_torch_cpu_batch_mean, cuda_sync=False))\n",
    "    print(\"torch_cpu_masked_mean:  %.3f ms\" % bench(fn_torch_cpu_masked_mean, cuda_sync=False))\n",
    "    print(\"torch_cpu_segment_mean: %.3f ms\" % bench(fn_torch_cpu_segment_mean, cuda_sync=False))\n",
    "\n",
    "    # NumPy CPU\n",
    "    x_np = x_cpu.numpy().astype(np.float32)   # use float32 internally\n",
    "    mask_np = mask_cpu.numpy()\n",
    "    x_seg_np = x_seg_cpu.numpy().astype(np.float32)\n",
    "    seg_ids_np = seg_ids_cpu.numpy()\n",
    "\n",
    "    def fn_numpy_batch_mean():\n",
    "        return np_safe_mean(x_np, axis=-1)\n",
    "\n",
    "    def fn_numpy_masked_mean():\n",
    "        return np_masked_mean(x_np, mask_np, axis=-1)\n",
    "\n",
    "    def fn_numpy_segment_mean():\n",
    "        return np_segment_mean(x_seg_np, seg_ids_np, num_segments=K)\n",
    "\n",
    "    print(\"=== NumPy CPU ===\")\n",
    "    print(\"numpy_batch_mean:   %.3f ms\" % bench(fn_numpy_batch_mean, cuda_sync=False))\n",
    "    print(\"numpy_masked_mean:  %.3f ms\" % bench(fn_numpy_masked_mean, cuda_sync=False))\n",
    "    print(\"numpy_segment_mean: %.3f ms\" % bench(fn_numpy_segment_mean, cuda_sync=False))\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        x_gpu = x_cpu.to(device)\n",
    "        mask_gpu = mask_cpu.to(device)\n",
    "        x_seg_gpu = x_seg_cpu.to(device)\n",
    "        seg_ids_gpu = seg_ids_cpu.to(device).to(torch.int32)\n",
    "\n",
    "        def fn_torch_gpu_batch_mean():\n",
    "            return torch_safe_mean(x_gpu, dim=-1)\n",
    "\n",
    "        def fn_torch_gpu_masked_mean():\n",
    "            return torch_masked_mean(x_gpu, mask_gpu, dim=-1)\n",
    "\n",
    "        def fn_torch_gpu_segment_mean():\n",
    "            return torch_segment_mean(x_seg_gpu, seg_ids_gpu.to(torch.long), num_segments=K)\n",
    "\n",
    "        print(\"=== PyTorch GPU ===\")\n",
    "        print(\"torch_gpu_batch_mean:   %.3f ms\" % bench(fn_torch_gpu_batch_mean))\n",
    "        print(\"torch_gpu_masked_mean:  %.3f ms\" % bench(fn_torch_gpu_masked_mean))\n",
    "        print(\"torch_gpu_segment_mean: %.3f ms\" % bench(fn_torch_gpu_segment_mean))\n",
    "\n",
    "        # Triton\n",
    "        if HAS_TRITON:\n",
    "            B_, D_ = x_gpu.shape\n",
    "\n",
    "            def fn_triton_batch_mean():\n",
    "                x32 = x_gpu.to(torch.float32)\n",
    "                out = torch.empty(B_, device=device, dtype=torch.float32)\n",
    "                grid = (B_,)\n",
    "                triton_row_mean_kernel[grid](\n",
    "                    x32, out, B_, D_,\n",
    "                    BLOCK_SIZE=128,\n",
    "                )\n",
    "                return out\n",
    "\n",
    "            def fn_triton_masked_mean():\n",
    "                x32 = x_gpu.to(torch.float32)\n",
    "                m8 = mask_gpu.to(torch.uint8)\n",
    "                out = torch.empty(B_, device=device, dtype=torch.float32)\n",
    "                grid = (B_,)\n",
    "                triton_row_masked_mean_kernel[grid](\n",
    "                    x32, m8, out, B_, D_,\n",
    "                    BLOCK_SIZE=128,\n",
    "                )\n",
    "                return out\n",
    "\n",
    "            def fn_triton_segment_mean():\n",
    "                # segment_sum -> divide\n",
    "                x32 = x_seg_gpu.to(torch.float32)\n",
    "                out = torch.zeros(K, D, dtype=torch.float32, device=device)\n",
    "                cnt = torch.zeros(K, dtype=torch.float32, device=device)\n",
    "                grid = (triton.cdiv(N, 32), triton.cdiv(D, 32))\n",
    "                triton_segment_sum_kernel[grid](\n",
    "                    x32, seg_ids_gpu, out, cnt,\n",
    "                    N, D, K,\n",
    "                    BLOCK_SIZE_N=32,\n",
    "                    BLOCK_SIZE_D=32,\n",
    "                )\n",
    "                denom = torch.clamp(cnt.view(K, 1), min=1.0)\n",
    "                mean = out / denom\n",
    "                mean[cnt == 0] = 0.0\n",
    "                return mean\n",
    "\n",
    "            print(\"=== Triton GPU ===\")\n",
    "            print(\"triton_batch_mean:   %.3f ms\" % bench(fn_triton_batch_mean))\n",
    "            print(\"triton_masked_mean:  %.3f ms\" % bench(fn_triton_masked_mean))\n",
    "            print(\"triton_segment_mean: %.3f ms\" % bench(fn_triton_segment_mean))\n",
    "\n",
    "        # CUDA custom\n",
    "        if cuda_kernels is not None:\n",
    "            def fn_cuda_batch_mean():\n",
    "                # returns float32\n",
    "                return cuda_kernels.row_mean(x_gpu)\n",
    "\n",
    "            def fn_cuda_masked_mean():\n",
    "                return cuda_kernels.row_masked_mean(x_gpu, mask_gpu.to(torch.uint8))\n",
    "\n",
    "            def fn_cuda_segment_mean():\n",
    "                out, cnt = cuda_kernels.segment_mean_raw(x_seg_gpu, seg_ids_gpu, K)\n",
    "                denom = torch.clamp(cnt.view(K, 1), min=1.0)\n",
    "                mean = out / denom\n",
    "                mean[cnt == 0] = 0.0\n",
    "                return mean\n",
    "\n",
    "            print(\"=== Custom CUDA ===\")\n",
    "            print(\"cuda_batch_mean:   %.3f ms\" % bench(fn_cuda_batch_mean))\n",
    "            print(\"cuda_masked_mean:  %.3f ms\" % bench(fn_cuda_masked_mean))\n",
    "            print(\"cuda_segment_mean: %.3f ms\" % bench(fn_cuda_segment_mean))"
   ],
   "metadata": {
    "id": "Ia134wDD-QLg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# triton_safe_masked_mean.py\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def safe_masked_mean_rowwise_kernel(\n",
    "    x_ptr,         # *f32\n",
    "    mask_ptr,      # *i32 or *f32 (0/1); pass nullptr for unmasked\n",
    "    out_ptr,       # *f32\n",
    "    B, N,\n",
    "    stride_xb, stride_xn,\n",
    "    stride_mb, stride_mn,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "):\n",
    "    b = tl.program_id(0)  # batch index\n",
    "\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "    row_x_ptr = x_ptr + b * stride_xb + offs_n * stride_xn\n",
    "\n",
    "    has_mask = mask_ptr != 0\n",
    "    if has_mask:\n",
    "        row_m_ptr = mask_ptr + b * stride_mb + offs_n * stride_mn\n",
    "\n",
    "    acc_sum = tl.zeros((), dtype=tl.float32)\n",
    "    acc_count = tl.zeros((), dtype=tl.float32)\n",
    "\n",
    "    for start_n in range(0, N, BLOCK_N):\n",
    "        cur_mask = start_n + offs_n < N\n",
    "\n",
    "        x_vals = tl.load(row_x_ptr + start_n * stride_xn,\n",
    "                         mask=cur_mask,\n",
    "                         other=0.0)\n",
    "\n",
    "        if has_mask:\n",
    "            m_vals = tl.load(row_m_ptr + start_n * stride_mn,\n",
    "                             mask=cur_mask,\n",
    "                             other=0)\n",
    "            # assume mask is 0/1 or bool, cast to float\n",
    "            m_vals_f = m_vals.to(tl.float32)\n",
    "        else:\n",
    "            m_vals_f = tl.where(cur_mask, 1.0, 0.0)\n",
    "\n",
    "        acc_sum += tl.sum(x_vals * m_vals_f, axis=0)\n",
    "        acc_count += tl.sum(m_vals_f, axis=0)\n",
    "\n",
    "    # safe mean: if acc_count == 0, define mean = 0\n",
    "    safe_count = tl.maximum(acc_count, 1.0)\n",
    "    mean = acc_sum / safe_count\n",
    "    mean = tl.where(acc_count > 0, mean, 0.0)\n",
    "\n",
    "    tl.store(out_ptr + b, mean)\n",
    "\n",
    "def safe_masked_mean_rowwise(x: torch.Tensor, mask: torch.Tensor | None = None):\n",
    "    \"\"\"\n",
    "    x: (B, N), float32\n",
    "    mask: (B, N) or None; 0/1 or bool\n",
    "    returns: (B,)\n",
    "    \"\"\"\n",
    "    assert x.dim() == 2\n",
    "    B, N = x.shape\n",
    "    x = x.contiguous()\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.to(torch.int32).contiguous()\n",
    "        mask_ptr = mask\n",
    "    else:\n",
    "        mask_ptr = torch.tensor([], device=x.device, dtype=torch.int32)  # dummy\n",
    "        # We'll treat 'mask_ptr != 0' as 'has_mask', so ensure it's not literally None.\n",
    "        # Instead: we pass mask_ptr.data_ptr()==0? Triton can't; hack: pass 0 below.\n",
    "\n",
    "    out = torch.empty(B, device=x.device, dtype=torch.float32)\n",
    "\n",
    "    BLOCK_N = 128\n",
    "    grid = (B,)\n",
    "\n",
    "    # carefully pass 0 for mask_ptr when mask is None\n",
    "    mask_arg = mask_ptr if mask is not None else 0\n",
    "\n",
    "    safe_masked_mean_rowwise_kernel[grid](\n",
    "        x,\n",
    "        mask_arg,\n",
    "        out,\n",
    "        B, N,\n",
    "        x.stride(0), x.stride(1),\n",
    "        mask.stride(0) if mask is not None else 0,\n",
    "        mask.stride(1) if mask is not None else 0,\n",
    "        BLOCK_N=BLOCK_N,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "// safe_masked_mean.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdint.h>\n",
    "\n",
    "__global__ void safe_masked_mean_kernel(\n",
    "    const float* __restrict__ x,\n",
    "    const uint8_t* __restrict__ mask,  // 0 or 1; nullptr for unmasked\n",
    "    int64_t N,\n",
    "    float* __restrict__ out_sum,\n",
    "    float* __restrict__ out_count\n",
    ") {\n",
    "    extern __shared__ float shmem[];\n",
    "    float* sh_sum   = shmem;\n",
    "    float* sh_count = shmem + blockDim.x;\n",
    "\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float local_sum = 0.0f;\n",
    "    float local_count = 0.0f;\n",
    "\n",
    "    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n",
    "        uint8_t m = mask ? mask[i] : 1;\n",
    "        if (m) {\n",
    "            local_sum += x[i];\n",
    "            local_count += 1.0f;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sh_sum[threadIdx.x] = local_sum;\n",
    "    sh_count[threadIdx.x] = local_count;\n",
    "    __syncthreads();\n",
    "\n",
    "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
    "        if (threadIdx.x < stride) {\n",
    "            sh_sum[threadIdx.x] += sh_sum[threadIdx.x + stride];\n",
    "            sh_count[threadIdx.x] += sh_count[threadIdx.x + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        atomicAdd(out_sum, sh_sum[0]);\n",
    "        atomicAdd(out_count, sh_count[0]);\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#include <torch/extension.h>  // or your own wrapper\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "std::pair<float, float> safe_masked_mean_cuda(\n",
    "    const float* d_x,\n",
    "    const uint8_t* d_mask,\n",
    "    int64_t N\n",
    ") {\n",
    "    float h_sum = 0.0f;\n",
    "    float h_count = 0.0f;\n",
    "\n",
    "    float* d_sum;\n",
    "    float* d_count;\n",
    "    cudaMalloc(&d_sum, sizeof(float));\n",
    "    cudaMalloc(&d_count, sizeof(float));\n",
    "    cudaMemcpy(d_sum, &h_sum, sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_count, &h_count, sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    size_t shmem = 2 * threads * sizeof(float);\n",
    "\n",
    "    safe_masked_mean_kernel<<<blocks, threads, shmem>>>(d_x, d_mask, N, d_sum, d_count);\n",
    "\n",
    "    cudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(&h_count, d_count, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_sum);\n",
    "    cudaFree(d_count);\n",
    "\n",
    "    // safe mean\n",
    "    float mean = (h_count > 0.0f) ? (h_sum / h_count) : 0.0f;\n",
    "    return {mean, h_count};\n",
    "}\n",
    "\n",
    "sum[seg] = \u03a3 x[i] for i with segment_ids[i] == seg and mask[i]==1\n",
    "count[seg] = \u03a3 1    for same\n",
    "mean[seg] = sum[seg] / max(count[seg], 1)\n",
    "\n",
    "// segment_mean.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdint.h>\n",
    "\n",
    "__global__ void segment_sum_count_kernel(\n",
    "    const float* __restrict__ x,\n",
    "    const int32_t* __restrict__ segment_ids,\n",
    "    const uint8_t* __restrict__ mask,    // 0/1 or nullptr\n",
    "    int64_t N,\n",
    "    float* __restrict__ seg_sums,\n",
    "    float* __restrict__ seg_counts,\n",
    "    int32_t num_segments\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n",
    "        uint8_t m = mask ? mask[i] : 1;\n",
    "        if (!m) {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        int32_t seg = segment_ids[i];\n",
    "        if (seg < 0 || seg >= num_segments) {\n",
    "            continue;  // or assert\n",
    "        }\n",
    "\n",
    "        float val = x[i];\n",
    "        atomicAdd(&seg_sums[seg], val);\n",
    "        atomicAdd(&seg_counts[seg], 1.0f);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void segment_safe_mean_kernel(\n",
    "    const float* __restrict__ seg_sums,\n",
    "    const float* __restrict__ seg_counts,\n",
    "    float* __restrict__ seg_means,\n",
    "    int32_t num_segments\n",
    ") {\n",
    "    int seg = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (seg >= num_segments) return;\n",
    "\n",
    "    float s = seg_sums[seg];\n",
    "    float c = seg_counts[seg];\n",
    "\n",
    "    if (c > 0.0f) {\n",
    "        seg_means[seg] = s / c;\n",
    "    } else {\n",
    "        seg_means[seg] = 0.0f;  // safe mean for empty segment\n",
    "    }\n",
    "}\n",
    "\n",
    "#include <torch/extension.h>  // or your own wrapper\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "std::pair<float, float> safe_masked_mean_cuda(\n",
    "    const float* d_x,\n",
    "    const uint8_t* d_mask,\n",
    "    int64_t N\n",
    ") {\n",
    "    float h_sum = 0.0f;\n",
    "    float h_count = 0.0f;\n",
    "\n",
    "    float* d_sum;\n",
    "    float* d_count;\n",
    "    cudaMalloc(&d_sum, sizeof(float));\n",
    "    cudaMalloc(&d_count, sizeof(float));\n",
    "    cudaMemcpy(d_sum, &h_sum, sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_count, &h_count, sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    size_t shmem = 2 * threads * sizeof(float);\n",
    "\n",
    "    safe_masked_mean_kernel<<<blocks, threads, shmem>>>(d_x, d_mask, N, d_sum, d_count);\n",
    "\n",
    "    cudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(&h_count, d_count, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(d_sum);\n",
    "    cudaFree(d_count);\n",
    "\n",
    "    // safe mean\n",
    "    float mean = (h_count > 0.0f) ? (h_sum / h_count) : 0.0f;\n",
    "    return {mean, h_count};\n",
    "}\n",
    "\n",
    "#sum[seg] = \u03a3 x[i] for i with segment_ids[i] == seg and mask[i]==1\n",
    "#count[seg] = \u03a3 1    for same\n",
    "#mean[seg] = sum[seg] / max(count[seg], 1)\n",
    "\n",
    "// segment_mean.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdint.h>\n",
    "\n",
    "__global__ void segment_sum_count_kernel(\n",
    "    const float* __restrict__ x,\n",
    "    const int32_t* __restrict__ segment_ids,\n",
    "    const uint8_t* __restrict__ mask,    // 0/1 or nullptr\n",
    "    int64_t N,\n",
    "    float* __restrict__ seg_sums,\n",
    "    float* __restrict__ seg_counts,\n",
    "    int32_t num_segments\n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n",
    "        uint8_t m = mask ? mask[i] : 1;\n",
    "        if (!m) {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        int32_t seg = segment_ids[i];\n",
    "        if (seg < 0 || seg >= num_segments) {\n",
    "            continue;  // or assert\n",
    "        }\n",
    "\n",
    "        float val = x[i];\n",
    "        atomicAdd(&seg_sums[seg], val);\n",
    "        atomicAdd(&seg_counts[seg], 1.0f);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void segment_safe_mean_kernel(\n",
    "    const float* __restrict__ seg_sums,\n",
    "    const float* __restrict__ seg_counts,\n",
    "    float* __restrict__ seg_means,\n",
    "    int32_t num_segments\n",
    ") {\n",
    "    int seg = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (seg >= num_segments) return;\n",
    "\n",
    "    float s = seg_sums[seg];\n",
    "    float c = seg_counts[seg];\n",
    "\n",
    "    if (c > 0.0f) {\n",
    "        seg_means[seg] = s / c;\n",
    "    } else {\n",
    "        seg_means[seg] = 0.0f;  // safe mean for empty segment\n",
    "    }\n",
    "}\n",
    "\n",
    "void segment_mean_cuda(\n",
    "    const float* d_x,\n",
    "    const int32_t* d_segment_ids,\n",
    "    const uint8_t* d_mask,      // may be nullptr\n",
    "    int64_t N,\n",
    "    int32_t num_segments,\n",
    "    float* d_out_means\n",
    ") {\n",
    "    float* d_sums;\n",
    "    float* d_counts;\n",
    "    cudaMalloc(&d_sums,   num_segments * sizeof(float));\n",
    "    cudaMalloc(&d_counts, num_segments * sizeof(float));\n",
    "    cudaMemset(d_sums,   0, num_segments * sizeof(float));\n",
    "    cudaMemset(d_counts, 0, num_segments * sizeof(float));\n",
    "\n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "\n",
    "    segment_sum_count_kernel<<<blocks, threads>>>(\n",
    "        d_x, d_segment_ids, d_mask, N, d_sums, d_counts, num_segments\n",
    "    );\n",
    "\n",
    "    int blocks_seg = (num_segments + threads - 1) / threads;\n",
    "    segment_safe_mean_kernel<<<blocks_seg, threads>>>(\n",
    "        d_sums, d_counts, d_out_means, num_segments\n",
    "    );\n",
    "\n",
    "    cudaFree(d_sums);\n",
    "    cudaFree(d_counts);\n",
    "}\n",
    "\n",
    "\n",
    "#triton segment mean sketch\n",
    "@triton.jit\n",
    "def segment_sum_count_kernel_triton(\n",
    "    x_ptr, seg_id_ptr, mask_ptr,\n",
    "    seg_sums_ptr, seg_counts_ptr,\n",
    "    N, NUM_SEGMENTS: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offs < N\n",
    "\n",
    "    x = tl.load(x_ptr + offs, mask=mask, other=0.0)\n",
    "    seg = tl.load(seg_id_ptr + offs, mask=mask, other=0)\n",
    "    has_mask = mask_ptr != 0\n",
    "    if has_mask:\n",
    "        m = tl.load(mask_ptr + offs, mask=mask, other=0)\n",
    "        valid = mask & (m != 0)\n",
    "    else:\n",
    "        valid = mask\n",
    "\n",
    "    x = tl.where(valid, x, 0.0)\n",
    "    seg = tl.where(valid, seg, 0)\n",
    "\n",
    "    # atomic adds\n",
    "    tl.atomic_add(seg_sums_ptr + seg, x, mask=valid)\n",
    "    tl.atomic_add(seg_counts_ptr + seg,\n",
    "                  tl.where(valid, 1.0, 0.0),\n",
    "                  mask=valid)"
   ],
   "metadata": {
    "id": "TDCfqhM7pyEh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Colab-ready benchmark: safe mean, masked mean, segment mean, batch mean\n",
    "# Across: PyTorch CPU, PyTorch GPU, NumPy CPU, Triton kernel, CUDA (CuPy) kernel\n",
    "import torch\n",
    "!pip install -q triton==3.0.0 cupy-cuda12x\n",
    "\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. Config\n",
    "# ------------------------------------------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Problem sizes\n",
    "BATCH = 4096     # number of rows\n",
    "N      = 1024    # row length\n",
    "SEG_K  = 128     # number of segments for segment mean\n",
    "\n",
    "WARMUP_ITERS = 5\n",
    "BENCH_ITERS  = 20\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Safe mean helpers (PyTorch & NumPy)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def torch_safe_mean(x: torch.Tensor, dim=None, keepdim=False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Safe mean: if count == 0, returns 0 (no NaN).\n",
    "    Works with arbitrary dimension, keeps gradients.\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        # flatten\n",
    "        x_flat = x.view(-1)\n",
    "        count = x_flat.numel()\n",
    "        if count == 0:\n",
    "            return x_flat.new_zeros(())\n",
    "        return x_flat.sum() / max(count, 1)\n",
    "    else:\n",
    "        # general dim\n",
    "        x = x.float()\n",
    "        ones = torch.ones_like(x, dtype=x.dtype)\n",
    "        count = ones.sum(dim=dim, keepdim=keepdim)\n",
    "        s = x.sum(dim=dim, keepdim=keepdim)\n",
    "        # clamp denominator to at least 1 to avoid NaNs; where count==0, we force 0\n",
    "        denom = count.clamp_min(1.0)\n",
    "        mean = s / denom\n",
    "        mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n",
    "        return mean\n",
    "\n",
    "def torch_masked_mean(x: torch.Tensor, mask: torch.Tensor, dim=-1, keepdim=False):\n",
    "    \"\"\"\n",
    "    mask: bool or 0/1, same shape as x.\n",
    "    \"\"\"\n",
    "    x = x.float()\n",
    "    mask = mask.to(dtype=x.dtype)\n",
    "    s = (x * mask).sum(dim=dim, keepdim=keepdim)\n",
    "    count = mask.sum(dim=dim, keepdim=keepdim)\n",
    "    denom = count.clamp_min(1.0)\n",
    "    mean = s / denom\n",
    "    mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n",
    "    return mean\n",
    "\n",
    "def torch_segment_mean(x: torch.Tensor, segment_ids: torch.Tensor, num_segments: int):\n",
    "    \"\"\"\n",
    "    x: [N, D] or [N]; segment_ids: [N] in [0, num_segments-1]\n",
    "    Returns [num_segments, D] or [num_segments]\n",
    "    \"\"\"\n",
    "    if x.dim() == 1:\n",
    "        x = x[:, None]\n",
    "        squeeze = True\n",
    "    else:\n",
    "        squeeze = False\n",
    "\n",
    "    N, D = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    segment_ids = segment_ids.to(device=device, dtype=torch.long)\n",
    "    out = torch.zeros(num_segments, D, device=device, dtype=x.dtype)\n",
    "    count = torch.zeros(num_segments, 1, device=device, dtype=x.dtype)\n",
    "\n",
    "    out.index_add_(0, segment_ids, x)\n",
    "    ones = torch.ones(N, 1, device=device, dtype=x.dtype)\n",
    "    count.index_add_(0, segment_ids, ones)\n",
    "\n",
    "    denom = count.clamp_min(1.0)\n",
    "    mean = out / denom\n",
    "    mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n",
    "\n",
    "    if squeeze:\n",
    "        mean = mean[:, 0]\n",
    "    return mean\n",
    "\n",
    "# NumPy equivalents (no gradient)\n",
    "def np_safe_mean(x: np.ndarray, axis=None, keepdims=False):\n",
    "    if x.size == 0:\n",
    "        return np.zeros((), dtype=x.dtype)\n",
    "    count = x.shape[axis] if axis is not None else x.size\n",
    "    s = x.sum(axis=axis, keepdims=keepdims)\n",
    "    return s / max(count, 1)\n",
    "\n",
    "def np_masked_mean(x: np.ndarray, mask: np.ndarray, axis=-1, keepdims=False):\n",
    "    x = x.astype(np.float32)\n",
    "    mask = mask.astype(np.float32)\n",
    "    s = (x * mask).sum(axis=axis, keepdims=keepdims)\n",
    "    count = mask.sum(axis=axis, keepdims=keepdims)\n",
    "    denom = np.clip(count, 1.0, None)\n",
    "    out = s / denom\n",
    "    out = np.where(count > 0, out, np.zeros_like(out))\n",
    "    return out\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def np_segment_mean(x: np.ndarray,\n",
    "                    segment_ids: np.ndarray,\n",
    "                    num_segments: int):\n",
    "    # Handle 1D x by temporarily promoting to 2D\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]   # (N,) -> (N, 1)\n",
    "        squeeze = True\n",
    "    else:\n",
    "        squeeze = False\n",
    "\n",
    "    N, D = x.shape\n",
    "    out = np.zeros((num_segments, D), dtype=x.dtype)      # sum per segment\n",
    "    count = np.zeros((num_segments,), dtype=np.int64)     # count per segment\n",
    "\n",
    "    # Accumulate per segment\n",
    "    for i in range(N):\n",
    "        seg = int(segment_ids[i])\n",
    "        if 0 <= seg < num_segments:\n",
    "            out[seg] += x[i]\n",
    "            count[seg] += 1\n",
    "\n",
    "    # Avoid div-by-zero by clamping denominator to at least 1\n",
    "    denom = np.maximum(count, 1).reshape(num_segments, 1)  # (K, 1)\n",
    "    mean = out / denom                                    # (K, D)\n",
    "\n",
    "    # For segments where count == 0, explicitly set mean to 0\n",
    "    zero_mask = (count == 0)  # (K,)\n",
    "    mean[zero_mask] = 0.0\n",
    "\n",
    "    if squeeze:\n",
    "        mean = mean[:, 0]  # (K,)\n",
    "    return mean\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Data setup\n",
    "# ------------------------------------------------------------\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "x_torch_cpu = torch.randn(BATCH, N, dtype=torch.float32)\n",
    "mask_torch_cpu = (torch.rand(BATCH, N) > 0.3).to(torch.bool)\n",
    "\n",
    "seg_ids_cpu = torch.randint(0, SEG_K, (BATCH,), dtype=torch.long)\n",
    "\n",
    "x_np = x_torch_cpu.numpy()\n",
    "mask_np = mask_torch_cpu.numpy().astype(np.bool_)\n",
    "seg_ids_np = seg_ids_cpu.numpy()\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    x_torch_gpu = x_torch_cpu.to(\"cuda\")\n",
    "    mask_torch_gpu = mask_torch_cpu.to(\"cuda\")\n",
    "    seg_ids_gpu = seg_ids_cpu.to(\"cuda\")\n",
    "else:\n",
    "    x_torch_gpu = None\n",
    "    mask_torch_gpu = None\n",
    "    seg_ids_gpu = None\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Triton kernel: row-wise mean (batch mean) for 2D tensor\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "@triton.jit\n",
    "def row_mean_kernel(X_ptr, Y_ptr, BATCH, N, BLOCK_SIZE: tl.constexpr):\n",
    "    row_id = tl.program_id(0)\n",
    "    offs = row_id * N + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offs < (row_id * N + N)\n",
    "\n",
    "    x = tl.load(X_ptr + offs, mask=mask, other=0.0)\n",
    "    # parallel reduction in block\n",
    "    # here we just sum and rely on BLOCK_SIZE == N for simplicity\n",
    "    # (you can generalize to partial tiles if needed)\n",
    "    s = tl.sum(x, axis=0)\n",
    "    # each program handles a whole row\n",
    "    denom = N\n",
    "    mean = s / denom\n",
    "    tl.store(Y_ptr + row_id, mean)\n",
    "\n",
    "def triton_row_mean(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [B, N] on CUDA\n",
    "    returns: [B]\n",
    "    \"\"\"\n",
    "    assert x.is_cuda\n",
    "    B, N = x.shape\n",
    "    y = torch.empty(B, device=x.device, dtype=x.dtype)\n",
    "\n",
    "    BLOCK_SIZE = N  # simple case: one block per row\n",
    "    grid = (B,)\n",
    "\n",
    "    row_mean_kernel[grid](\n",
    "        x, y,\n",
    "        BATCH=B,\n",
    "        N=N,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_warps=4,\n",
    "    )\n",
    "    return y\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. CUDA kernel with CuPy: row-wise mean\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "cuda_row_mean_src = r\"\"\"\n",
    "extern \"C\" __global__\n",
    "void row_mean(const float* __restrict__ x,\n",
    "              float* __restrict__ y,\n",
    "              int B, int N) {\n",
    "    int row = blockIdx.x;\n",
    "    if (row >= B) return;\n",
    "\n",
    "    float sum = 0.0f;\n",
    "    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n",
    "        sum += x[row * N + i];\n",
    "    }\n",
    "\n",
    "    __shared__ float smem[256]; // up to 256 threads\n",
    "    int tid = threadIdx.x;\n",
    "    smem[tid] = sum;\n",
    "    __syncthreads();\n",
    "\n",
    "    // simple reduction in shared memory\n",
    "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
    "        if (tid < stride) {\n",
    "            smem[tid] += smem[tid + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (tid == 0) {\n",
    "        y[row] = smem[0] / (float)N;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "row_mean_kernel_cuda = cp.RawKernel(cuda_row_mean_src, \"row_mean\")\n",
    "\n",
    "def cuda_row_mean(x_torch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Takes a CUDA torch tensor [B, N], uses CuPy to run custom kernel,\n",
    "    returns torch tensor [B].\n",
    "    \"\"\"\n",
    "    assert x_torch.is_cuda\n",
    "    B, N = x_torch.shape\n",
    "    # zero-copy via DLPack\n",
    "    x_cu = cp.fromDlpack(torch.utils.dlpack.to_dlpack(x_torch))\n",
    "    y_cu = cp.empty((B,), dtype=cp.float32)\n",
    "\n",
    "    threads_per_block = 256\n",
    "    blocks = (B,)\n",
    "\n",
    "    row_mean_kernel_cuda(blocks, (threads_per_block,),\n",
    "                         (x_cu, y_cu, B, N))\n",
    "\n",
    "    # back to torch\n",
    "    y_torch = torch.utils.dlpack.from_dlpack(y_cu.toDlpack())\n",
    "    return y_torch\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Benchmark helpers\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class BenchResult:\n",
    "    name: str\n",
    "    time_ms: float\n",
    "\n",
    "def bench(fn, iters=BENCH_ITERS, warmup=WARMUP_ITERS):\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        out = fn()\n",
    "        if isinstance(out, torch.Tensor) and out.is_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        out = fn()\n",
    "        if isinstance(out, torch.Tensor) and out.is_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) * 1000.0 / iters\n",
    "\n",
    "results = []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. PyTorch CPU benchmarks\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== PyTorch CPU ===\")\n",
    "\n",
    "# mean (batch-level: row-wise)\n",
    "def fn_torch_cpu_batch_mean():\n",
    "    return torch_safe_mean(x_torch_cpu, dim=1)  # [B]\n",
    "\n",
    "t = bench(fn_torch_cpu_batch_mean)\n",
    "results.append(BenchResult(\"torch_cpu_batch_mean\", t))\n",
    "print(\"torch_cpu_batch_mean: %.3f ms\" % t)\n",
    "\n",
    "# masked mean (row-wise)\n",
    "def fn_torch_cpu_masked_mean():\n",
    "    return torch_masked_mean(x_torch_cpu, mask_torch_cpu, dim=1)\n",
    "\n",
    "t = bench(fn_torch_cpu_masked_mean)\n",
    "results.append(BenchResult(\"torch_cpu_masked_mean\", t))\n",
    "print(\"torch_cpu_masked_mean: %.3f ms\" % t)\n",
    "\n",
    "# segment mean (over batch dimension)\n",
    "def fn_torch_cpu_segment_mean():\n",
    "    return torch_segment_mean(x_torch_cpu, seg_ids_cpu, num_segments=SEG_K)\n",
    "\n",
    "t = bench(fn_torch_cpu_segment_mean)\n",
    "results.append(BenchResult(\"torch_cpu_segment_mean\", t))\n",
    "print(\"torch_cpu_segment_mean: %.3f ms\" % t)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. PyTorch GPU benchmarks (if available)\n",
    "# ------------------------------------------------------------\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"\\n=== PyTorch GPU ===\")\n",
    "\n",
    "    def fn_torch_gpu_batch_mean():\n",
    "        return torch_safe_mean(x_torch_gpu, dim=1)\n",
    "\n",
    "    t = bench(fn_torch_gpu_batch_mean)\n",
    "    results.append(BenchResult(\"torch_gpu_batch_mean\", t))\n",
    "    print(\"torch_gpu_batch_mean: %.3f ms\" % t)\n",
    "\n",
    "    def fn_torch_gpu_masked_mean():\n",
    "        return torch_masked_mean(x_torch_gpu, mask_torch_gpu, dim=1)\n",
    "\n",
    "    t = bench(fn_torch_gpu_masked_mean)\n",
    "    results.append(BenchResult(\"torch_gpu_masked_mean\", t))\n",
    "    print(\"torch_gpu_masked_mean: %.3f ms\" % t)\n",
    "\n",
    "    def fn_torch_gpu_segment_mean():\n",
    "        return torch_segment_mean(x_torch_gpu, seg_ids_gpu, num_segments=SEG_K)\n",
    "\n",
    "    t = bench(fn_torch_gpu_segment_mean)\n",
    "    results.append(BenchResult(\"torch_gpu_segment_mean\", t))\n",
    "    print(\"torch_gpu_segment_mean: %.3f ms\" % t)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. NumPy CPU benchmarks\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== NumPy CPU ===\")\n",
    "\n",
    "def fn_numpy_batch_mean():\n",
    "    return np_safe_mean(x_np, axis=1)\n",
    "\n",
    "t = bench(fn_numpy_batch_mean)\n",
    "results.append(BenchResult(\"numpy_batch_mean\", t))\n",
    "print(\"numpy_batch_mean: %.3f ms\" % t)\n",
    "\n",
    "def fn_numpy_masked_mean():\n",
    "    return np_masked_mean(x_np, mask_np, axis=1)\n",
    "\n",
    "t = bench(fn_numpy_masked_mean)\n",
    "results.append(BenchResult(\"numpy_masked_mean\", t))\n",
    "print(\"numpy_masked_mean: %.3f ms\" % t)\n",
    "\n",
    "def fn_numpy_segment_mean():\n",
    "    return np_segment_mean(x_np, seg_ids_np, num_segments=SEG_K)\n",
    "\n",
    "t = bench(fn_numpy_segment_mean)\n",
    "results.append(BenchResult(\"numpy_segment_mean\", t))\n",
    "print(\"numpy_segment_mean: %.3f ms\" % t)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9. Triton benchmark (row-wise mean only)\n",
    "# ------------------------------------------------------------\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"\\n=== Triton row-wise mean (CUDA) ===\")\n",
    "\n",
    "    def fn_triton_row_mean():\n",
    "        return triton_row_mean(x_torch_gpu)\n",
    "\n",
    "    t = bench(fn_triton_row_mean)\n",
    "    results.append(BenchResult(\"triton_row_mean\", t))\n",
    "    print(\"triton_row_mean: %.3f ms\" % t)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10. CUDA (CuPy) kernel benchmark (row-wise mean only)\n",
    "# ------------------------------------------------------------\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"\\n=== CUDA (CuPy) row-wise mean ===\")\n",
    "\n",
    "    def fn_cuda_row_mean():\n",
    "        return cuda_row_mean(x_torch_gpu)\n",
    "\n",
    "    t = bench(fn_cuda_row_mean)\n",
    "    results.append(BenchResult(\"cuda_row_mean\", t))\n",
    "    print(\"cuda_row_mean: %.3f ms\" % t)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 11. Summary\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== Summary (ms per call) ===\")\n",
    "for r in results:\n",
    "    print(f\"{r.name:30s}: {r.time_ms:8.3f} ms\")"
   ],
   "metadata": {
    "id": "cF_qp5KNuyyz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Math trick in self attention\n",
    "Karpathy YT https://www.youtube.com/watch?v=kCc8FmEb1nY 42:27\n"
   ],
   "metadata": {
    "id": "wUPZFtNNE40E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n",
    "#we want to look at past tokens from currnt positio\n",
    "# Batch, Time, Channels\n",
    "#simplest way to communicate with past tokens is to take average of tokens before\n",
    "# current token. This vector of 5 past tokens with an average becomes\n",
    "#\n",
    "xbow = torch.zeros(B,T,C)\n",
    "for batch in range(B):\n",
    "  for time in range(T):\n",
    "    xprev = x[batch, time+1, ] #t,C\n",
    "    xbow[b,t] = torch.mean()\n"
   ],
   "metadata": {
    "id": "aoQwFMnrAw1U"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}