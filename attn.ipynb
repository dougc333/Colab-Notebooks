{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvo8L6H3NlYA0lPMrxA4Ia"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Attention**"],"metadata":{"id":"-dHG38b82AXJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZZqFCNJ1_zs","executionInfo":{"status":"ok","timestamp":1767041064694,"user_tz":480,"elapsed":20479,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"a9487b56-5f74-409c-d6b4-489c10120cc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/'Colab Notebooks'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I-9OpGAI2m_z","executionInfo":{"status":"ok","timestamp":1767041066746,"user_tz":480,"elapsed":382,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"981a861e-0d6d-4a2a-c6c1-87552dd0b158"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["def sdpa(Wq, Wk, Wv, text):\n","  embed = get_embeddings(text)\n","  Q = torch.matmul(embed, Wq)\n","  K = torch.matmul(embed, Wk)\n","  V = torch.matmul(embed, Wv)\n","  print(Q.shape,K.shape,embed.shape)\n","  attn_scores = torch.matmul(Q, torch.transpose(K,1,2)) / torch.sqrt(torch.tensor(embed.shape[2]))\n","  #d_k is the embedding dimension\n","  #do we need -1?\n","  print(\"attn_scores shape\",attn_scores.shape)\n","  scaled = torch.softmax(attn_scores,dim=-1)\n","  attn = torch.matmul(scaled, V)\n","\n","  return attn\n","\n","Wq = torch.rand(768,768)\n","Wk = torch.rand(768,768)\n","Wv = torch.rand(768,768)\n","\n","print(sdpa(Wq, Wk, Wv, \"hello\").shape)\n","\n"],"metadata":{"id":"2EDTjVFylzd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)\\mathbf{W}^O$$"],"metadata":{"id":"T9_MSkVDBYcM"}},{"cell_type":"markdown","source":["$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$"],"metadata":{"id":"rPLpWeZYl3is"}},{"cell_type":"code","source":["#karpathy lets build gpt 1:02:00\n","torch.manual_seed(1337)\n","B,T,C = 4, 8, 32\n","x = torch.randn(B,T,C)\n","\n","tril = torch.tril(torch.ones(T,T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril==0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","\n","out = wei @ x\n"],"metadata":{"id":"f7bK3fYtlzgg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Triton SDPA\n","\n","When $\\text{softmax}$ is applied:$$\\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\text{ with causal mask}\\right)$$The $e^{-\\infty}$ terms become zero, effectively giving zero attention weight to all future tokens."],"metadata":{"id":"7XTjyLd_DTE4"}},{"cell_type":"code","source":["#karpathy lets build gpt 1:02:00\n","torch.manual_seed(1337)\n","B,T,C = 4, 8, 32\n","x = torch.randn(B,T,C)\n","\n","tril = torch.tril(torch.ones(T,T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril==0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","\n","out = wei @ x\n"],"metadata":{"id":"hz4OTezw5QaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import triton\n","import triton.language as tl\n","import math\n","\n","# Define the Triton Kernel\n","@triton.jit\n","def causal_attention_kernel(\n","    Q, K, V, O,  # Data Pointers for Query, Key, Value, Output\n","    sm_scale,  # Scaling factor: 1/sqrt(d_k)\n","    # Tensors dimensions\n","    Lq, Lk, Lv, Lo,  # Strides for Q, K, V, O\n","    N_CTX,  # Sequence Length (Max tokens)\n","    D_HEAD,  # Head Dimension (d_k)\n","    # Block dimensions (These are fixed by the user when launching)\n","    BLOCK_M: tl.constexpr,\n","    BLOCK_N: tl.constexpr,\n","    BLOCK_DMODEL: tl.constexpr,\n","):\n","    \"\"\"\n","    Computes Causal Masked Scaled Dot-Product Attention.\n","\n","    This kernel implements the online softmax trick to avoid materializing the\n","    full N x N attention matrix in global memory, which is the core principle\n","    of FlashAttention.\n","    \"\"\"\n","    # 1. Block Indexing for parallelization\n","    # Program ID (PID) maps to a Query block M (rows in the attention matrix)\n","    pid_m = tl.program_id(0)\n","\n","    # Initialize a pointer to the output block (O_ptr)\n","    # O is indexed by [pid_m * BLOCK_M, :], scaled by the strides.\n","    offs_om = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n","    offs_n = tl.arange(0, BLOCK_DMODEL)\n","    O_ptr = O + offs_om[:, None] * Lo + offs_n[None, :] * Lk\n","\n","    # Initialize the accumulators for the output (O_i), running maximum (m_i),\n","    # and normalization factor (l_i). These are the core variables for online softmax.\n","    m_i = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n","    l_i = tl.full([BLOCK_M], 0.0, dtype=tl.float32)\n","    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n","\n","    # 2. Load Query block Q_i\n","    # Q is indexed by [pid_m * BLOCK_M, :]\n","    offs_qm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n","    offs_d = tl.arange(0, BLOCK_DMODEL)\n","    Q_ptr = Q + offs_qm[:, None] * Lq + offs_d[None, :] * Lk\n","\n","    # Load Q block and multiply by the scaling factor\n","    q = tl.load(Q_ptr) * sm_scale\n","\n","    # 3. Iterate over blocks of K_j and V_j (tiling along the sequence dimension N)\n","    for start_n in range(0, N_CTX, BLOCK_N):\n","        # Create pointer offsets for the current block K_j and V_j\n","        offs_n_load = start_n + tl.arange(0, BLOCK_N)\n","        offs_k = offs_n_load[None, :] * Lk + offs_d[:, None] * Lq\n","        K_ptr = K + offs_k\n","        V_ptr = V + offs_n_load[:, None] * Lv + offs_d[None, :] * Lk\n","\n","        # Load K_j and V_j block\n","        k = tl.load(K_ptr)\n","        v = tl.load(V_ptr)\n","\n","        # 4. Compute attention scores S_ij = Q_i * K_j^T\n","        # s has shape [BLOCK_M, BLOCK_N]\n","        s = tl.dot(q, k, allow_tf32=True)\n","\n","        # 5. Apply Causal Masking (Prevent attending to future tokens)\n","        # s must be masked where query index > key index.\n","        # This is where the causal constraint (i <= j) is enforced.\n","        mask = offs_qm[:, None] >= offs_n_load[None, :]\n","        s = tl.where(mask, s, float(\"-inf\"))\n","\n","        # 6. Online Softmax Update (Row-wise max and sum)\n","        # This is the core trick for numerical stability and memory efficiency.\n","\n","        # 6a. Compute the new row-wise maximum m_j\n","        m_j = tl.max(s, 1)\n","\n","        # 6b. Update the running maximum m_i\n","        m_new = tl.maximum(m_i, m_j)\n","\n","        # 6c. Compute the exponential terms e_i and e_j\n","        alpha = tl.exp(m_i - m_new)\n","        beta = tl.exp(m_j - m_new)\n","\n","        # 6d. Update the running normalization factor l_i\n","        l_new = alpha * l_i + beta\n","        l_i = l_new\n","\n","        # 6e. Re-scale the previous accumulator acc\n","        acc_scale = alpha / l_i\n","        acc = acc * acc_scale[:, None]\n","\n","        # 6f. Compute the attention weights and update the accumulator\n","        s = s - m_new[:, None]\n","        p = tl.exp(s) * (beta / l_i)[:, None]\n","\n","        # 6g. Update the accumulator: acc_new = acc_old + P_ij * V_j\n","        acc = acc + tl.dot(p, v, allow_tf32=True)\n","\n","        # 6h. Update the running maximum m_i for the next iteration\n","        m_i = m_new\n","\n","    # 7. Write the final result to the output tensor O\n","    # The final output is acc (weighted sum of V) divided by the final normalization l_i\n","    tl.store(O_ptr, acc / l_i[:, None])\n","\n","# ----------------------------------------------------------------------\n","# Python Host Wrapper\n","# ----------------------------------------------------------------------\n","\n","def run_attention_kernel(q, k, v, is_causal=True):\n","    \"\"\"\n","    Runs the Triton attention kernel with torch tensors.\n","\n","    Args:\n","        q (torch.Tensor): Query tensor (L, D).\n","        k (torch.Tensor): Key tensor (L, D).\n","        v (torch.Tensor): Value tensor (L, D).\n","        is_causal (bool): Whether to apply causal masking.\n","    \"\"\"\n","    assert q.shape == k.shape == v.shape, \"Q, K, V must have the same shape (L, D) for self-attention.\"\n","    assert q.is_cuda and k.is_cuda and v.is_cuda, \"Inputs must be on the GPU.\"\n","\n","    N_CTX, D_HEAD = q.shape[0], q.shape[1]\n","\n","    # Hyperparameters: Tune these for performance\n","    BLOCK_M = 64  # Block size for the Query dimension (rows)\n","    BLOCK_N = 64  # Block size for the Key dimension (columns)\n","    BLOCK_DMODEL = D_HEAD # Block size for the Head dimension\n","\n","    # Define the scaling factor: 1/sqrt(d_k)\n","    sm_scale = 1.0 / math.sqrt(D_HEAD)\n","\n","    # Output tensor initialization (N_CTX, D_HEAD)\n","    o = torch.empty_like(q)\n","\n","    # 1D launch grid: we launch one program for each block M (rows) in the sequence\n","    grid = lambda META: (triton.cdiv(N_CTX, META['BLOCK_M']),)\n","\n","    # Kernel call\n","    causal_attention_kernel[grid](\n","        q, k, v, o,  # Pointers\n","        sm_scale,  # Scaling factor\n","        q.stride(0), k.stride(0), v.stride(0), o.stride(0),  # Strides\n","        N_CTX, D_HEAD,\n","        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=BLOCK_DMODEL,\n","        num_warps=4,\n","        num_stages=3\n","    )\n","    return o\n","\n","# ----------------------------------------------------------------------\n","# Example Usage\n","# ----------------------------------------------------------------------\n","if __name__ == '__main__':\n","    # Ensure inputs are in a format compatible with Triton (float16 is typical for perf)\n","    dtype = torch.float16\n","    device = 'cuda'\n","\n","    # Set Sequence Length (N_CTX) and Head Dimension (D_HEAD)\n","    N_CTX = 256\n","    D_HEAD = 64\n","\n","    # Create dummy tensors for Q, K, V (all derived from the same input for self-attention)\n","    # Shape: [Sequence Length, Head Dimension]\n","    Q_data = torch.randn(N_CTX, D_HEAD, dtype=dtype, device=device)\n","    K_data = torch.randn(N_CTX, D_HEAD, dtype=dtype, device=device)\n","    V_data = torch.randn(N_CTX, D_HEAD, dtype=dtype, device=device)\n","\n","    # Run the Triton kernel\n","    output_triton = run_attention_kernel(Q_data, K_data, V_data)\n","\n","    print(f\"Input Shape (Q, K, V): {Q_data.shape}\")\n","    print(f\"Output Shape (O):      {output_triton.shape}\")\n","    print(f\"\\nExample Output (First 5 elements of first row):\\n{output_triton[0, :5]}\")\n","\n","    # For comparison, you would compare this output against a known, verified\n","    # PyTorch implementation (like F.scaled_dot_product_attention with causal=True).\n","    # The numerical results should be close, demonstrating the kernel works."],"metadata":{"id":"7EFU1dQGlzjX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Strategies for cleaning data. Do these work?"],"metadata":{"id":"4DkbbRnl5X4x"}},{"cell_type":"code","source":["candidate_policies = [\n","    {\"theta\": 0.05, \"noisy_weight\": 0.5},\n","    {\"theta\": 0.10, \"noisy_weight\": 0.5},\n","    {\"theta\": 0.10, \"noisy_weight\": 0.2},\n","    {\"theta\": 0.15, \"noisy_weight\": 0.2},\n","    {\"theta\": 0.20, \"noisy_weight\": 0.0},  # drop most uncertain\n","    {\"theta\": 0.30, \"noisy_weight\": 0.0},\n","]\n","\n","def apply_policy(policy, conf):\n","    theta = policy[\"theta\"]\n","    noisy_weight = policy[\"noisy_weight\"]\n","    w = np.ones_like(conf, dtype=float)\n","    w[conf < theta] = noisy_weight\n","    return w\n","\n","n_trials = 20\n","epsilon = 0.3\n","K = len(candidate_policies)\n","Q = np.zeros(K)\n","N = np.zeros(K)\n","\n","for t in range(n_trials):\n","    if np.random.rand() < epsilon:\n","        k = np.random.randint(K)\n","    else:\n","        k = np.argmax(Q)\n","\n","    policy = candidate_policies[k]\n","    sample_weight = apply_policy(policy, conf)\n","\n","    clf = train_model(X_train, y_train, sample_weight=sample_weight)\n","    reward = eval_model(clf, X_val, y_val)\n","\n","    N[k] += 1\n","    Q[k] += (reward - Q[k]) / N[k]\n","\n","    print(f\"Iter {t:02d} | trial policy {k} {policy} | F1={reward:.4f}\")\n","\n","best_k = np.argmax(Q)\n","best_policy = candidate_policies[best_k]\n","print(\"\\nBase F1:\", base_f1)\n","print(\"Best policy from bandit:\", best_policy, \"Estimated F1:\", Q[best_k])"],"metadata":{"id":"ZwGGw2nTdgai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Literal, Dict, Any, Tuple, Callable\n","\n","class NoisyDatasetCleaner:\n","    \"\"\"\n","    A generic wrapper for noisy supervised datasets.\n","    Strategies:\n","      - 'bandit_weight'\n","      - 'cleanlab'\n","      - 'none'\n","    You can extend it with active-relabelling logic.\n","    \"\"\"\n","    def __init__(self,\n","                 strategy: Literal[\"none\", \"bandit_weight\", \"cleanlab\"] = \"none\",\n","                 clf_factory: Callable[[], Any] = None,\n","                 strategy_kwargs: Optional[Dict[str, Any]] = None):\n","        self.strategy = strategy\n","        self.clf_factory = clf_factory or (lambda: LogisticRegression(max_iter=300, n_jobs=-1))\n","        self.strategy_kwargs = strategy_kwargs or {}\n","        self.sample_weight_ = None\n","        self._cleanlab_model = None\n","\n","    def fit(self, X, y) -> \"NoisyDatasetCleaner\":\n","        if self.strategy == \"none\":\n","            self.sample_weight_ = np.ones(len(y), dtype=float)\n","\n","        elif self.strategy == \"bandit_weight\":\n","            self._fit_bandit_weight(X, y)\n","\n","        elif self.strategy == \"cleanlab\":\n","            self._fit_cleanlab(X, y)\n","\n","        else:\n","            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n","\n","        return self\n","\n","    def _fit_bandit_weight(self, X, y):\n","        # train base model\n","        base_clf = self.clf_factory()\n","        base_clf.fit(X, y)\n","        p = base_clf.predict_proba(X)[:, 1]\n","        conf = np.abs(p - 0.5)\n","\n","        # simple discrete bandit over thresholds, like earlier\n","        candidate_policies = self.strategy_kwargs.get(\"candidate_policies\") or [\n","            {\"theta\": 0.05, \"noisy_weight\": 0.5},\n","            {\"theta\": 0.10, \"noisy_weight\": 0.5},\n","            {\"theta\": 0.10, \"noisy_weight\": 0.2},\n","            {\"theta\": 0.15, \"noisy_weight\": 0.2},\n","            {\"theta\": 0.20, \"noisy_weight\": 0.0},\n","            {\"theta\": 0.30, \"noisy_weight\": 0.0},\n","        ]\n","        n_trials = self.strategy_kwargs.get(\"n_trials\", 10)\n","        epsilon  = self.strategy_kwargs.get(\"epsilon\", 0.3)\n","        X_train, X_val, y_train, y_val, conf_train = train_test_split(\n","            X, y, conf, test_size=0.2, random_state=0, stratify=y\n","        )\n","\n","        def apply_policy(policy, conf_vec):\n","            w = np.ones_like(conf_vec, dtype=float)\n","            w[conf_vec < policy[\"theta\"]] = policy[\"noisy_weight\"]\n","            return w\n","\n","        K = len(candidate_policies)\n","        Q = np.zeros(K)\n","        N = np.zeros(K)\n","\n","        for _ in range(n_trials):\n","            if np.random.rand() < epsilon:\n","                k = np.random.randint(K)\n","            else:\n","                k = np.argmax(Q)\n","\n","            policy = candidate_policies[k]\n","            weights_train = apply_policy(policy, conf_train)\n","\n","            clf = self.clf_factory()\n","            clf.fit(X_train, y_train, sample_weight=weights_train)\n","            y_pred_val = clf.predict(X_val)\n","            reward = f1_score(y_val, y_pred_val)\n","\n","            N[k] += 1\n","            Q[k] += (reward - Q[k]) / N[k]\n","\n","        best_k = np.argmax(Q)\n","        best_policy = candidate_policies[best_k]\n","\n","        # final weights on full data\n","        self.sample_weight_ = apply_policy(best_policy, conf)\n","\n","    def _fit_cleanlab(self, X, y):\n","        from cleanlab.classification import CleanLearning\n","\n","        base_clf = self.clf_factory()\n","        cl = CleanLearning(clf=base_clf)\n","        cl.fit(X, y)\n","        self._cleanlab_model = cl\n","\n","        # label issues summary -> create weights\n","        issues = cl.get_label_issues()\n","        is_issue = issues[\"is_label_issue\"].values\n","        # simple scheme: 0.3 weight for suspected issues\n","        w = np.ones(len(y), dtype=float)\n","        w[is_issue] = 0.3\n","        self.sample_weight_ = w\n","\n","    def get_weights(self) -> np.ndarray:\n","        if self.sample_weight_ is None:\n","            raise RuntimeError(\"Call fit() first\")\n","        return self.sample_weight_\n","\n","    def fit_clean_model(self, X, y):\n","        \"\"\"Train a final classifier using the learned sample weights.\"\"\"\n","        w = self.get_weights()\n","        clf = self.clf_factory()\n","        clf.fit(X, y, sample_weight=w)\n","        return clf"],"metadata":{"id":"Mm12LAlMKVO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleaner = NoisyDatasetCleaner(strategy=\"bandit_weight\")\n","cleaner.fit(X_train, y_train)\n","weights = cleaner.get_weights()\n","\n","clf_clean = cleaner.fit_clean_model(X_train, y_train)\n","f1_clean = eval_model(clf_clean, X_val, y_val)\n","print(\"F1 with NoisyDatasetCleaner (bandit_weight):\", f1_clean)\n","\n","# Or:\n","cleaner_cl = NoisyDatasetCleaner(strategy=\"cleanlab\")\n","cleaner_cl.fit(X_train, y_train)\n","clf_cleanlab = cleaner_cl.fit_clean_model(X_train, y_train)\n","print(\"F1 with NoisyDatasetCleaner (cleanlab):\", eval_model(clf_cleanlab, X_val, y_val))"],"metadata":{"id":"pSbwmU1Cvtc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create KV server wo vllm\n"],"metadata":{"id":"RQqpLwGO5kku"}},{"cell_type":"code","source":["# tinyllama_server_kv_stream.py\n","import asyncio\n","import json\n","import time\n","import uuid\n","from typing import List, Optional\n","\n","import torch\n","from fastapi import FastAPI\n","from fastapi.responses import StreamingResponse\n","from pydantic import BaseModel\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# ---------------------------------------------------------\n","# Model load\n","# ---------------------------------------------------------\n","MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","\n","print(f\"Loading model: {MODEL_NAME}\")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","dtype = torch.float16 if device == \"cuda\" else torch.float32\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    torch_dtype=dtype,\n","    device_map=\"auto\" if device == \"cuda\" else None,\n",")\n","model.to(device)\n","model.eval()\n","\n","# ---------------------------------------------------------\n","# FastAPI + schemas\n","# ---------------------------------------------------------\n","app = FastAPI(title=\"TinyLlama KV streaming demo\")\n","\n","class ChatMessage(BaseModel):\n","    role: str\n","    content: str\n","\n","class ChatRequest(BaseModel):\n","    model: Optional[str] = None\n","    messages: List[ChatMessage]\n","    max_tokens: int = 128\n","    temperature: float = 0.7\n","    top_p: float = 1.0\n","    n: int = 1\n","    stream: bool = False\n","    stop: Optional[List[str]] = None\n","\n","# ---------------------------------------------------------\n","# Prompt formatting\n","# ---------------------------------------------------------\n","def build_prompt(messages: List[ChatMessage]) -> str:\n","    parts = []\n","    for m in messages:\n","        if m.role == \"user\":\n","            parts.append(f\"User: {m.content}\")\n","        elif m.role == \"assistant\":\n","            parts.append(f\"Assistant: {m.content}\")\n","        else:\n","            parts.append(f\"{m.role.capitalize()}: {m.content}\")\n","    parts.append(\"Assistant:\")\n","    return \"\\n\".join(parts)\n","\n","# ---------------------------------------------------------\n","# Sampling helpers\n","# ---------------------------------------------------------\n","\n","def sample_next_token(\n","    logits: torch.Tensor,\n","    temperature: float,\n","    top_p: float,\n",") -> int:\n","    \"\"\"\n","    logits: [vocab_size] (for a single position)\n","    returns: int token id\n","    \"\"\"\n","    if temperature <= 0:\n","        # greedy\n","        return int(torch.argmax(logits, dim=-1).item())\n","\n","    # temperature scaling\n","    logits = logits / temperature\n","\n","    # softmax to probs\n","    probs = torch.softmax(logits, dim=-1)\n","\n","    if top_p < 1.0:\n","        # nucleus sampling\n","        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n","        cumulative = torch.cumsum(sorted_probs, dim=-1)\n","\n","        # keep minimal set that sums to >= top_p\n","        mask = cumulative - sorted_probs > top_p\n","        sorted_probs[mask] = 0\n","        sorted_probs = sorted_probs / sorted_probs.sum()\n","        idx = torch.multinomial(sorted_probs, 1)\n","        token_id = sorted_indices[idx]\n","        return int(token_id.item())\n","    else:\n","        # plain multinomial over full vocab\n","        token_id = torch.multinomial(probs, 1)\n","        return int(token_id.item())\n","\n","# ---------------------------------------------------------\n","# Non-streaming (simple single-call generate)\n","# ---------------------------------------------------------\n","\n","async def handle_non_stream(req: ChatRequest):\n","    prompt = build_prompt(req.messages)\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        gen_ids = model.generate(\n","            **inputs,\n","            max_new_tokens=req.max_tokens,\n","            temperature=req.temperature,\n","            top_p=req.top_p,\n","            do_sample=req.temperature > 0,\n","            pad_token_id=tokenizer.pad_token_id,\n","            eos_token_id=tokenizer.eos_token_id,\n","        )\n","\n","    input_len = inputs[\"input_ids\"].shape[1]\n","    new_tokens = gen_ids[0, input_len:]\n","    text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n","\n","    prompt_tokens = int(inputs[\"input_ids\"].numel())\n","    completion_tokens = int(new_tokens.numel())\n","    total_tokens = prompt_tokens + completion_tokens\n","\n","    resp = {\n","        \"id\": f\"chatcmpl-{uuid.uuid4().hex}\",\n","        \"object\": \"chat.completion\",\n","        \"created\": int(time.time()),\n","        \"model\": req.model or MODEL_NAME,\n","        \"choices\": [\n","            {\n","                \"index\": 0,\n","                \"message\": {\"role\": \"assistant\", \"content\": text},\n","                \"finish_reason\": \"stop\",\n","            }\n","        ],\n","        \"usage\": {\n","            \"prompt_tokens\": prompt_tokens,\n","            \"completion_tokens\": completion_tokens,\n","            \"total_tokens\": total_tokens,\n","        },\n","    }\n","    return resp\n","\n","# ---------------------------------------------------------\n","# Real per-token streaming with KV cache\n","# ---------------------------------------------------------\n","\n","async def handle_stream(req: ChatRequest):\n","    prompt = build_prompt(req.messages)\n","    request_id = f\"chatcmpl-{uuid.uuid4().hex}\"\n","    model_name = req.model or MODEL_NAME\n","    created = int(time.time())\n","\n","    async def event_stream():\n","        # 1) initial input: full prompt\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","        input_ids = inputs[\"input_ids\"]  # [1, seq_len]\n","        attention_mask = inputs[\"attention_mask\"]\n","\n","        max_new_tokens = req.max_tokens\n","        eos_id = tokenizer.eos_token_id\n","\n","        # Send initial chunk with role (OpenAI-style)\n","        first_chunk = {\n","            \"id\": request_id,\n","            \"object\": \"chat.completion.chunk\",\n","            \"created\": created,\n","            \"model\": model_name,\n","            \"choices\": [\n","                {\n","                    \"index\": 0,\n","                    \"delta\": {\"role\": \"assistant\"},\n","                    \"finish_reason\": None,\n","                }\n","            ],\n","        }\n","        yield f\"data: {json.dumps(first_chunk)}\\n\\n\"\n","\n","        past_key_values = None\n","        generated = []\n","        finish_reason = None\n","\n","        for step in range(max_new_tokens):\n","            # 2) forward pass: either full prompt (first step) or just last token (subsequent steps)\n","            with torch.no_grad():\n","                outputs = model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    past_key_values=past_key_values,\n","                    use_cache=True,\n","                )\n","\n","            logits = outputs.logits[:, -1, :]  # [1, vocab]\n","            past_key_values = outputs.past_key_values\n","\n","            # 3) sample next token\n","            next_token_id = sample_next_token(\n","                logits[0], req.temperature, req.top_p\n","            )\n","            generated.append(next_token_id)\n","\n","            if next_token_id == eos_id:\n","                finish_reason = \"stop\"\n","                break\n","\n","            # 4) decode just this token to text piece\n","            token_text = tokenizer.decode([next_token_id], skip_special_tokens=True)\n","\n","            if token_text:\n","                chunk = {\n","                    \"id\": request_id,\n","                    \"object\": \"chat.completion.chunk\",\n","                    \"created\": created,\n","                    \"model\": model_name,\n","                    \"choices\": [\n","                        {\n","                            \"index\": 0,\n","                            \"delta\": {\"content\": token_text},\n","                            \"finish_reason\": None,\n","                        }\n","                    ],\n","                }\n","                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n","\n","            # 5) prepare for next step: feed only this token, no need to resend the whole prompt\n","            input_ids = torch.tensor([[next_token_id]], device=device)\n","            attention_mask = None  # not strictly needed when using past with single token\n","\n","            # let event loop breathe a bit\n","            await asyncio.sleep(0)\n","\n","        if finish_reason is None:\n","            finish_reason = \"length\"\n","\n","        # final empty delta with finish_reason\n","        done_chunk = {\n","            \"id\": request_id,\n","            \"object\": \"chat.completion.chunk\",\n","            \"created\": created,\n","            \"model\": model_name,\n","            \"choices\": [\n","                {\n","                    \"index\": 0,\n","                    \"delta\": {},\n","                    \"finish_reason\": finish_reason,\n","                }\n","            ],\n","        }\n","        yield f\"data: {json.dumps(done_chunk)}\\n\\n\"\n","        yield \"data: [DONE]\\n\\n\"\n","\n","    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n","\n","# ---------------------------------------------------------\n","# Main endpoint\n","# ---------------------------------------------------------\n","\n","@app.post(\"/v1/chat/completions\")\n","async def chat_completions(req: ChatRequest):\n","    if req.stream:\n","        return await handle_stream(req)\n","    else:\n","        return await handle_non_stream(req)"],"metadata":{"id":"O1TEubioKVYl"},"execution_count":null,"outputs":[]}]}