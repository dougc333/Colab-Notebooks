{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyO7gN2JE3hfzocA9TJWKHNT"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dougc333/Colab-Notebooks/blob/main/char_pytorch_trans.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rw2l6nK4Ucm5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766163362398,
     "user_tz": 480,
     "elapsed": 735,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "bfa1dbe2-9df9-4210-e844-a2e4ef043adf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#char_pytorch_trans.ipynb\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/drive/MyDrive/'Colab Notebooks'"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Cr6lsqDUc6B",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766163362593,
     "user_tz": 480,
     "elapsed": 6,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "94334d41-59cb-48c2-d4f0-bdadcd074242"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Model definition: 2-layer Transformer LM\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    d_ff: int = 256\n",
    "    max_seq_len: int = 128\n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "class TinyTransformerLM(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.pos_emb = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.n_heads,\n",
    "            dim_feedforward=cfg.d_ff,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True,  # (B, T, C)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=cfg.n_layers,\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.normal_(self.tok_emb.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        idx: LongTensor [B, T], token indices\n",
    "        returns: logits [B, T, vocab_size]\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "\n",
    "        # Token + positional embeddings\n",
    "        tok = self.tok_emb(idx)                      # [B, T, C]\n",
    "        pos_ids = torch.arange(T, device=device)     # [T]\n",
    "        pos = self.pos_emb(pos_ids)[None, :, :]      # [1, T, C]\n",
    "        x = tok + pos                                # [B, T, C]\n",
    "\n",
    "        # Causal mask so position t cannot attend to >t\n",
    "        # PyTorch expects (T, T) mask for encoder with batch_first=True\n",
    "        mask = torch.triu(torch.ones(T, T, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        x = self.transformer(x, mask)                # [B, T, C]\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)                     # [B, T, V]\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens: int, temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        Autoregressive sampling.\n",
    "        idx: [B, T] initial context\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop if longer than max_seq_len\n",
    "            idx_cond = idx[:, -self.cfg.max_seq_len:]\n",
    "            logits = self(idx_cond)                  # [B, T, V]\n",
    "            logits = logits[:, -1, :]                # last step\n",
    "            logits = logits / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # [B, 1]\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Tiny char-level dataset utilities\n",
    "# ============================================================\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.itos[int(i)] for i in ids)\n",
    "\n",
    "\n",
    "def make_batches(data_ids, block_size, batch_size, device):\n",
    "    \"\"\"\n",
    "    Yield (x, y) batches where y is next-token target.\n",
    "    \"\"\"\n",
    "    n = len(data_ids) - block_size\n",
    "    while True:\n",
    "        ix = torch.randint(0, n, (batch_size,))\n",
    "        x = torch.stack(\n",
    "            [torch.tensor(data_ids[i:i + block_size]) for i in ix]\n",
    "        )\n",
    "        y = torch.stack(\n",
    "            [torch.tensor(data_ids[i + 1:i + block_size + 1]) for i in ix]\n",
    "        )\n",
    "        yield x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Demo: train + inference benchmark\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # Toy training text (replace with any corpus you like)\n",
    "    text = (\n",
    "        \"To be, or not to be, that is the question:\\n\"\n",
    "        \"Whether 'tis nobler in the mind to suffer\\n\"\n",
    "        \"The slings and arrows of outrageous fortune,\\n\"\n",
    "        \"Or to take arms against a sea of troubles\\n\"\n",
    "        \"And by opposing end them.\\n\"\n",
    "    )\n",
    "    tokenizer = CharTokenizer(text)\n",
    "    data_ids = tokenizer.encode(text)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    block_size = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    cfg = ModelConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=128,\n",
    "        n_heads=4,\n",
    "        n_layers=2,\n",
    "        d_ff=256,\n",
    "        max_seq_len=block_size,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "    model = TinyTransformerLM(cfg).to(device)\n",
    "\n",
    "    print(\"Model parameters:\",\n",
    "          sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    train_iter = make_batches(data_ids, block_size, batch_size, device)\n",
    "\n",
    "    # --------- Training loop (very small, just to show it learns) ----------\n",
    "    model.train()\n",
    "    n_steps = 300\n",
    "    for step in range(1, n_steps + 1):\n",
    "        x, y = next(train_iter)\n",
    "        logits = model(x)                            # [B, T, V]\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, cfg.vocab_size),\n",
    "            y.view(-1),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"step {step}/{n_steps}, loss={loss.item():.4f}\")\n",
    "\n",
    "    # ========================================================\n",
    "    # 4. Qualitative sample\n",
    "    # ========================================================\n",
    "    model.eval()\n",
    "    ctx = \"To be\"\n",
    "    ctx_ids = torch.tensor([tokenizer.encode(ctx)], device=device)\n",
    "    out_ids = model.generate(ctx_ids, max_new_tokens=80, temperature=0.8)[0]\n",
    "    print(\"\\n--- Sample generation ---\")\n",
    "    print(tokenizer.decode(out_ids.tolist()))\n",
    "\n",
    "    # ========================================================\n",
    "    # 5. Inference performance benchmark\n",
    "    # ========================================================\n",
    "    print(\"\\n--- Inference benchmark ---\")\n",
    "    B = 8\n",
    "    T = block_size\n",
    "    dummy_input = torch.randint(\n",
    "        0, cfg.vocab_size, (B, T), device=device\n",
    "    )\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "\n",
    "    # Measure forward pass throughput\n",
    "    n_iters = 100\n",
    "    torch.cuda.synchronize(device) if device == \"cuda\" else None\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_iters):\n",
    "            _ = model(dummy_input)\n",
    "    torch.cuda.synchronize(device) if device == \"cuda\" else None\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    total_time = t1 - t0\n",
    "    avg_time = total_time / n_iters\n",
    "    tokens_per_batch = B * T\n",
    "    toks_per_sec = tokens_per_batch / avg_time\n",
    "\n",
    "    print(f\"Forward pass: B={B}, T={T}\")\n",
    "    print(f\"Avg latency: {avg_time*1e3:.3f} ms\")\n",
    "    print(f\"Throughput:  {toks_per_sec:.0f} tokens/sec\")\n",
    "\n",
    "    # Autoregressive generation timing\n",
    "    gen_len = 64\n",
    "    B_gen = 1\n",
    "    start_ids = torch.randint(\n",
    "        0, cfg.vocab_size, (B_gen, 1), device=device\n",
    "    )\n",
    "\n",
    "    torch.cuda.synchronize(device) if device == \"cuda\" else None\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(start_ids, max_new_tokens=gen_len, temperature=1.0)\n",
    "    torch.cuda.synchronize(device) if device == \"cuda\" else None\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    gen_time = t1 - t0\n",
    "    print(\n",
    "        f\"Generate {gen_len} tokens (B={B_gen}): \"\n",
    "        f\"{gen_time*1e3:.2f} ms \"\n",
    "        f\"({gen_len/gen_time:.0f} tok/sec)\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pg_TXBCkUdD7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766162900564,
     "user_tz": 480,
     "elapsed": 12939,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "408db141-fbc8-4627-efef-48e30041a934"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "Model parameters: 0.281344 M\n",
      "step 50/300, loss=2.1429\n",
      "step 100/300, loss=1.5502\n",
      "step 150/300, loss=0.9286\n",
      "step 200/300, loss=0.4686\n",
      "step 250/300, loss=0.3410\n",
      "step 300/300, loss=0.2422\n",
      "\n",
      "--- Sample generation ---\n",
      "To be, the mind to suffer\n",
      "The slings and arows of orageous fortune,\n",
      "Or to take arms a\n",
      "\n",
      "--- Inference benchmark ---\n",
      "Forward pass: B=8, T=64\n",
      "Avg latency: 1.106 ms\n",
      "Throughput:  463084 tokens/sec\n",
      "Generate 64 tokens (B=1): 144.67 ms (442 tok/sec)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers"
   ],
   "metadata": {
    "id": "KS4ZEUAXioru",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766166369586,
     "user_tz": 480,
     "elapsed": 10281,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "class TinyConfig(PretrainedConfig):\n",
    "    model_type = \"tiny_transformer_lm\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size: int = 0,        # <-- make optional\n",
    "                 d_model: int = 128,\n",
    "                 n_heads: int = 4,\n",
    "                 n_layers: int = 2,\n",
    "                 d_ff: int = 256,\n",
    "                 max_seq_len: int = 128,\n",
    "                 **kwargs):\n",
    "        # Call base class to let HF handle extra generation kwargs\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "\n",
    "class TinyHFModel(PreTrainedModel):\n",
    "    config_class = TinyConfig\n",
    "\n",
    "    def __init__(self, config: TinyConfig):\n",
    "        super().__init__(config)\n",
    "        self.inner = TinyTransformerLM(\n",
    "            ModelConfig(\n",
    "                vocab_size=config.vocab_size,\n",
    "                d_model=config.d_model,\n",
    "                n_heads=config.n_heads,\n",
    "                n_layers=config.n_layers,\n",
    "                d_ff=config.d_ff,\n",
    "                max_seq_len=config.max_seq_len,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        logits = self.inner(input_ids)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config.vocab_size),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, **kwargs):\n",
    "        return self.inner.generate(input_ids, max_new_tokens, temperature)"
   ],
   "metadata": {
    "id": "AwzazUxCirnA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766166670302,
     "user_tz": 480,
     "elapsed": 20,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.itos[int(i)] for i in ids)\n",
    "\n",
    "\n",
    "def make_batches(data_ids, block_size, batch_size, device):\n",
    "    n = len(data_ids) - block_size\n",
    "    while True:\n",
    "        ix = torch.randint(0, n, (batch_size,))\n",
    "        x = torch.stack(\n",
    "            [torch.tensor(data_ids[i:i + block_size]) for i in ix]\n",
    "        )\n",
    "        y = torch.stack(\n",
    "            [torch.tensor(data_ids[i + 1:i + block_size + 1]) for i in ix]\n",
    "        )\n",
    "        yield x.to(device), y.to(device)"
   ],
   "metadata": {
    "id": "zQWVtIWZuQaQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766166673769,
     "user_tz": 480,
     "elapsed": 11,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# toy training text\n",
    "text = (\n",
    "    \"To be, or not to be, that is the question:\\n\"\n",
    "    \"Whether 'tis nobler in the mind to suffer\\n\"\n",
    "    \"The slings and arrows of outrageous fortune,\\n\"\n",
    "    \"Or to take arms against a sea of troubles\\n\"\n",
    "    \"And by opposing end them.\\n\"\n",
    ")\n",
    "\n",
    "tokenizer = CharTokenizer(text)\n",
    "data_ids = tokenizer.encode(text)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "block_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "cfg = TinyConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    d_ff=256,\n",
    "    max_seq_len=block_size,\n",
    ")\n",
    "model = TinyHFModel(cfg).to(device)\n",
    "\n",
    "print(\"Params (M):\", sum(p.numel() for p in model.parameters()) / 1e6)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "train_iter = make_batches(data_ids, block_size, batch_size, device)\n",
    "\n",
    "model.train()\n",
    "n_steps = 200\n",
    "\n",
    "for step in range(1, n_steps + 1):\n",
    "    x, y = next(train_iter)\n",
    "    out = model(x, labels=y)\n",
    "    loss = out[\"loss\"]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"step {step}/{n_steps}, loss={loss.item():.4f}\")\n",
    "\n",
    "# quick sample\n",
    "model.eval()\n",
    "ctx = \"To be\"\n",
    "ctx_ids = torch.tensor([tokenizer.encode(ctx)], device=device)\n",
    "with torch.no_grad():\n",
    "    gen_ids = model.generate(ctx_ids, max_new_tokens=80, temperature=0.8)[0]\n",
    "print(\"\\nSample:\")\n",
    "print(tokenizer.decode(gen_ids.tolist()))\n",
    "\n",
    "# ---- Save as HF checkpoint ----\n",
    "save_dir = \"tiny_tlm\"\n",
    "model.save_pretrained(save_dir)\n",
    "\n",
    "# save a minimal tokenizer mapping for your own use (NOT a full HF tokenizer)\n",
    "import json, os\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "with open(os.path.join(save_dir, \"char_vocab.json\"), \"w\") as f:\n",
    "    json.dump(tokenizer.stoi, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nSaved TinyHFModel + char vocab to: {save_dir}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d-Jk6cxuQcr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766166678279,
     "user_tz": 480,
     "elapsed": 2466,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "1914a629-53a2-4fbb-f64e-6348156f3cce"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "Params (M): 0.281344\n",
      "step 50/200, loss=2.1444\n",
      "step 100/200, loss=1.5407\n",
      "step 150/200, loss=0.9207\n",
      "step 200/200, loss=0.5086\n",
      "\n",
      "Sample:\n",
      "To be, to to thetis nond sufeagertous founeage,\n",
      "Ans f by ound traropos es f os oslesu\n",
      "\n",
      "Saved TinyHFModel + char vocab to: tiny_tlm\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 3 options\n",
    "from tiny_model_def import TinyConfig, TinyHFModel  # your code file\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load config + model from the saved folder\n",
    "config = TinyConfig.from_pretrained(\"tiny_tlm\")\n",
    "model = TinyHFModel.from_pretrained(\"tiny_tlm\", config=config).to(device)\n",
    "\n",
    "# You also need a tokenizer; for char-level, use the one you built earlier:\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import json, os\n",
    "\n",
    "with open(os.path.join(\"tiny_tlm\", \"char_vocab.json\")) as f:\n",
    "    stoi = json.load(f)\n",
    "\n",
    "# Build a simple tokenizer around the char vocab\n",
    "itos = {v: k for k, v in stoi.items()}\n",
    "vocab = stoi.copy()\n",
    "\n",
    "tok = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=None,  # we\u2019ll use custom encode/decode\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    ")\n",
    "tok.vocab = vocab\n",
    "tok._tokenizer = None  # (for simple demo; for real use you\u2019d wire up `tokenizers`)\n",
    "\n",
    "def encode(s):\n",
    "    return [vocab.get(ch, vocab[\"<unk>\"]) for ch in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join(itos[int(i)] for i in ids if int(i) in itos)\n",
    "\n",
    "# Use the model directly (simplest) \u2013 no pipeline:\n",
    "model.eval()\n",
    "prompt = \"To be\"\n",
    "ids = torch.tensor([encode(prompt)], device=device)\n",
    "with torch.no_grad():\n",
    "    out_ids = model.generate(ids, max_new_tokens=80)[0]\n",
    "print(decode(out_ids.tolist()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "QWtTzjyAuQhW",
    "executionInfo": {
     "status": "error",
     "timestamp": 1766167186696,
     "user_tz": 480,
     "elapsed": 838,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "9a852370-94c5-4d0e-a21e-0a6ba58ecbae"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiny_model_def'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3828686301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtiny_model_def\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTinyConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTinyHFModel\u001b[0m  \u001b[0;31m# your code file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiny_model_def'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ],
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile tiny_model_def.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    d_ff: int = 256\n",
    "    max_seq_len: int = 128\n",
    "    dropout: float = 0.1\n",
    "\n",
    "class TinyTransformerLM(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TinyConfig(PretrainedConfig):\n",
    "    model_type = \"tiny_transformer_lm\"\n",
    "    def __init__(self,\n",
    "                 vocab_size: int = 0,\n",
    "                 d_model=128,\n",
    "                 n_heads=4,\n",
    "                 n_layers=2,\n",
    "                 d_ff=256,\n",
    "                 max_seq_len=128,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "class TinyHFModel(PreTrainedModel):\n",
    "    config_class = TinyConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.inner = TinyTransformerLM(\n",
    "            ModelConfig(\n",
    "                vocab_size=config.vocab_size,\n",
    "                d_model=config.d_model,\n",
    "                n_heads=config.n_heads,\n",
    "                n_layers=config.n_layers,\n",
    "                d_ff=config.d_ff,\n",
    "                max_seq_len=config.max_seq_len,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, labels=None, **kwargs):\n",
    "        logits = self.inner(input_ids)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config.vocab_size),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, **kwargs):\n",
    "        return self.inner.generate(input_ids, max_new_tokens, temperature)"
   ],
   "metadata": {
    "id": "-ifw0jbhuQjb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Model definition: 2-layer Transformer LM\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    d_ff: int = 256\n",
    "    max_seq_len: int = 128\n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "class TinyTransformerLM(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.pos_emb = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.n_heads,\n",
    "            dim_feedforward=cfg.d_ff,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True,  # (B, T, C)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=cfg.n_layers,\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.normal_(self.tok_emb.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        idx: LongTensor [B, T], token indices\n",
    "        returns: logits [B, T, vocab_size]\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "\n",
    "        # Token + positional embeddings\n",
    "        tok = self.tok_emb(idx)                      # [B, T, C]\n",
    "        pos_ids = torch.arange(T, device=device)     # [T]\n",
    "        pos = self.pos_emb(pos_ids)[None, :, :]      # [1, T, C]\n",
    "        x = tok + pos                                # [B, T, C]\n",
    "\n",
    "        # Causal mask so position t cannot attend to >t\n",
    "        # PyTorch expects (T, T) mask for encoder with batch_first=True\n",
    "        mask = torch.triu(torch.ones(T, T, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        x = self.transformer(x, mask)                # [B, T, C]\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)                     # [B, T, V]\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens: int, temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        Autoregressive sampling.\n",
    "        idx: [B, T] initial context\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop if longer than max_seq_len\n",
    "            idx_cond = idx[:, -self.cfg.max_seq_len:]\n",
    "            logits = self(idx_cond)                  # [B, T, V]\n",
    "            logits = logits[:, -1, :]                # last step\n",
    "            logits = logits / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # [B, 1]\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Tiny char-level dataset utilities\n",
    "# ============================================================\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.itos[int(i)] for i in ids)\n",
    "\n",
    "\n",
    "def make_batches(data_ids, block_size, batch_size, device):\n",
    "    \"\"\"\n",
    "    Yield (x, y) batches where y is next-token target.\n",
    "    \"\"\"\n",
    "    n = len(data_ids) - block_size\n",
    "    while True:\n",
    "        ix = torch.randint(0, n, (batch_size,))\n",
    "        x = torch.stack(\n",
    "            [torch.tensor(data_ids[i:i + block_size]) for i in ix]\n",
    "        )\n",
    "        y = torch.stack(\n",
    "            [torch.tensor(data_ids[i + 1:i + block_size + 1]) for i in ix]\n",
    "        )\n",
    "        yield x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Demo: train + inference benchmark\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # Toy training text (replace with any corpus you like)\n",
    "    text = (\n",
    "        \"To be, or not to be, that is the question:\\n\"\n",
    "        \"Whether 'tis nobler in the mind to suffer\\n\"\n",
    "        \"The slings and arrows of outrageous fortune,\\n\"\n",
    "        \"Or to take arms against a sea of troubles\\n\"\n",
    "        \"And by opposing end them.\\n\"\n",
    "    )\n",
    "    tokenizer = CharTokenizer(text)\n",
    "    data_ids = tokenizer.encode(text)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    block_size = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    cfg = ModelConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=128,\n",
    "        n_heads=4,\n",
    "        n_layers=2,\n",
    "        d_ff=256,\n",
    "        max_seq_len=block_size,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "    model = TinyTransformerLM(cfg).to(device)\n",
    "\n",
    "    print(\"Model parameters:\",\n",
    "          sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    train_iter = make_batches(data_ids, block_size, batch_size, device)\n",
    "\n",
    "    # --------- Training loop (very small, just to show it learns) ----------\n",
    "    model.train()\n",
    "    n_steps = 300\n",
    "    for step in range(1, n_steps + 1):\n",
    "        x, y = next(train_iter)\n",
    "        logits = model(x)                            # [B, T, V]\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, cfg.vocab_size),\n",
    "            y.view(-1),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"step {step}/{n_steps}, loss={loss.item():.4f}\")\n",
    "\n",
    "    # ========================================================\n",
    "    # 4. Qualitative sample\n",
    "    # ========================================================\n",
    "    model.eval()\n",
    "    ctx = \"To be\"\n",
    "    ctx_ids = torch.tensor([tokenizer.encode(ctx)], device=device)\n",
    "    out_ids = model.generate(ctx_ids, max_new_tokens=80, temperature=0.8)[0]\n",
    "    print(\"\\n--- Sample generation ---\")\n",
    "    print(tokenizer.decode(out_ids.tolist()))\n",
    "\n",
    "    # ========================================================\n",
    "    # 5. Inference performance benchmark\n",
    "    # ========================================================\n",
    "    print(\"\\n--- Inference benchmark ---\")\n",
    "    B = 8\n",
    "    T = block_size\n",
    "    dummy_input = torch.randint(\n",
    "        0, cfg.vocab_size, (B, T), device=device\n",
    "    )\n",
    "\n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "\n",
    "    # Measure forward pass throughput\n",
    "    n_iters = 100\n",
    "    torch.cuda.synchronize(device) if device == \"cuda\" else None\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_iters):\n",
    "            _ = model(dummy_input)\n",
    "    torch.cuda.synchronize(device) if device == \"cuda\" else None\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    total_time = t1 - t0\n",
    "    avg_time = total_time / n_iters\n",
    "    tokens_per_batch = B * T\n",
    "    toks_per_sec = tokens_per_batch / avg_time\n",
    "\n",
    "    print(f\"Forward pass: B={B}, T={T}\")\n",
    "    print(f\"Avg latency: {avg_time*1e3:.3f} ms\")\n",
    "    print(f\"Throughput:  {toks_per_sec:.0f} tokens/sec\")\n",
    "\n",
    "    # Autoregressive generation timing\n",
    "    gen_len = 64\n",
    "    B_gen = 1\n",
    "    start_ids = torch.randint(\n",
    "        0, cfg.vocab_size, (B_gen, 1), device=device\n",
    "    )\n",
    "\n",
    "    torch.cuda.synchronize(device) if device == \"cuda\" else None\n",
    "    t0 = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(start_ids, max_new_tokens=gen_len, temperature=1.0)\n",
    "    torch.cuda.synchronize(device) if device == \"cuda\" else None\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    gen_time = t1 - t0\n",
    "    print(\n",
    "        f\"Generate {gen_len} tokens (B={B_gen}): \"\n",
    "        f\"{gen_time*1e3:.2f} ms \"\n",
    "        f\"({gen_len/gen_time:.0f} tok/sec)\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766173061083,
     "user_tz": 480,
     "elapsed": 20159,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "61d5070d-84a2-4e8f-ce84-0c7f58f8eea8",
    "id": "gfTtBkXRHYov"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "Model parameters: 0.281344 M\n",
      "step 50/300, loss=2.1031\n",
      "step 100/300, loss=1.5847\n",
      "step 150/300, loss=0.9563\n",
      "step 200/300, loss=0.4899\n",
      "step 250/300, loss=0.3331\n",
      "step 300/300, loss=0.2214\n",
      "\n",
      "--- Sample generation ---\n",
      "To be, or to to be, that is the question:\n",
      "Whether 'tis nobler in the mind to sufer\n",
      "Th\n",
      "\n",
      "--- Inference benchmark ---\n",
      "Forward pass: B=8, T=64\n",
      "Avg latency: 2.430 ms\n",
      "Throughput:  210730 tokens/sec\n",
      "Generate 64 tokens (B=1): 218.02 ms (294 tok/sec)\n"
     ]
    }
   ]
  }
 ]
}