{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrR7ZHfc9E0DGA2OMSQhx9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N239S-aQw5hI","executionInfo":{"status":"ok","timestamp":1753635951555,"user_tz":420,"elapsed":16545,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"762dc057-b24f-4126-fde2-346b26433cc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# Deepseek Stuff\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/makemore\n","# import os\n","# os.chdir('/content/drive/My Drive/makemore')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kIPT5P9NyOR8","executionInfo":{"status":"ok","timestamp":1753635952163,"user_tz":420,"elapsed":603,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"123b399a-f804-43db-f81b-d7e488bc9545"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/makemore\n"]}]},{"cell_type":"code","source":["!pip install deepspeed --upgrade --quiet\n","!pip install mpi4py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzeht93Z2JZR","executionInfo":{"status":"ok","timestamp":1753636770930,"user_tz":420,"elapsed":13580,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"7e996542-48b8-41d1-d980-cae6f4794417"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mpi4py\n","  Downloading mpi4py-4.1.0-cp311-cp311-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (16 kB)\n","Downloading mpi4py-4.1.0-cp311-cp311-manylinux1_x86_64.manylinux_2_5_x86_64.whl (1.4 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.1/1.4 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: mpi4py\n","Successfully installed mpi4py-4.1.0\n"]}]},{"cell_type":"code","source":["import torch\n","import deepspeed\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"CVWuvf_SyOUQ","executionInfo":{"status":"ok","timestamp":1753636772817,"user_tz":420,"elapsed":12,"user":{"displayName":"doug chang","userId":"06495228775351504429"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# train_mlp_ds.py\n","import torch\n","import deepspeed\n","#from mlp_model import MLP\n","\n","\n","# mlp_model.py\n","import torch.nn as nn\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_dim=100, hidden_dim=256, output_dim=10):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, output_dim)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# Dummy dataset\n","def get_data(batch_size=32, input_dim=100, num_samples=1000):\n","    X = torch.randn(num_samples, input_dim)\n","    y = torch.randint(0, 10, (num_samples,))\n","    return torch.utils.data.DataLoader(\n","        torch.utils.data.TensorDataset(X, y), batch_size=batch_size\n","    )\n","\n","# Loss\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","ds_config = {\n","    \"train_batch_size\": 32,\n","    \"gradient_accumulation_steps\": 1,\n","    \"zero_optimization\": {\n","        \"stage\": 2\n","    },\n","    \"fp16\": {\n","        \"enabled\": False\n","    }\n","}\n","\n","# Load model\n","model = MLP()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n","train_loader = get_data()\n","\n","# DeepSpeed config\n","ds_config = \"ds_config.json\"\n","model_engine, optimizer, _, _ = deepspeed.initialize(\n","    model=model,\n","    model_parameters=model.parameters(),\n","    optimizer=optimizer,\n","    config=ds_config\n",")\n","\n","# Training loop\n","for epoch in range(5):\n","    for batch in train_loader:\n","        x, y = batch\n","        x, y = x.to(model_engine.device), y.to(model_engine.device)\n","\n","        outputs = model_engine(x)\n","        loss = criterion(outputs, y)\n","\n","        model_engine.backward(loss)\n","        model_engine.step()\n","\n","    print(f\"Epoch {epoch} done\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZzqCGE-2GO5","executionInfo":{"status":"ok","timestamp":1753637260857,"user_tz":420,"elapsed":3138,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"cc01456d-e838-4bda-d5ca-db109800e849"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[2025-07-27 17:27:37,704] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.2, git-hash=unknown, git-branch=unknown\n","[2025-07-27 17:27:37,714] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 1\n","[2025-07-27 17:27:37,721] [INFO] [engine.py:1339:_configure_distributed_model] ********** distributed groups summary **********\n","\t self.dp_world_size=1\n","\t self.mp_world_size=1\n","\t self.seq_dp_world_size=1\n","\t self.sequence_parallel_size=1\n","***********************************************\n","[2025-07-27 17:27:37,724] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n","[2025-07-27 17:27:37,726] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n","[2025-07-27 17:27:37,727] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n","[2025-07-27 17:27:37,728] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n","[2025-07-27 17:27:37,729] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n","[2025-07-27 17:27:37,730] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n","[2025-07-27 17:27:37,734] [INFO] [stage_1_and_2.py:172:__init__] Reduce bucket size 500000000\n","[2025-07-27 17:27:37,734] [INFO] [stage_1_and_2.py:173:__init__] Allgather bucket size 500000000\n","[2025-07-27 17:27:37,735] [INFO] [stage_1_and_2.py:174:__init__] CPU Offload: False\n","[2025-07-27 17:27:37,736] [INFO] [stage_1_and_2.py:175:__init__] Round robin gradient partitioning: False\n","[2025-07-27 17:27:38,689] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n","[2025-07-27 17:27:38,692] [INFO] [utils.py:782:see_memory_usage] MA 1.33 GB         Max_MA 1.33 GB         CA 1.33 GB         Max_CA 1 GB \n","[2025-07-27 17:27:38,693] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 2.62 GB, percent = 20.7%\n","[2025-07-27 17:27:39,396] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n","[2025-07-27 17:27:39,398] [INFO] [utils.py:782:see_memory_usage] MA 1.33 GB         Max_MA 1.33 GB         CA 1.33 GB         Max_CA 1 GB \n","[2025-07-27 17:27:39,400] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 2.63 GB, percent = 20.7%\n","[2025-07-27 17:27:39,401] [INFO] [stage_1_and_2.py:599:__init__] optimizer state initialized\n","[2025-07-27 17:27:39,991] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n","[2025-07-27 17:27:39,996] [INFO] [utils.py:782:see_memory_usage] MA 1.33 GB         Max_MA 1.33 GB         CA 1.33 GB         Max_CA 1 GB \n","[2025-07-27 17:27:39,998] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 2.63 GB, percent = 20.7%\n","[2025-07-27 17:27:40,001] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n","[2025-07-27 17:27:40,005] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n","[2025-07-27 17:27:40,006] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n","[2025-07-27 17:27:40,008] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[(0.9, 0.999)]\n","[2025-07-27 17:27:40,010] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True\n","[2025-07-27 17:27:40,011] [INFO] [config.py:954:print] DeepSpeedEngine configuration:\n","[2025-07-27 17:27:40,012] [INFO] [config.py:958:print]   activation_checkpointing_config  {\n","    \"partition_activations\": false, \n","    \"contiguous_memory_optimization\": false, \n","    \"cpu_checkpointing\": false, \n","    \"number_checkpoints\": null, \n","    \"synchronize_checkpoint_boundary\": false, \n","    \"profile\": false\n","}\n","[2025-07-27 17:27:40,013] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n","[2025-07-27 17:27:40,014] [INFO] [config.py:958:print]   amp_enabled .................. False\n","[2025-07-27 17:27:40,015] [INFO] [config.py:958:print]   amp_params ................... False\n","[2025-07-27 17:27:40,018] [INFO] [config.py:958:print]   autotuning_config ............ {\n","    \"enabled\": false, \n","    \"start_step\": null, \n","    \"end_step\": null, \n","    \"metric_path\": null, \n","    \"arg_mappings\": null, \n","    \"metric\": \"throughput\", \n","    \"model_info\": null, \n","    \"results_dir\": \"autotuning_results\", \n","    \"exps_dir\": \"autotuning_exps\", \n","    \"overwrite\": true, \n","    \"fast\": true, \n","    \"start_profile_step\": 3, \n","    \"end_profile_step\": 5, \n","    \"tuner_type\": \"gridsearch\", \n","    \"tuner_early_stopping\": 5, \n","    \"tuner_num_trials\": 50, \n","    \"model_info_path\": null, \n","    \"mp_size\": 1, \n","    \"max_train_batch_size\": null, \n","    \"min_train_batch_size\": 1, \n","    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n","    \"min_train_micro_batch_size_per_gpu\": 1, \n","    \"num_tuning_micro_batch_sizes\": 3\n","}\n","[2025-07-27 17:27:40,019] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False\n","[2025-07-27 17:27:40,020] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}\n","[2025-07-27 17:27:40,022] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False\n","[2025-07-27 17:27:40,023] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True\n","[2025-07-27 17:27:40,024] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False\n","[2025-07-27 17:27:40,025] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7d3b31062310>\n","[2025-07-27 17:27:40,026] [INFO] [config.py:958:print]   communication_data_type ...... None\n","[2025-07-27 17:27:40,027] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False\n","[2025-07-27 17:27:40,028] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n","[2025-07-27 17:27:40,029] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False\n","[2025-07-27 17:27:40,030] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False\n","[2025-07-27 17:27:40,031] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n","[2025-07-27 17:27:40,032] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False\n","[2025-07-27 17:27:40,033] [INFO] [config.py:958:print]   dataloader_drop_last ......... False\n","[2025-07-27 17:27:40,033] [INFO] [config.py:958:print]   disable_allgather ............ False\n","[2025-07-27 17:27:40,034] [INFO] [config.py:958:print]   dump_state ................... False\n","[2025-07-27 17:27:40,035] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False\n","[2025-07-27 17:27:40,035] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1\n","[2025-07-27 17:27:40,036] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer\n","[2025-07-27 17:27:40,037] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0\n","[2025-07-27 17:27:40,038] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100\n","[2025-07-27 17:27:40,039] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06\n","[2025-07-27 17:27:40,039] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01\n","[2025-07-27 17:27:40,040] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False\n","[2025-07-27 17:27:40,041] [INFO] [config.py:958:print]   elasticity_enabled ........... False\n","[2025-07-27 17:27:40,042] [INFO] [config.py:958:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False\n","[2025-07-27 17:27:40,043] [INFO] [config.py:958:print]   flops_profiler_config ........ {\n","    \"enabled\": false, \n","    \"recompute_fwd_factor\": 0.0, \n","    \"profile_step\": 1, \n","    \"module_depth\": -1, \n","    \"top_modules\": 1, \n","    \"detailed\": true, \n","    \"output_file\": null\n","}\n","[2025-07-27 17:27:40,044] [INFO] [config.py:958:print]   global_rank .................. 0\n","[2025-07-27 17:27:40,045] [INFO] [config.py:958:print]   grad_accum_dtype ............. None\n","[2025-07-27 17:27:40,045] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 1\n","[2025-07-27 17:27:40,046] [INFO] [config.py:958:print]   gradient_clipping ............ 0.0\n","[2025-07-27 17:27:40,047] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0\n","[2025-07-27 17:27:40,048] [INFO] [config.py:958:print]   graph_harvesting ............. False\n","[2025-07-27 17:27:40,048] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n","[2025-07-27 17:27:40,049] [INFO] [config.py:958:print]   load_universal_checkpoint .... False\n","[2025-07-27 17:27:40,050] [INFO] [config.py:958:print]   memory_breakdown ............. False\n","[2025-07-27 17:27:40,050] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False\n","[2025-07-27 17:27:40,053] [INFO] [config.py:958:print]   mics_shard_size .............. -1\n","[2025-07-27 17:27:40,055] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n","[2025-07-27 17:27:40,055] [INFO] [config.py:958:print]   nebula_config ................ {\n","    \"enabled\": false, \n","    \"persistent_storage_path\": null, \n","    \"persistent_time_interval\": 100, \n","    \"num_of_version_in_retention\": 2, \n","    \"enable_nebula_load\": true, \n","    \"load_path\": null\n","}\n","[2025-07-27 17:27:40,056] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False\n","[2025-07-27 17:27:40,058] [INFO] [config.py:958:print]   optimizer_name ............... None\n","[2025-07-27 17:27:40,058] [INFO] [config.py:958:print]   optimizer_params ............. None\n","[2025-07-27 17:27:40,060] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n","[2025-07-27 17:27:40,060] [INFO] [config.py:958:print]   pld_enabled .................. False\n","[2025-07-27 17:27:40,062] [INFO] [config.py:958:print]   pld_params ................... False\n","[2025-07-27 17:27:40,063] [INFO] [config.py:958:print]   prescale_gradients ........... False\n","[2025-07-27 17:27:40,063] [INFO] [config.py:958:print]   scheduler_name ............... None\n","[2025-07-27 17:27:40,065] [INFO] [config.py:958:print]   scheduler_params ............. None\n","[2025-07-27 17:27:40,066] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32\n","[2025-07-27 17:27:40,067] [INFO] [config.py:958:print]   sparse_attention ............. None\n","[2025-07-27 17:27:40,067] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False\n","[2025-07-27 17:27:40,068] [INFO] [config.py:958:print]   steps_per_print .............. None\n","[2025-07-27 17:27:40,068] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n","[2025-07-27 17:27:40,070] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True\n","[2025-07-27 17:27:40,071] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None\n","[2025-07-27 17:27:40,071] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False\n","[2025-07-27 17:27:40,072] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None\n","[2025-07-27 17:27:40,072] [INFO] [config.py:958:print]   train_batch_size ............. 32\n","[2025-07-27 17:27:40,073] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  32\n","[2025-07-27 17:27:40,074] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False\n","[2025-07-27 17:27:40,074] [INFO] [config.py:958:print]   use_node_local_storage ....... False\n","[2025-07-27 17:27:40,075] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False\n","[2025-07-27 17:27:40,076] [INFO] [config.py:958:print]   weight_quantization_config ... None\n","[2025-07-27 17:27:40,076] [INFO] [config.py:958:print]   world_size ................... 1\n","[2025-07-27 17:27:40,077] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  False\n","[2025-07-27 17:27:40,077] [INFO] [config.py:958:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n","[2025-07-27 17:27:40,078] [INFO] [config.py:958:print]   zero_enabled ................. True\n","[2025-07-27 17:27:40,079] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True\n","[2025-07-27 17:27:40,079] [INFO] [config.py:958:print]   zero_optimization_stage ...... 2\n","[2025-07-27 17:27:40,080] [INFO] [config.py:944:print_user_config]   json = {\n","    \"train_batch_size\": 32, \n","    \"gradient_accumulation_steps\": 1, \n","    \"fp16\": {\n","        \"enabled\": false\n","    }, \n","    \"zero_optimization\": {\n","        \"stage\": 2\n","    }\n","}\n","Epoch 0 done\n","Epoch 1 done\n","Epoch 2 done\n","Epoch 3 done\n","Epoch 4 done\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JoDus9axyOWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OtU5aaqnyOY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6udPYf6HyObU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VrOTSf1AyOd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YufhQ-KlyOgP"},"execution_count":null,"outputs":[]}]}