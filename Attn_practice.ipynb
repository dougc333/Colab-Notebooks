{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMs0VIAbc7pKKbeyy392+dG"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dougc333/Colab-Notebooks/blob/main/Attn_practice.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Attention impl**"
   ],
   "metadata": {
    "id": "7JU__9MhqWbo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3DzizSPE1ux",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766949165531,
     "user_tz": 480,
     "elapsed": 19229,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "f2be3c63-ed2c-43de-e663-6791745ff5ed"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd '/content/drive/MyDrive'"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "we3H2bmsE2oI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766949194172,
     "user_tz": 480,
     "elapsed": 60,
     "user": {
      "displayName": "doug chang",
      "userId": "06495228775351504429"
     }
    },
    "outputId": "b6c36b26-0c6b-4042-e5d1-aeb086626b8b"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHKuZiJXqSkx"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def dense_attention(q, k, v, valid_lens):\n",
    "    \"\"\"\n",
    "    Reference implementation: standard attention assuming\n",
    "    each sequence has a contiguous [0..L-1] KV buffer.\n",
    "\n",
    "    q: [B, H, Dh]                (last token queries)\n",
    "    k, v: [B, Lmax, H, Dh]\n",
    "    valid_lens: [B] integers, sequence lengths\n",
    "    \"\"\"\n",
    "    B, H, Dh = q.shape\n",
    "    _, Lmax, _, _ = k.shape\n",
    "    out = torch.zeros(B, H, Dh, dtype=q.dtype)\n",
    "\n",
    "    for b in range(B):\n",
    "        L = int(valid_lens[b])\n",
    "        kb = k[b, :L]          # [L, H, Dh]\n",
    "        vb = v[b, :L]\n",
    "\n",
    "        scale = 1.0 / math.sqrt(Dh)\n",
    "        # scores[h, L]\n",
    "        scores = torch.einsum(\"hd,lhd->hl\", q[b] * scale, kb)\n",
    "        attn = torch.softmax(scores, dim=-1)  # [H, L]\n",
    "        # output[h, Dh]\n",
    "        o = torch.einsum(\"hl,lhd->hd\", attn, vb)\n",
    "        out[b] = o\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def paged_attention(q, key_cache, value_cache, block_table, seq_lens, block_size):\n",
    "    \"\"\"\n",
    "    Simple 'paged' attention:\n",
    "\n",
    "    q: [B, H, Dh]                         (last token queries)\n",
    "    key_cache:   [N_blocks, block_size, H, Dh]\n",
    "    value_cache: [N_blocks, block_size, H, Dh]\n",
    "    block_table: [B, max_blocks]\n",
    "        - block_table[b, i] = physical block index for\n",
    "          logical block i of sequence b\n",
    "    seq_lens: [B] sequence lengths (in tokens)\n",
    "    block_size: int, tokens per block\n",
    "\n",
    "    Returns:\n",
    "        out: [B, H, Dh]\n",
    "    \"\"\"\n",
    "    B, H, Dh = q.shape\n",
    "    N_blocks, Bsz_block, Hc, Dhc = key_cache.shape\n",
    "\n",
    "    assert Bsz_block == block_size\n",
    "    assert Hc == H and Dhc == Dh\n",
    "    assert block_table.shape[0] == B\n",
    "\n",
    "    out = torch.zeros(B, H, Dh, dtype=q.dtype)\n",
    "\n",
    "    for b in range(B):\n",
    "        L = int(seq_lens[b])\n",
    "        if L == 0:\n",
    "            continue\n",
    "\n",
    "        # How many logical blocks are needed for this sequence?\n",
    "        num_logical_blocks = (L + block_size - 1) // block_size\n",
    "\n",
    "        # Step 1: follow the block table to find the physical blocks\n",
    "        blocks_idx = block_table[b, :num_logical_blocks]   # [num_logical_blocks]\n",
    "\n",
    "        # Step 2: gather those blocks from the global KV cache\n",
    "        # k_blocks, v_blocks: [num_logical_blocks, block_size, H, Dh]\n",
    "        k_blocks = key_cache[blocks_idx]\n",
    "        v_blocks = value_cache[blocks_idx]\n",
    "\n",
    "        # Step 3: flatten blocks into a single [L_eff, H, Dh] sequence\n",
    "        # (we may have padded tokens beyond L in the last block; trim them)\n",
    "        k_flat = k_blocks.reshape(num_logical_blocks * block_size, H, Dh)[:L]\n",
    "        v_flat = v_blocks.reshape(num_logical_blocks * block_size, H, Dh)[:L]\n",
    "\n",
    "        # Step 4: standard scaled dot-product attention over this (logical) sequence\n",
    "        scale = 1.0 / math.sqrt(Dh)\n",
    "        scores = torch.einsum(\"hd,lhd->hl\", q[b] * scale, k_flat)  # [H, L]\n",
    "        attn = torch.softmax(scores, dim=-1)                       # [H, L]\n",
    "        o = torch.einsum(\"hl,lhd->hd\", attn, v_flat)               # [H, Dh]\n",
    "\n",
    "        out[b] = o\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# show dense attn == paged attn\n",
    "def demo():\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    B, Lmax, H, Dh = 2, 8, 4, 8\n",
    "    block_size = 4\n",
    "\n",
    "    # Fake data\n",
    "    q = torch.randn(B, H, Dh)\n",
    "    k_dense = torch.randn(B, Lmax, H, Dh)\n",
    "    v_dense = torch.randn(B, Lmax, H, Dh)\n",
    "    seq_lens = torch.tensor([8, 8])  # all full-length in this demo\n",
    "\n",
    "    # Dense baseline\n",
    "    dense_out = dense_attention(q, k_dense, v_dense, seq_lens)\n",
    "\n",
    "    # Build a paged KV cache\n",
    "    # Each sequence will get Lmax/block_size blocks, stored contiguously\n",
    "    blocks_per_seq = Lmax // block_size\n",
    "    num_blocks_total = B * blocks_per_seq\n",
    "\n",
    "    key_cache = torch.zeros(num_blocks_total, block_size, H, Dh)\n",
    "    value_cache = torch.zeros_like(key_cache)\n",
    "    block_table = torch.full((B, blocks_per_seq), -1, dtype=torch.long)\n",
    "\n",
    "    next_block = 0\n",
    "    for b in range(B):\n",
    "        L = int(seq_lens[b])\n",
    "        num_blocks = (L + block_size - 1) // block_size\n",
    "\n",
    "        # logical blocks 0..num_blocks-1 map to physical blocks [next_block ..)\n",
    "        blocks_idx = list(range(next_block, next_block + num_blocks))\n",
    "        block_table[b, :num_blocks] = torch.tensor(blocks_idx)\n",
    "\n",
    "        # reshape dense K/V into blocks and write into cache\n",
    "        k_b = k_dense[b, :L].reshape(num_blocks, block_size, H, Dh)\n",
    "        v_b = v_dense[b, :L].reshape(num_blocks, block_size, H, Dh)\n",
    "\n",
    "        key_cache[next_block : next_block + num_blocks] = k_b\n",
    "        value_cache[next_block : next_block + num_blocks] = v_b\n",
    "\n",
    "        next_block += num_blocks\n",
    "\n",
    "    # Paged attention output\n",
    "    paged_out = paged_attention(\n",
    "        q,\n",
    "        key_cache,\n",
    "        value_cache,\n",
    "        block_table,\n",
    "        seq_lens,\n",
    "        block_size,\n",
    "    )\n",
    "\n",
    "    print(\"Max |dense - paged|:\", (dense_out - paged_out).abs().max().item())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()"
   ],
   "metadata": {
    "id": "PNCah_7wCOAV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TWlta_DuCOCn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "a6BrdlngCOE9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VLLM **Impl**"
   ],
   "metadata": {
    "id": "nAqZ98nkC3Gn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install \"vllm>=0.5.0\" \"transformers>=4.45.0\" torch"
   ],
   "metadata": {
    "id": "Xyg0zCNNC5f5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "# In recent HF, ALL_ATTENTION_FUNCTIONS is shared between backends.\n",
    "# If this import fails, search in your transformers installation for ALL_ATTENTION_FUNCTIONS\n",
    "from transformers.models.llama.modeling_llama import ALL_ATTENTION_FUNCTIONS\n",
    "\n",
    "from vllm import LLM\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Config\n",
    "# -----------------------------\n",
    "class ToyConfig(PretrainedConfig):\n",
    "    model_type = \"toy-transformer\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 50257,   # match GPT-2 tokenizer\n",
    "        hidden_size: int = 128,\n",
    "        num_hidden_layers: int = 2,\n",
    "        num_attention_heads: int = 4,\n",
    "        intermediate_size: int = 256,\n",
    "        max_position_embeddings: int = 512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "\n",
    "        # vLLM/Transformers backend stuff\n",
    "        # This tells vLLM that the base model supports its attention backend.\n",
    "        self._attn_implementation = \"vllm\"\n",
    "        self.is_decoder = True\n",
    "        self.is_encoder_decoder = False\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Attention block compatible with vLLM backend\n",
    "# -----------------------------\n",
    "class ToyAttention(nn.Module):\n",
    "    # decoder-only causal LM, so attention is causal\n",
    "    is_causal = True\n",
    "\n",
    "    def __init__(self, config: ToyConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        bsz, seqlen, h = hidden_states.shape\n",
    "        head_dim = self.head_dim\n",
    "        num_heads = self.num_heads\n",
    "\n",
    "        # project to Q, K, V\n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "\n",
    "        # (bsz, seqlen, num_heads, head_dim) -> (bsz, num_heads, seqlen, head_dim)\n",
    "        def shape(x):\n",
    "            return (\n",
    "                x.view(bsz, seqlen, num_heads, head_dim)\n",
    "                .transpose(1, 2)\n",
    "                .contiguous()\n",
    "            )\n",
    "\n",
    "        q = shape(q)\n",
    "        k = shape(k)\n",
    "        v = shape(v)\n",
    "\n",
    "        # This is the crucial hook: use vLLM attention backend\n",
    "        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states=q,\n",
    "            key_states=k,\n",
    "            value_states=v,\n",
    "            attention_mask=attention_mask,\n",
    "            is_causal=self.is_causal,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # (bsz, num_heads, seqlen, head_dim) -> (bsz, seqlen, h)\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2)\n",
    "            .reshape(bsz, seqlen, h)\n",
    "            .contiguous()\n",
    "        )\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "class ToyMLP(nn.Module):\n",
    "    def __init__(self, config: ToyConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class ToyBlock(nn.Module):\n",
    "    def __init__(self, config: ToyConfig):\n",
    "        super().__init__()\n",
    "        self.self_attn = ToyAttention(config)\n",
    "        self.mlp = ToyMLP(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        # Self-attention\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln1(hidden_states)\n",
    "        attn_output, _ = self.self_attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = residual + attn_output\n",
    "\n",
    "        # MLP\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln2(hidden_states)\n",
    "        hidden_states = residual + self.mlp(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Base model (decoder LM) + LM head\n",
    "# -----------------------------\n",
    "class ToyModel(PreTrainedModel):\n",
    "    config_class = ToyConfig\n",
    "    _supports_attention_backend = True  # required for vLLM backend\n",
    "\n",
    "    def __init__(self, config: ToyConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.embed_positions = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [ToyBlock(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "        # Important for generation: LM head\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.embed_tokens = new_embeddings\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        bsz, seqlen = input_ids.shape\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(\n",
    "                seqlen, dtype=torch.long, device=input_ids.device\n",
    "            )\n",
    "            position_ids = position_ids.unsqueeze(0).expand(bsz, seqlen)\n",
    "\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        pos_embeds = self.embed_positions(position_ids)\n",
    "        hidden_states = inputs_embeds + pos_embeds\n",
    "\n",
    "        # (very simple): just apply full attention over all tokens\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # standard LM shift\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, self.config.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "            )\n",
    "\n",
    "        if not self.config.use_return_dict:\n",
    "            output = (logits,)\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=None,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Save as a HF-style model directory\n",
    "# -----------------------------\n",
    "def save_toy_model(model_dir: str = \"toy_vllm_model\"):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # use GPT-2 tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "    config = ToyConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=2,\n",
    "        num_attention_heads=4,\n",
    "        intermediate_size=256,\n",
    "        max_position_embeddings=512,\n",
    "    )\n",
    "\n",
    "    # Add HF-style auto_map so Transformers/vLLM know how to import this class\n",
    "    auto_map = {\n",
    "        \"AutoConfig\": \"__main__.ToyConfig\",\n",
    "        # For vLLM + Transformers backend, the base model should be the one in auto_map.\n",
    "        # We treat ToyModel as the causal LM base.\n",
    "        \"AutoModelForCausalLM\": \"__main__.ToyModel\",\n",
    "    }\n",
    "\n",
    "    cfg_dict = config.to_dict()\n",
    "    cfg_dict[\"auto_map\"] = auto_map\n",
    "    cfg_dict[\"architectures\"] = [\"ToyModel\"]\n",
    "\n",
    "    with open(os.path.join(model_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(cfg_dict, f, indent=2)\n",
    "\n",
    "    model = ToyModel(config)\n",
    "    model.save_pretrained(model_dir)\n",
    "\n",
    "    print(f\"Saved toy model + tokenizer to: {model_dir}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Load with vLLM and generate\n",
    "# -----------------------------\n",
    "def run_with_vllm(model_dir: str = \"toy_vllm_model\"):\n",
    "    # Force Transformers backend; we\u2019re using a Transformers-style custom model\n",
    "    llm = LLM(model=model_dir, model_impl=\"transformers\", trust_remote_code=True)\n",
    "\n",
    "    prompts = [\"Hello world\", \"Once upon a time\"]\n",
    "    outputs = llm.generate(prompts, max_tokens=20)\n",
    "\n",
    "    for i, out in enumerate(outputs):\n",
    "        print(f\"\\nPrompt: {prompts[i]!r}\")\n",
    "        print(\"Completion:\", out.outputs[0].text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_toy_model()\n",
    "    run_with_vllm()"
   ],
   "metadata": {
    "id": "AUEClxdsC5if"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# training toymodel\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from toy_vllm_model import ToyModel, ToyConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load config + model\n",
    "config = ToyConfig()\n",
    "model = ToyModel(config)\n",
    "\n",
    "# Load a text dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=128)\n",
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator for next-token prediction\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,        # causal LM objective (autoregressive)\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"toy_trained\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,          # you can enable fp16 if you want\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "id": "1yqI0TiHC5k1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# full example w training\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from toy_vllm_model import ToyModel, ToyConfig, save_toy_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: save base toy model\n",
    "save_toy_model(\"toy_vllm_model\")\n",
    "\n",
    "# Step 2: load base model for training\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "config = ToyConfig(vocab_size=tokenizer.vocab_size)\n",
    "model = ToyModel(config)\n",
    "\n",
    "# Step 3: load dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "tokenized = dataset.map(\n",
    "    lambda e: tokenizer(e[\"text\"], truncation=True, max_length=128),\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "# Step 4: collator\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Step 5: training args\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"toy_vllm_trained\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=25,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 6: save trained model in HF format\n",
    "trainer.save_model(\"toy_vllm_trained\")\n",
    "\n",
    "print(\"Training complete! Now you can serve it in vLLM:\")\n",
    "print(\"vllm serve toy_vllm_trained --model-impl transformers --trust-remote-code\")"
   ],
   "metadata": {
    "id": "U90qj6eTC5nc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vllm serve toy_vllm_trained --model-impl transformers --trust-remote-code"
   ],
   "metadata": {
    "id": "uLBynEHwC5py"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}