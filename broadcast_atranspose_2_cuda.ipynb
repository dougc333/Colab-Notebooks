{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO/Ry7sLbTzZTytFhrW3RmZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **a.T CUDA**\n","\n","Intro to CUDA programming wiht a.T\n"],"metadata":{"id":"A_XOWlW0SlxZ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oz8OAXqgtK9d","executionInfo":{"status":"ok","timestamp":1767204335382,"user_tz":480,"elapsed":23338,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"fbb8c360-7ae1-461e-beb7-3b1e2e73ea5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/'Colab Notebooks'\n","# this is saved in /content/drive/MyDrive/Colab Notebooks\n","# need to move it to Colab-Notebooks which is git checked in and make sure you\n","# are editing this copy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJBcQ8_6xjW3","executionInfo":{"status":"ok","timestamp":1767204336097,"user_tz":480,"elapsed":713,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"348a51c0-9484-4a73-96a1-7396c0b03d54"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"markdown","source":["\n","# **Update cuda drivers**\n","Match nvcc to nvidia-smi"],"metadata":{"id":"uYor7OcQ0Zvv"}},{"cell_type":"code","source":["%cd /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmUCsXF-401X","executionInfo":{"status":"ok","timestamp":1767142294869,"user_tz":480,"elapsed":8,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"c3a7053c-63ca-4959-9737-30ce93587c97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vG2L7IrT5E4M","executionInfo":{"status":"ok","timestamp":1767158520153,"user_tz":480,"elapsed":115,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"a6540b2e-2478-4c49-a8d7-a22ddf477c19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Dec 31 05:22:00 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!nvcc --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FuU2CPBa5Gm4","executionInfo":{"status":"ok","timestamp":1767158522503,"user_tz":480,"elapsed":108,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"c8cd7eb1-b754-41be-f26f-8fbcec1ec34f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Thu_Jun__6_02:18:23_PDT_2024\n","Cuda compilation tools, release 12.5, V12.5.82\n","Build cuda_12.5.r12.5/compiler.34385749_0\n"]}]},{"cell_type":"code","source":["\n","import os\n","print(\"colab_dc333.update_12_4 downgrades colab from cuda12.5 to cuda12.4 to make nvcc and nvidia-smi match\")\n","print(\"this takes 5minutes, please wait ...\")\n","os.environ['DEBIAN_FRONTEND'] = \"noninteractive\"\n","#os.system('apt-get update && apt-get install -y git')\n","#os.system('apt-get install -y emacs')\n","#os.system('apt-get install net-tools')\n","#os.system('apt-get install -y mlocate')\n","#os.system('apt-get install -y keyboard-configuration')\n","os.system('wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb')\n","os.system('dpkg -i cuda-keyring_1.1-1_all.deb')\n","os.system('apt-get update')\n","os.system('apt-get -y install cuda-toolkit-12-4')\n","os.system('rm /etc/alternatives/cuda')\n","os.system('ln -s  /usr/local/cuda-12.4 /etc/alternatives/cuda')\n","os.system('nvcc --version')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SW9TdpGb0ZHY","executionInfo":{"status":"ok","timestamp":1767159560581,"user_tz":480,"elapsed":7481,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"f2984ef6-9ac1-45e6-db56-872936a31559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["colab_dc333.update_12_4 downgrades colab from cuda12.5 to cuda12.4 to make nvcc and nvidia-smi match\n","this takes 5minutes, please wait ...\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# install nsys\n","\n","#os.system('apt update')\n","os.system('apt install -y --no-install-recommends gnupg')\n","os.system('echo \"deb http://developer.download.nvidia.com/devtools/repos/ubuntu$(source /etc/lsb-release; echo \"$DISTRIB_RELEASE\" | tr -d .)/$(dpkg --print-architecture) /\" | tee /etc/apt/sources.list.d/nvidia-devtools.list')\n","os.system('apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub')\n","os.system('apt update')\n","os.system('apt install nsight-systems-cli')\n","os.system('nsys status -e')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtKci03W0ZJv","executionInfo":{"status":"ok","timestamp":1767159571403,"user_tz":480,"elapsed":10818,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"45ca0619-3066-46f4-8874-d81b30c2e19d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# install ncu\n","!apt-get update\n","!apt-get install -y nvidia-nsight-compute\n","!ncu --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nav8q_dI0ZMW","executionInfo":{"status":"ok","timestamp":1767159576815,"user_tz":480,"elapsed":5413,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"086c610a-c250-4e50-ed79-a1f3c72fb3e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n","\r0% [Connecting to security.ubuntu.com (91.189.92.22)] [Connected to cloud.r-pro\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.22)] [Co\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.22)] [Co\r                                                                               \rHit:5 https://cli.github.com/packages stable InRelease\n","\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.22)] [Co\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:9 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Ign:1 https://developer.download.nvidia.com/devtools/repos/ubuntu/amd64  InRelease\n","Err:10 https://developer.download.nvidia.com/devtools/repos/ubuntu/amd64  Release\n","  404  Not Found [IP: 23.0.162.178 443]\n","Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Reading package lists... Done\n","E: The repository 'http://developer.download.nvidia.com/devtools/repos/ubuntu/amd64  Release' does not have a Release file.\n","N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n","N: See apt-secure(8) manpage for repository creation and user configuration details.\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","E: Unable to locate package nvidia-nsight-compute\n","NVIDIA (R) Nsight Compute Command Line Profiler\n","Copyright (c) 2018-2024 NVIDIA Corporation\n","Version 2024.2.1.0 (build 34372528) (public-release)\n"]}]},{"cell_type":"markdown","source":["Slow path transpose_gemm.cu vs fast path where transpose is implemented in stride and index changes\n","\n","nvcc -O3 -lineinfo transpose_gemm.cu -o transpose_gemm -lcublas"],"metadata":{"id":"yXHQyr910q8i"}},{"cell_type":"code","source":["// transpose_gemm.cu\n","#include <cstdio>\n","#include <cublas_v2.h>\n","#include <cuda_runtime.h>\n","\n","#define CHECK_CUDA(call) \\\n","  do { \\\n","    cudaError_t err = (call); \\\n","    if (err != cudaSuccess) { \\\n","      printf(\"CUDA error %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n","      exit(1); \\\n","    } \\\n","  } while (0)\n","\n","#define CHECK_CUBLAS(call) \\\n","  do { \\\n","    cublasStatus_t st = (call); \\\n","    if (st != CUBLAS_STATUS_SUCCESS) { \\\n","      printf(\"cuBLAS error %s:%d: %d\\n\", __FILE__, __LINE__, st); \\\n","      exit(1); \\\n","    } \\\n","  } while (0)\n","\n","__global__ void transpose_kernel(const float* __restrict__ A,\n","                                 float* __restrict__ AT,\n","                                 int M, int N) {\n","  // A: (M, N) row-major\n","  // AT: (N, M) row-major\n","  int row = blockIdx.y * blockDim.y + threadIdx.y;\n","  int col = blockIdx.x * blockDim.x + threadIdx.x;\n","  if (row < M && col < N) {\n","    AT[col * M + row] = A[row * N + col];\n","  }\n","}\n","\n","int main() {\n","  int M = 1024;\n","  int K = 1024;\n","  int N = 1024;\n","\n","  size_t bytesA = M * K * sizeof(float);\n","  size_t bytesB = K * N * sizeof(float);\n","  size_t bytesBT = N * K * sizeof(float);\n","  size_t bytesC = M * N * sizeof(float);\n","\n","  float *hA = (float*)malloc(bytesA);\n","  float *hB = (float*)malloc(bytesB);\n","\n","  for (int i = 0; i < M*K; ++i) hA[i] = 0.001f * i;\n","  for (int i = 0; i < K*N; ++i) hB[i] = 0.002f * i;\n","\n","  float *dA, *dB, *dBT, *dC;\n","  CHECK_CUDA(cudaMalloc(&dA, bytesA));\n","  CHECK_CUDA(cudaMalloc(&dB, bytesB));\n","  CHECK_CUDA(cudaMalloc(&dBT, bytesBT));\n","  CHECK_CUDA(cudaMalloc(&dC, bytesC));\n","\n","  CHECK_CUDA(cudaMemcpy(dA, hA, bytesA, cudaMemcpyHostToDevice));\n","  CHECK_CUDA(cudaMemcpy(dB, hB, bytesB, cudaMemcpyHostToDevice));\n","\n","  cublasHandle_t handle;\n","  CHECK_CUBLAS(cublasCreate(&handle));\n","\n","  dim3 block(32, 32);\n","  dim3 grid((K + block.x - 1) / block.x,\n","            (N + block.y - 1) / block.y);\n","\n","  // Time with CUDA events\n","  cudaEvent_t start, stop;\n","  CHECK_CUDA(cudaEventCreate(&start));\n","  CHECK_CUDA(cudaEventCreate(&stop));\n","\n","  CHECK_CUDA(cudaEventRecord(start));\n","\n","  // 1) Explicit transpose B -> BT\n","  transpose_kernel<<<grid, block>>>(dB, dBT, K, N);\n","  CHECK_CUDA(cudaGetLastError());\n","\n","  // 2) GEMM: C = A * BT\n","  float alpha = 1.0f, beta = 0.0f;\n","  // Row-major trick: treat as column-major by swapping M/N and trans flags\n","  //   A: (M, K)\n","  //   BT: (N, K) – but we want it interpreted as (K, N) column-major\n","  CHECK_CUBLAS(cublasSgemm(\n","      handle,\n","      CUBLAS_OP_N, CUBLAS_OP_T,  // B is effectively transposed\n","      N, M, K,\n","      &alpha,\n","      dBT, N,     // B^T\n","      dA, K,      // A\n","      &beta,\n","      dC, N));    // C\n","\n","  CHECK_CUDA(cudaEventRecord(stop));\n","  CHECK_CUDA(cudaEventSynchronize(stop));\n","  float ms = 0.0f;\n","  CHECK_CUDA(cudaEventElapsedTime(&ms, start, stop));\n","  printf(\"Naive transpose+gemm time: %.3f ms\\n\", ms);\n","\n","  CHECK_CUBLAS(cublasDestroy(handle));\n","  CHECK_CUDA(cudaFree(dA));\n","  CHECK_CUDA(cudaFree(dB));\n","  CHECK_CUDA(cudaFree(dBT));\n","  CHECK_CUDA(cudaFree(dC));\n","  free(hA);\n","  free(hB);\n","  return 0;\n","}"],"metadata":{"id":"MhDJnqXn0XQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["// strided_gemm.cu\n","#include <cstdio>\n","#include <cublas_v2.h>\n","#include <cuda_runtime.h>\n","\n","#define CHECK_CUDA(call)  /* same as above */\n","#define CHECK_CUBLAS(call) /* same as above */\n","\n","int main() {\n","  int M = 1024;\n","  int K = 1024;\n","  int N = 1024;\n","\n","  size_t bytesA = M * K * sizeof(float);\n","  size_t bytesB = K * N * sizeof(float);\n","  size_t bytesC = M * N * sizeof(float);\n","\n","  float *hA = (float*)malloc(bytesA);\n","  float *hB = (float*)malloc(bytesB);\n","\n","  for (int i = 0; i < M*K; ++i) hA[i] = 0.001f * i;\n","  for (int i = 0; i < K*N; ++i) hB[i] = 0.002f * i;\n","\n","  float *dA, *dB, *dC;\n","  CHECK_CUDA(cudaMalloc(&dA, bytesA));\n","  CHECK_CUDA(cudaMalloc(&dB, bytesB));\n","  CHECK_CUDA(cudaMalloc(&dC, bytesC));\n","\n","  CHECK_CUDA(cudaMemcpy(dA, hA, bytesA, cudaMemcpyHostToDevice));\n","  CHECK_CUDA(cudaMemcpy(dB, hB, bytesB, cudaMemcpyHostToDevice));\n","\n","  cublasHandle_t handle;\n","  CHECK_CUBLAS(cublasCreate(&handle));\n","\n","  cudaEvent_t start, stop;\n","  CHECK_CUDA(cudaEventCreate(&start));\n","  CHECK_CUDA(cudaEventCreate(&stop));\n","\n","  CHECK_CUDA(cudaEventRecord(start));\n","\n","  float alpha = 1.0f, beta = 0.0f;\n","\n","  // C = A * B^T, but we NEVER transpose B in memory.\n","  // We just tell cuBLAS: opB = CUBLAS_OP_T\n","  CHECK_CUBLAS(cublasSgemm(\n","      handle,\n","      CUBLAS_OP_N, CUBLAS_OP_T,  // A normal, B interpreted transposed\n","      N, M, K,\n","      &alpha,\n","      dB, N,     // B, but read as transposed\n","      dA, K,     // A\n","      &beta,\n","      dC, N));   // C\n","\n","  CHECK_CUDA(cudaEventRecord(stop));\n","  CHECK_CUDA(cudaEventSynchronize(stop));\n","  float ms = 0.0f;\n","  CHECK_CUDA(cudaEventElapsedTime(&ms, start, stop));\n","  printf(\"Direct cuBLAS (opB=T) time: %.3f ms\\n\", ms);\n","\n","  CHECK_CUBLAS(cublasDestroy(handle));\n","  CHECK_CUDA(cudaFree(dA));\n","  CHECK_CUDA(cudaFree(dB));\n","  CHECK_CUDA(cudaFree(dC));\n","  free(hA);\n","  free(hB);\n","  return 0;\n","}"],"metadata":{"id":"4CuOMuwx0XTo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["/content# ./strided_gemm\n","Direct cuBLAS (opB=T) time: 0.000 ms\n","/content# ./transpose_gemm\n","Naive transpose+gemm time: 6.761 ms\n"],"metadata":{"id":"AtQCgpM696Oj"}},{"cell_type":"code","source":["/#include <cstdio>\n","#include <cuda_runtime.h>\n","\n","#define TILE_DIM 32\n","#define BLOCK_ROWS 8\n","\n","// ====================== Naive kernel ======================\n","\n","__global__ void transpose_naive(const float* __restrict__ in,\n","                                float* __restrict__ out,\n","                                int H, int W) {\n","    int row = blockIdx.y * blockDim.y + threadIdx.y;\n","    int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (row < H && col < W) {\n","        out[col * H + row] = in[row * W + col];\n","    }\n","}\n","\n","// ====================== Tiled kernel ======================\n","\n","__global__ void transpose_tiled(const float* __restrict__ in,\n","                                float* __restrict__ out,\n","                                int H, int W) {\n","    __shared__ float tile[TILE_DIM][TILE_DIM + 1]; // +1 to avoid bank conflicts\n","\n","    int x = blockIdx.x * TILE_DIM + threadIdx.x;  // column index in input\n","    int y = blockIdx.y * TILE_DIM + threadIdx.y;  // row index in input\n","\n","    // Load tile from global memory into shared memory\n","    for (int i = 0; i < TILE_DIM; i += BLOCK_ROWS) {\n","        int yy = y + i;\n","        if (x < W && yy < H) {\n","            tile[threadIdx.y + i][threadIdx.x] = in[yy * W + x];\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    // Transpose tile and write back to global memory\n","    x = blockIdx.y * TILE_DIM + threadIdx.x;  // transposed col index\n","    y = blockIdx.x * TILE_DIM + threadIdx.y;  // transposed row index\n","\n","    for (int i = 0; i < TILE_DIM; i += BLOCK_ROWS) {\n","        int yy = y + i;\n","        if (x < H && yy < W) {\n","            out[yy * H + x] = tile[threadIdx.x][threadIdx.y + i];\n","        }\n","    }\n","}\n","\n","// ====================== Benchmark main ======================\n","\n","int main() {\n","    const int H = 4096;\n","    const int W = 4096;\n","    size_t bytes = H * W * sizeof(float);\n","\n","    float *d_in, *d_out;\n","    cudaMalloc(&d_in, bytes);\n","    cudaMalloc(&d_out, bytes);\n","\n","    cudaMemset(d_in, 1, bytes);\n","    cudaMemset(d_out, 0, bytes);\n","\n","    dim3 block1(16, 16);\n","    dim3 grid1((W + block1.x - 1) / block1.x,\n","               (H + block1.y - 1) / block1.y);\n","\n","    dim3 block2(TILE_DIM, BLOCK_ROWS);\n","    dim3 grid2((W + TILE_DIM - 1) / TILE_DIM,\n","               (H + TILE_DIM - 1) / TILE_DIM);\n","\n","    // Warmup both kernels\n","    for (int i = 0; i < 5; ++i) {\n","        transpose_naive<<<grid1, block1>>>(d_in, d_out, H, W);\n","        transpose_tiled<<<grid2, block2>>>(d_in, d_out, H, W);\n","    }\n","    cudaDeviceSynchronize();\n","\n","    // Time naive\n","    float ms_naive;\n","    {\n","        cudaEvent_t start, stop;\n","        cudaEventCreate(&start);\n","        cudaEventCreate(&stop);\n","\n","        cudaEventRecord(start);\n","        for (int i = 0; i < 50; ++i) {\n","            transpose_naive<<<grid1, block1>>>(d_in, d_out, H, W);\n","        }\n","        cudaEventRecord(stop);\n","        cudaEventSynchronize(stop);\n","        cudaEventElapsedTime(&ms_naive, start, stop);\n","\n","        cudaEventDestroy(start);\n","        cudaEventDestroy(stop);\n","    }\n","\n","    // Time tiled\n","    float ms_tiled;\n","    {\n","        cudaEvent_t start, stop;\n","        cudaEventCreate(&start);\n","        cudaEventCreate(&stop);\n","\n","        cudaEventRecord(start);\n","        for (int i = 0; i < 50; ++i) {\n","            transpose_tiled<<<grid2, block2>>>(d_in, d_out, H, W);\n","        }\n","        cudaEventRecord(stop);\n","        cudaEventSynchronize(stop);\n","        cudaEventElapsedTime(&ms_tiled, start, stop);\n","\n","        cudaEventDestroy(start);\n","        cudaEventDestroy(stop);\n","    }\n","\n","    printf(\"Naive transpose:  %f ms (avg over 50)\\n\", ms_naive / 50.0f);\n","    printf(\"Tiled transpose:  %f ms (avg over 50)\\n\", ms_tiled / 50.0f);\n","\n","    cudaFree(d_in);\n","    cudaFree(d_out);\n","    return 0;\n","}"],"metadata":{"id":"jcnGfFTj7F0D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["/content# ./transpose_bench\n","Naive transpose:  1.234682 ms (avg over 50)\n","Tiled transpose:  0.669043 ms (avg over 50)"],"metadata":{"id":"8PWtQYas71y0"}},{"cell_type":"code","source":["nsys profile \\\n","  -t cuda,osrt \\\n","  -o transpose_report \\\n","  ./transpose_bench"],"metadata":{"id":"6yjRihw47F2W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ncu --set full --page raw --csv ./transpose_bench > transpose_stall.csv"],"metadata":{"id":"Ek75lAgx7F43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","import csv\n","import json\n","import sys\n","\n","if len(sys.argv) != 3:\n","    print(\"usage: python csv_to_json.py input.csv output.json\")\n","    sys.exit(1)\n","\n","inp, out = sys.argv[1], sys.argv[2]\n","\n","metrics = {}\n","\n","with open(inp, newline=\"\") as f:\n","    reader = csv.reader(f)\n","\n","    name_idx = None\n","    value_idx = None\n","\n","    for row in reader:\n","        if not row:\n","            continue\n","\n","        # Detect header rows for metric tables\n","        # Typical Nsight Compute CSV metric header looks like:\n","        # ID, \"Metric Name\", \"Metric Description\", \"Metric Unit\", \"Metric Value\"\n","        if \"Metric Name\" in row and \"Metric Value\" in row:\n","            name_idx = row.index(\"Metric Name\")\n","            value_idx = row.index(\"Metric Value\")\n","            continue\n","\n","        # If we don't have indices yet, skip\n","        if name_idx is None or value_idx is None:\n","            continue\n","\n","        if len(row) <= max(name_idx, value_idx):\n","            continue\n","\n","        metric_name = row[name_idx].strip()\n","        metric_val_str = row[value_idx].strip()\n","\n","        try:\n","            val = float(metric_val_str)\n","        except ValueError:\n","            continue\n","\n","        metrics[metric_name] = val\n","\n","\n","def get(name: str, default: float = 0.0) -> float:\n","    return float(metrics.get(name, default))\n","\n","\n","# Build the JSON structure used by the React NsightProfileCharts component\n","data = {\n","    \"kernelName\": \"transpose_stall\",  # you can override this later if you like\n","    \"occupancy\": {\n","        \"theoretical\": get(\"sm__maximum_warps_active.avg.pct_of_peak_sustained_active\"),\n","        \"achieved\":    get(\"sm__warps_active.avg.pct_of_peak_sustained_active\"),\n","    },\n","    \"throughput\": {\n","        \"compute\": get(\"sm__throughput.avg.pct_of_peak_sustained_elapsed\"),\n","        \"memory\":  get(\"lts__throughput.avg.pct_of_peak_sustained_elapsed\"),\n","        \"dram\":    get(\"dram__throughput.avg.pct_of_peak_sustained_elapsed\"),\n","    },\n","    # Still leaving stall diagnostics as 0.0 for now – those live in the\n","    # \"Diagnostics\" / \"SpeedOfLight\" text sections, not simple numeric metrics.\n","    \"stalls\": {\n","        \"uncoalescedGlobalAccesses\": 0.0,\n","        \"l1StorePattern\": 0.0,\n","        \"longScoreboard\": 0.0,\n","    },\n","}\n","\n","with open(out, \"w\") as f:\n","    json.dump(data, f, indent=2)\n","\n","print(\"Wrote\", out)"],"metadata":{"id":"ynBNHakY7F7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"npyBM9dZ7F9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CYFCqjBH7F_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jJ0djR3j7GCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C4-mxDa07GEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_c = segment_mean(vals_flat, seg_ids, K=C)[c]"],"metadata":{"id":"7ka3IEAxPHHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# segment mean using scater\n","scatter_mean_1d(vals, seg_ids, K)  # using index_add_ (scatter)"],"metadata":{"id":"tm5md98I0XV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# ---------- Scatter / segment-mean primitives ----------\n","\n","def scatter_mean_1d(vals: torch.Tensor, seg_ids: torch.Tensor, K: int) -> torch.Tensor:\n","    \"\"\"\n","    vals:    (N,)  1D tensor of values\n","    seg_ids: (N,)  int64/int32 segment IDs in [0, K-1]\n","    K:       number of segments\n","\n","    Returns: (K,) mean per segment.\n","    \"\"\"\n","    sums = vals.new_zeros(K)\n","    counts = vals.new_zeros(K, dtype=torch.long)\n","\n","    # sums[k]   += vals[i] for all i where seg_ids[i] == k\n","    # counts[k] += 1       for all i where seg_ids[i] == k\n","    sums.index_add_(0, seg_ids, vals)\n","    counts.index_add_(0, seg_ids, torch.ones_like(seg_ids, dtype=torch.long))\n","\n","    return sums / counts.clamp_min(1).to(sums.dtype)\n","\n","\n","class ScatterBatchNorm1d(nn.Module):\n","    \"\"\"\n","    A BatchNorm1d analog that computes per-channel mean/var\n","    using generic scatter/segment-mean primitives.\n","\n","    Input: (N, C)  (we'll flatten (B, S, D) -> (B*S, D) before calling).\n","    \"\"\"\n","\n","    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True):\n","        super().__init__()\n","        self.num_features = num_features\n","        self.eps = eps\n","        self.affine = affine\n","\n","        if affine:\n","            self.weight = nn.Parameter(torch.ones(num_features))\n","            self.bias = nn.Parameter(torch.zeros(num_features))\n","        else:\n","            self.register_parameter(\"weight\", None)\n","            self.register_parameter(\"bias\", None)\n","\n","        # For benchmarking / training only, we skip running_mean/var\n","        # and track_running_stats logic to keep the code focused on the\n","        # scatter-based reduction.\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (N, C)\n","        \"\"\"\n","        N, C = x.shape\n","        assert C == self.num_features\n","\n","        # Flatten to scalars\n","        vals = x.reshape(-1)               # (N*C,)\n","        # Segment IDs: for each row n, features 0..C-1\n","        # seg_ids[k] = feature index for that scalar\n","        seg_ids = torch.arange(C, device=x.device).repeat(N)  # (N*C,)\n","        K = C\n","\n","        # Compute per-channel mean and second moment via scatter mean\n","        mean = scatter_mean_1d(vals, seg_ids, K)        # (C,)\n","        mean2 = scatter_mean_1d(vals * vals, seg_ids, K)  # (C,)\n","\n","        var = mean2 - mean * mean  # (C,)\n","\n","        mean = mean.view(1, C)\n","        var = var.view(1, C)\n","\n","        y = (x - mean) / torch.sqrt(var + self.eps)\n","\n","        if self.affine:\n","            y = y * self.weight.view(1, C) + self.bias.view(1, C)\n","\n","        return y\n","\n","\n","# ---------- Mini Transformer blocks ----------\n","\n","class MiniTransformerBlockBN(nn.Module):\n","    \"\"\"\n","    Mini Transformer block using standard nn.BatchNorm1d\n","    on the (B*S, D) flattened representation.\n","    \"\"\"\n","\n","    def __init__(self, d_model=64, nhead=4, dim_ff=128, dropout=0.1):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n","        self.bn1 = nn.BatchNorm1d(d_model, affine=True, track_running_stats=False)\n","        self.bn2 = nn.BatchNorm1d(d_model, affine=True, track_running_stats=False)\n","\n","        self.linear1 = nn.Linear(d_model, dim_ff)\n","        self.linear2 = nn.Linear(dim_ff, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (B, S, D)\n","        \"\"\"\n","        B, S, D = x.shape\n","\n","        # Self-attention\n","        attn_out, _ = self.attn(x, x, x, need_weights=False)\n","        x = x + attn_out\n","\n","        # BN over (B*S, D)\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.bn1(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        # FFN\n","        y = self.linear2(self.dropout(self.activation(self.linear1(x))))\n","        x = x + y\n","\n","        # BN again\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.bn2(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        return x\n","\n","\n","class MiniTransformerBlockScatterBN(nn.Module):\n","    \"\"\"\n","    Mini Transformer block using ScatterBatchNorm1d\n","    instead of nn.BatchNorm1d.\n","    \"\"\"\n","\n","    def __init__(self, d_model=64, nhead=4, dim_ff=128, dropout=0.1):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n","        self.sbn1 = ScatterBatchNorm1d(d_model, affine=True)\n","        self.sbn2 = ScatterBatchNorm1d(d_model, affine=True)\n","\n","        self.linear1 = nn.Linear(d_model, dim_ff)\n","        self.linear2 = nn.Linear(dim_ff, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (B, S, D)\n","        \"\"\"\n","        B, S, D = x.shape\n","\n","        # Self-attention\n","        attn_out, _ = self.attn(x, x, x, need_weights=False)\n","        x = x + attn_out\n","\n","        # Scatter-based BN over (B*S, D)\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.sbn1(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        # FFN\n","        y = self.linear2(self.dropout(self.activation(self.linear1(x))))\n","        x = x + y\n","\n","        # Scatter-based BN again\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.sbn2(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        return x\n","\n","\n","# ---------- Simple benchmark ----------\n","\n","def benchmark_block(\n","    block: nn.Module,\n","    name: str,\n","    device: str = \"cuda\",\n","    B: int = 32,\n","    S: int = 128,\n","    D: int = 64,\n","    iters: int = 50,\n","    warmup: int = 10,\n","):\n","    block = block.to(device)\n","    block.train()\n","    x = torch.randn(B, S, D, device=device)\n","    opt = torch.optim.Adam(block.parameters(), lr=1e-3)\n","\n","    # Warmup\n","    for _ in range(warmup):\n","        opt.zero_grad(set_to_none=True)\n","        y = block(x)\n","        loss = y.pow(2).mean()\n","        loss.backward()\n","        opt.step()\n","\n","    # Timing (forward + backward)\n","    if device.startswith(\"cuda\"):\n","        torch.cuda.synchronize()\n","        start = torch.cuda.Event(enable_timing=True)\n","        end = torch.cuda.Event(enable_timing=True)\n","\n","        start.record()\n","        for _ in range(iters):\n","            opt.zero_grad(set_to_none=True)\n","            y = block(x)\n","            loss = y.pow(2).mean()\n","            loss.backward()\n","            opt.step()\n","        end.record()\n","        torch.cuda.synchronize()\n","        elapsed_ms = start.elapsed_time(end) / iters\n","    else:\n","        t0 = time.time()\n","        for _ in range(iters):\n","            opt.zero_grad(set_to_none=True)\n","            y = block(x)\n","            loss = y.pow(2).mean()\n","            loss.backward()\n","            opt.step()\n","        t1 = time.time()\n","        elapsed_ms = (t1 - t0) * 1000 / iters\n","\n","    print(f\"{name}: {elapsed_ms:.3f} ms/iter \"\n","          f\"(B={B}, S={S}, D={D}, device={device})\")\n","\n","\n","if __name__ == \"__main__\":\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(\"Using device:\", device)\n","\n","    # Quick sanity check: ScatterBatchNorm1d ~= BatchNorm1d (train-mode)\n","    N, C = 8, 16\n","    x_test = torch.randn(N, C, device=device)\n","    bn = nn.BatchNorm1d(C, affine=True, track_running_stats=False).to(device)\n","    sbn = ScatterBatchNorm1d(C, affine=True).to(device)\n","    with torch.no_grad():\n","        sbn.weight.copy_(bn.weight)\n","        sbn.bias.copy_(bn.bias)\n","    y1 = bn(x_test)\n","    y2 = sbn(x_test)\n","    print(\"Max diff BN vs ScatterBN (sanity check):\",\n","          (y1 - y2).abs().max().item())\n","\n","    # Benchmark the two transformer blocks\n","    benchmark_block(MiniTransformerBlockBN(), \"MiniTransformer + BatchNorm\", device=device)\n","    benchmark_block(MiniTransformerBlockScatterBN(), \"MiniTransformer + ScatterMeanBN\", device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxNCUNgYO6BW","executionInfo":{"status":"ok","timestamp":1767148402131,"user_tz":480,"elapsed":477,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"aafcffb4-845a-4e7e-d1b1-49ad2bda3921"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Max diff BN vs ScatterBN (sanity check): 3.5762786865234375e-07\n","MiniTransformer + BatchNorm: 2.922 ms/iter (B=32, S=128, D=64, device=cuda)\n","MiniTransformer + ScatterMeanBN: 4.801 ms/iter (B=32, S=128, D=64, device=cuda)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OdpyVQ_bO6KN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ClAL_tNCO6Mj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gWAELR-VO6PB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DoB6-aeHO6Rf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7APL5P6qO6TZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HmBv9a_VO6V6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# karpathy replicating medians in (B,T,C) format with triangular matrix multiply\n","import torch\n","torch.manual_seed(42)\n","a = torch.ones(3,3)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('----')\n","print('b=')\n","print(b)\n","print('----')\n","print('c=')\n","print(c)\n","print('---end multiply w identiy matrix---')\n","\n","# can see matrix multiply with Identiy matrix and data produces sums in the columns. Column sums 2+6+6=16, 7+4+5=16\n","# the matrix multiply is a sum when we take the dot product. First row [1,1,1] * first col [2,6,6] gives sum 2+6+6=14,\n","\n","# second step take the lower triangular matrix instead if Identity matrix. This adds 0s\n","print('---replace with lower triangular and make rows sum to 1---')\n","print('  ')\n","a = torch.tril(torch.ones(3,3))/torch.sum(a,1,keepdim=True)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('----')\n","print('b=')\n","print(b)\n","print('----')\n","print('c=')\n","print(c)\n","print('---end multiply w lower triagular rows normalzied to sum 1---')\n"],"metadata":{"id":"GipzwfmKhI-L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767148413196,"user_tz":480,"elapsed":69,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"b46f1191-7884-435c-e4e5-7796aa5f8cb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","----\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","----\n","c=\n","tensor([[14., 16.],\n","        [14., 16.],\n","        [14., 16.]])\n","---end multiply w identiy matrix---\n","---replace with lower triangular and make rows sum to 1---\n","  \n","a=\n","tensor([[0.3333, 0.0000, 0.0000],\n","        [0.3333, 0.3333, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","----\n","b=\n","tensor([[0., 4.],\n","        [0., 3.],\n","        [8., 4.]])\n","----\n","c=\n","tensor([[0.0000, 1.3333],\n","        [0.0000, 2.3333],\n","        [2.6667, 3.6667]])\n","---end multiply w lower triagular rows normalzied to sum 1---\n"]}]},{"cell_type":"code","source":["import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","try:\n","    import triton\n","    import triton.language as tl\n","    HAS_TRITON = True\n","except ImportError:\n","    HAS_TRITON = False\n","    print(\"Triton not found; Triton-based ScatterBN will be disabled.\")\n","\n","\n","# ============================================================\n","#  Scatter / segment-mean primitives (PyTorch + Triton)\n","# ============================================================\n","\n","def scatter_mean_1d(vals: torch.Tensor, seg_ids: torch.Tensor, K: int) -> torch.Tensor:\n","    \"\"\"\n","    Generic segment-mean using PyTorch's index_add_.\n","\n","    vals:    (N,)\n","    seg_ids: (N,) int64/int32\n","    K:       number of segments\n","\n","    Returns: (K,) segment means.\n","    \"\"\"\n","    sums = vals.new_zeros(K)\n","    counts = vals.new_zeros(K, dtype=torch.long)\n","\n","    sums.index_add_(0, seg_ids, vals)\n","    counts.index_add_(0, seg_ids, torch.ones_like(seg_ids, dtype=torch.long))\n","\n","    return sums / counts.clamp_min(1).to(sums.dtype)\n","\n","\n","# ---------------- Triton kernel for per-channel stats ----------------\n","\n","if HAS_TRITON:\n","    @triton.jit\n","    def bn_stats_triton_kernel(\n","        x_ptr,        # *f32, shape (N, C), row-major\n","        mean_ptr,     # *f32, shape (C,)\n","        var_ptr,      # *f32, shape (C,)\n","        N, C,\n","        BLOCK_N: tl.constexpr,\n","    ):\n","        \"\"\"\n","        Each program handles one channel (feature dimension).\n","        We compute mean and variance across the N dimension for that channel.\n","\n","        x is assumed contiguous of shape (N, C).\n","        \"\"\"\n","\n","        c = tl.program_id(0)  # channel index\n","        if c >= C:\n","            return\n","\n","        # Accumulators in fp32\n","        sum_x = tl.zeros((), dtype=tl.float32)\n","        sum_x2 = tl.zeros((), dtype=tl.float32)\n","\n","        # Loop over rows in tiles of BLOCK_N\n","        # (We manually unroll over N dimension)\n","        n = 0\n","        while n < N:\n","            offs_n = n + tl.arange(0, BLOCK_N)\n","            mask = offs_n < N\n","\n","            # x index = n * C + c (row-major)\n","            idx = offs_n * C + c\n","            x_vals = tl.load(x_ptr + idx, mask=mask, other=0.0)\n","            x_vals = x_vals.to(tl.float32)\n","\n","            sum_x += tl.sum(x_vals, axis=0)\n","            sum_x2 += tl.sum(x_vals * x_vals, axis=0)\n","\n","            n += BLOCK_N\n","\n","        n_f = tl.full((), N, dtype=tl.float32)\n","        mean = sum_x / n_f\n","        ex2 = sum_x2 / n_f\n","        var = ex2 - mean * mean\n","\n","        tl.store(mean_ptr + c, mean)\n","        tl.store(var_ptr + c, var)\n","\n","\n","    def bn_stats_triton(x: torch.Tensor):\n","        \"\"\"\n","        x: (N, C) float32, contiguous\n","\n","        Returns:\n","            mean: (C,)\n","            var:  (C,)\n","        \"\"\"\n","        assert x.ndim == 2\n","        assert x.is_cuda\n","        assert x.dtype == torch.float32\n","        x = x.contiguous()\n","        N, C = x.shape\n","\n","        mean = torch.empty(C, device=x.device, dtype=torch.float32)\n","        var = torch.empty(C, device=x.device, dtype=torch.float32)\n","\n","        BLOCK_N = 256\n","        grid = (C,)\n","\n","        bn_stats_triton_kernel[grid](\n","            x, mean, var,\n","            N, C,\n","            BLOCK_N=BLOCK_N,\n","        )\n","\n","        return mean, var\n","\n","\n","# ============================================================\n","#  Normalization modules\n","# ============================================================\n","\n","class ScatterBatchNorm1d(nn.Module):\n","    \"\"\"\n","    BatchNorm1d implemented via generic scatter/segment-mean using PyTorch ops.\n","\n","    Input: (N, C)\n","    \"\"\"\n","\n","    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True):\n","        super().__init__()\n","        self.num_features = num_features\n","        self.eps = eps\n","        self.affine = affine\n","\n","        if affine:\n","            self.weight = nn.Parameter(torch.ones(num_features))\n","            self.bias = nn.Parameter(torch.zeros(num_features))\n","        else:\n","            self.register_parameter(\"weight\", None)\n","            self.register_parameter(\"bias\", None)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (N, C)\n","        \"\"\"\n","        N, C = x.shape\n","        assert C == self.num_features\n","\n","        vals = x.reshape(-1)  # (N*C,)\n","\n","        # segment ids: feature index for each scalar\n","        seg_ids = torch.arange(C, device=x.device).repeat(N)  # (N*C,)\n","        K = C\n","\n","        mean = scatter_mean_1d(vals, seg_ids, K)                # (C,)\n","        mean2 = scatter_mean_1d(vals * vals, seg_ids, K)        # (C,)\n","        var = mean2 - mean * mean                               # (C,)\n","\n","        mean = mean.view(1, C)\n","        var = var.view(1, C)\n","\n","        y = (x - mean) / torch.sqrt(var + self.eps)\n","\n","        if self.affine:\n","            y = y * self.weight.view(1, C) + self.bias.view(1, C)\n","\n","        return y\n","\n","\n","class ScatterBatchNorm1dTriton(nn.Module):\n","    \"\"\"\n","    BatchNorm1d-like layer whose per-channel stats are computed via a Triton kernel.\n","\n","    Input: (N, C)\n","    \"\"\"\n","\n","    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True):\n","        super().__init__()\n","        if not HAS_TRITON:\n","            raise RuntimeError(\"Triton is not available but ScatterBatchNorm1dTriton was instantiated.\")\n","\n","        self.num_features = num_features\n","        self.eps = eps\n","        self.affine = affine\n","\n","        if affine:\n","            self.weight = nn.Parameter(torch.ones(num_features))\n","            self.bias = nn.Parameter(torch.zeros(num_features))\n","        else:\n","            self.register_parameter(\"weight\", None)\n","            self.register_parameter(\"bias\", None)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (N, C), float32, CUDA\n","        \"\"\"\n","        assert x.is_cuda\n","        assert x.dtype == torch.float32\n","\n","        N, C = x.shape\n","        assert C == self.num_features\n","\n","        mean, var = bn_stats_triton(x)  # (C,), (C,)\n","\n","        mean = mean.view(1, C)\n","        var = var.view(1, C)\n","\n","        y = (x - mean) / torch.sqrt(var + self.eps)\n","\n","        if self.affine:\n","            y = y * self.weight.view(1, C) + self.bias.view(1, C)\n","\n","        return y\n","\n","\n","# ============================================================\n","#  Mini Transformer blocks\n","# ============================================================\n","\n","class MiniTransformerBlockBN(nn.Module):\n","    \"\"\"\n","    Mini Transformer block with standard nn.BatchNorm1d.\n","    \"\"\"\n","\n","    def __init__(self, d_model=64, nhead=4, dim_ff=128, dropout=0.1):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n","        self.bn1 = nn.BatchNorm1d(d_model, affine=True, track_running_stats=False)\n","        self.bn2 = nn.BatchNorm1d(d_model, affine=True, track_running_stats=False)\n","\n","        self.linear1 = nn.Linear(d_model, dim_ff)\n","        self.linear2 = nn.Linear(dim_ff, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: (B, S, D)\n","        \"\"\"\n","        B, S, D = x.shape\n","\n","        attn_out, _ = self.attn(x, x, x, need_weights=False)\n","        x = x + attn_out\n","\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.bn1(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        y = self.linear2(self.dropout(self.activation(self.linear1(x))))\n","        x = x + y\n","\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.bn2(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        return x\n","\n","\n","class MiniTransformerBlockScatterBN(nn.Module):\n","    \"\"\"\n","    Mini Transformer block using ScatterBatchNorm1d (PyTorch scatter).\n","    \"\"\"\n","\n","    def __init__(self, d_model=64, nhead=4, dim_ff=128, dropout=0.1):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n","        self.sbn1 = ScatterBatchNorm1d(d_model, affine=True)\n","        self.sbn2 = ScatterBatchNorm1d(d_model, affine=True)\n","\n","        self.linear1 = nn.Linear(d_model, dim_ff)\n","        self.linear2 = nn.Linear(dim_ff, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        B, S, D = x.shape\n","\n","        attn_out, _ = self.attn(x, x, x, need_weights=False)\n","        x = x + attn_out\n","\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.sbn1(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        y = self.linear2(self.dropout(self.activation(self.linear1(x))))\n","        x = x + y\n","\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.sbn2(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        return x\n","\n","\n","class MiniTransformerBlockScatterBNTriton(nn.Module):\n","    \"\"\"\n","    Mini Transformer block using ScatterBatchNorm1dTriton.\n","    \"\"\"\n","\n","    def __init__(self, d_model=64, nhead=4, dim_ff=128, dropout=0.1):\n","        super().__init__()\n","        if not HAS_TRITON:\n","            raise RuntimeError(\"Triton not available; cannot create Triton-based block.\")\n","\n","        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n","        self.sbn1 = ScatterBatchNorm1dTriton(d_model, affine=True)\n","        self.sbn2 = ScatterBatchNorm1dTriton(d_model, affine=True)\n","\n","        self.linear1 = nn.Linear(d_model, dim_ff)\n","        self.linear2 = nn.Linear(dim_ff, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        B, S, D = x.shape\n","\n","        attn_out, _ = self.attn(x, x, x, need_weights=False)\n","        x = x + attn_out\n","\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.sbn1(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        y = self.linear2(self.dropout(self.activation(self.linear1(x))))\n","        x = x + y\n","\n","        x_flat = x.view(B * S, D)\n","        x_flat = self.sbn2(x_flat)\n","        x = x_flat.view(B, S, D)\n","\n","        return x\n","\n","\n","# ============================================================\n","#  Benchmarking utilities\n","# ============================================================\n","\n","def benchmark_block(\n","    block: nn.Module,\n","    name: str,\n","    device: str = \"cuda\",\n","    B: int = 32,\n","    S: int = 128,\n","    D: int = 64,\n","    iters: int = 50,\n","    warmup: int = 10,\n","):\n","    block = block.to(device)\n","    block.train()\n","    x = torch.randn(B, S, D, device=device, dtype=torch.float32)\n","    opt = torch.optim.Adam(block.parameters(), lr=1e-3)\n","\n","    # Warmup\n","    for _ in range(warmup):\n","        opt.zero_grad(set_to_none=True)\n","        y = block(x)\n","        loss = y.pow(2).mean()\n","        loss.backward()\n","        opt.step()\n","\n","    # Timing (forward + backward)\n","    if device.startswith(\"cuda\"):\n","        torch.cuda.synchronize()\n","        start = torch.cuda.Event(enable_timing=True)\n","        end = torch.cuda.Event(enable_timing=True)\n","\n","        start.record()\n","        for _ in range(iters):\n","            opt.zero_grad(set_to_none=True)\n","            y = block(x)\n","            loss = y.pow(2).mean()\n","            loss.backward()\n","            opt.step()\n","        end.record()\n","        torch.cuda.synchronize()\n","        elapsed_ms = start.elapsed_time(end) / iters\n","    else:\n","        t0 = time.time()\n","        for _ in range(iters):\n","            opt.zero_grad(set_to_none=True)\n","            y = block(x)\n","            loss = y.pow(2).mean()\n","            loss.backward()\n","            opt.step()\n","        t1 = time.time()\n","        elapsed_ms = (t1 - t0) * 1000 / iters\n","\n","    print(f\"{name}: {elapsed_ms:.3f} ms/iter \"\n","          f\"(B={B}, S={S}, D={D}, device={device})\")\n","\n","\n","if __name__ == \"__main__\":\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(\"Using device:\", device)\n","\n","    # Sanity check for PyTorch ScatterBN vs BatchNorm1d\n","    N, C = 8, 16\n","    x_test = torch.randn(N, C, device=device)\n","    bn = nn.BatchNorm1d(C, affine=True, track_running_stats=False).to(device)\n","    sbn = ScatterBatchNorm1d(C, affine=True).to(device)\n","    with torch.no_grad():\n","        sbn.weight.copy_(bn.weight)\n","        sbn.bias.copy_(bn.bias)\n","    y1 = bn(x_test)\n","    y2 = sbn(x_test)\n","    print(\"Max diff BN vs ScatterBN(PyTorch):\",\n","          (y1 - y2).abs().max().item())\n","\n","    # If Triton is available, sanity check it too\n","    if HAS_TRITON and device == \"cuda\":\n","        sbn_t = ScatterBatchNorm1dTriton(C, affine=True).to(device)\n","        with torch.no_grad():\n","            sbn_t.weight.copy_(bn.weight)\n","            sbn_t.bias.copy_(bn.bias)\n","        y3 = sbn_t(x_test)\n","        print(\"Max diff BN vs ScatterBN(Triton):\",\n","              (y1 - y3).abs().max().item())\n","\n","    print(\"\\n=== Benchmarking mini Transformer blocks ===\")\n","    benchmark_block(MiniTransformerBlockBN(), \"MiniTransformer + BatchNorm\", device=device)\n","    benchmark_block(MiniTransformerBlockScatterBN(), \"MiniTransformer + ScatterBN(PyTorch)\", device=device)\n","\n","    if HAS_TRITON and device == \"cuda\":\n","        benchmark_block(MiniTransformerBlockScatterBNTriton(), \"MiniTransformer + ScatterBN(Triton)\", device=device)\n","    else:\n","        print(\"Skipping Triton-based block benchmark (no Triton or no CUDA).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Xi_cG3fPwjC","executionInfo":{"status":"ok","timestamp":1767148418561,"user_tz":480,"elapsed":911,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"e73679ec-1163-4d98-b9aa-cbe06c63b1bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Max diff BN vs ScatterBN(PyTorch): 4.76837158203125e-07\n","Max diff BN vs ScatterBN(Triton): 2.384185791015625e-07\n","\n","=== Benchmarking mini Transformer blocks ===\n","MiniTransformer + BatchNorm: 4.074 ms/iter (B=32, S=128, D=64, device=cuda)\n","MiniTransformer + ScatterBN(PyTorch): 4.648 ms/iter (B=32, S=128, D=64, device=cuda)\n","MiniTransformer + ScatterBN(Triton): 3.073 ms/iter (B=32, S=128, D=64, device=cuda)\n"]}]},{"cell_type":"code","source":["x=torch.tensor([[1,2,3],[4,5,6]])\n","print(x, x.shape)\n","#y = x/torch.sum(x,dim=1)\n","print(torch.sum(x, dim=0, keepdim=True))\n","print(torch.sum(x, dim=0, keepdim=False))\n","print(torch.sum(x, dim=1, keepdim=True))\n","print(torch.sum(x, dim=1, keepdim=False))\n","#print(s)\n","print(torch.sum(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucxpn5RQZ6nM","executionInfo":{"status":"ok","timestamp":1764652076726,"user_tz":480,"elapsed":7,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"6de78ca2-5167-4744-c266-39feeb434cb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2, 3],\n","        [4, 5, 6]]) torch.Size([2, 3])\n","tensor([[5, 7, 9]])\n","tensor([5, 7, 9])\n","tensor([[ 6],\n","        [15]])\n","tensor([ 6, 15])\n","tensor([[3],\n","        [7]])\n","tensor(21)\n"]}]},{"cell_type":"markdown","source":["# **Matrix and Vector multiply for averages.**\n","\n","There are different versions of averages which are used for normalization. Karpathy develops the t-1 or the autoregresseive average where the average of the sequence of a,b,c,d has 4 averages; at t=0, avg=a, t=1 avg=(a+b)/2, t=2 avg=\n","(a+b+c)/3, etc...\n","\n","\n","<ul>\n","<li>$v = \\frac{1}{N}[1,1,1,...]$ where num ones = N</li>\n","<li>A cumulative average is the conventional average $\\frac{1}{N}\\sum_0^{N-1}x_i$. It is time invariant. Shifting the sequence produces the same average. $v=[1,1,1...len(x)]$ and the data is $x$. cumulative avg = $v^T@x$</li>\n","<li>A column average $v^T@X$</li>\n","<li>How to derive row and column avg. X=(B,D). Create a 2x3 test matrix\n","[[1,2,3],[4,5,6]]. Make sure the TM is not symmetric to reduce confusion. We have to options an identity matrix 1x2 if I@X or 3x1 if X@I. There are 2 rows so a row sum must have 2 rows so you know 1x2 is rows and there are 3 cols and you know you need 3 columns for a column sum. so I@X is row sum and X@I is column sum. Then add 1/N to get avg. N=num elements in row or col.\n","</li>\n","<li>A row average $X@v$</li>\n","<li>Weighted avg for softmax. W=(T), V=(T,D). $avg=W^T@V$ Output = (D,) or (D,1) if keepdims=True. Because avg collapases and removes dimensions by default</li>\n","<li>How to derive Weighted Softmax Avg</li>\n","<li>Sequence avg: <li>\n","<li>How to derive sequence avg. </li>\n","</ul>"],"metadata":{"id":"2VKZ-B-GGCGX"}},{"cell_type":"code","source":["import torch\n","\n","x = torch.tensor([1,2,3,4,5,6]).float()\n","\n","v = (1/6)*(torch.ones(6))\n","print(\"avg of sum of all elements v@x:\",v@x)\n","tri = torch.tril(torch.ones(6,6))\n","avg = (tri@x)/torch.arange(1,7)\n","\n","print(\"sums:\",tri@x)\n","print(\"torch arange:\",torch.arange(1,7))\n","print(\"rolling avg:\",avg) # 1/1, (1+2)/2, (1+2+3)/3,..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zRqW-1qzYSoh","executionInfo":{"status":"ok","timestamp":1764641233838,"user_tz":480,"elapsed":45,"user":{"displayName":"doug chang","userId":"06495228775351504429"}},"outputId":"9c5e9486-b76b-4041-d6f7-5677cd7643df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["avg of sum of all elements v@x: tensor(3.5000)\n","sums: tensor([ 1.,  3.,  6., 10., 15., 21.])\n","torch arange: tensor([1, 2, 3, 4, 5, 6])\n","avg: tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000])\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from typing import Optional, Union\n","\n","# avoid division by 0\n","def safe_torch_mean(x: torch.Tensor) -> torch.Tensor:\n","    return x.float().sum() / max(x.numel(), 1)\n","\n","def safe_mean(\n","    x: torch.Tensor,\n","    dim: Optional[int] = None,\n","    keepdim: bool = False,\n","    default: Union[float, int] = 0.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Mean that:\n","      - casts non-floating dtypes to float32\n","      - returns `default` when there are no elements along `dim`\n","    \"\"\"\n","    if not x.is_floating_point():\n","        x = x.to(torch.float32)\n","\n","    if dim is None:\n","        if x.numel() == 0:\n","            # scalar default\n","            return x.new_tensor(float(default))\n","        return x.mean()\n","\n","    # Mean along a dimension\n","    if x.size(dim) == 0:\n","        # build output shape manually\n","        out_shape = list(x.shape)\n","        if keepdim:\n","            out_shape[dim] = 1\n","        else:\n","            del out_shape[dim]\n","        return x.new_full(out_shape, float(default))\n","\n","    return x.mean(dim=dim, keepdim=keepdim)\n","\n","x = torch.tensor([], dtype=torch.float32)\n","print(safe_mean(x))  # tensor(0.)\n","\n","x = torch.randint(0, 10, (3,), dtype=torch.int8)\n","print(safe_mean(x))  # float32 mean, no error\n","\n","\n","def masked_mean(\n","    x: torch.Tensor,\n","    mask: torch.Tensor,\n","    dim: int,\n","    keepdim: bool = False,\n","    default: Union[float, int] = 0.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Mean over elements where mask == 1/True along `dim`.\n","\n","    x: (..., D, ...)\n","    mask: same shape as x or broadcastable to x\n","    \"\"\"\n","    if not x.is_floating_point():\n","        x = x.to(torch.float32)\n","\n","    # make mask float for multiplication\n","    m = mask.to(x.dtype)\n","    # broadcast OK: this relies on PyTorch broadcasting\n","    masked_x = x * m\n","\n","    # sum over dim\n","    num = masked_x.sum(dim=dim, keepdim=keepdim)\n","    den = m.sum(dim=dim, keepdim=keepdim)\n","\n","    # safe division: where den > 0, num / den; else default\n","    default_tensor = num.new_full(num.shape, float(default))\n","    mean = torch.where(den > 0, num / torch.clamp(den, min=1e-12), default_tensor)\n","    return mean\n","\n","x = torch.tensor([[1., 2., 3.],\n","                  [4., 5., 6.]])\n","mask = torch.tensor([[1, 0, 1],\n","                     [0, 0, 0]])  # second row all masked out\n","\n","print(masked_mean(x, mask, dim=1))\n","# tensor([2., 0.])  (last row default=0)\n","\n","print(masked_mean(x, mask, dim=1, default=-1.0))\n","# tensor([2., -1.])\n","\n","\n","def segment_mean(\n","    values: torch.Tensor,\n","    segment_ids: torch.Tensor,\n","    num_segments: Optional[int] = None,\n","    default: Union[float, int] = 0.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Compute mean over segments:\n","        segment_means[k] = mean(values[segment_ids == k])\n","\n","    values: (N, D) or (N,)\n","    segment_ids: (N,) ints in [0, num_segments-1]\n","    \"\"\"\n","    if not values.is_floating_point():\n","        values = values.to(torch.float32)\n","\n","    if values.dim() == 1:\n","        values = values.unsqueeze(-1)  # make it (N, 1)\n","\n","    N, D = values.shape\n","    segment_ids = segment_ids.to(torch.long)\n","\n","    if num_segments is None:\n","        num_segments = int(segment_ids.max().item()) + 1 if N > 0 else 0\n","\n","    device = values.device\n","    dtype = values.dtype\n","\n","    # sums for each segment\n","    sums = torch.zeros(num_segments, D, device=device, dtype=dtype)\n","    counts = torch.zeros(num_segments, 1, device=device, dtype=dtype)\n","\n","    # index_add along segment dimension\n","    sums.index_add_(0, segment_ids, values)\n","    counts.index_add_(0, segment_ids, torch.ones_like(values[:, :1]))\n","\n","    default_tensor = sums.new_full(sums.shape, float(default))\n","    means = torch.where(\n","        counts > 0,\n","        sums / torch.clamp(counts, min=1e-12),\n","        default_tensor,\n","    )\n","\n","    # squeeze if original was 1D\n","    if values.shape[1] == 1:\n","        means = means.squeeze(-1)\n","\n","    return means\n","\n","vals = torch.tensor([[1., 2.],\n","                     [3., 4.],\n","                     [10., 20.]], dtype=torch.float32)\n","seg = torch.tensor([0, 0, 2])   # segment 1 is empty\n","\n","print(segment_mean(vals, seg, num_segments=3, default=0.0))\n","# tensor([[2., 3.],      # mean of rows 0 and 1\n","#         [0., 0.],      # empty segment -> default\n","#         [10., 20.]])   # row 2\n","\n","def batch_safe_mean(\n","    x: torch.Tensor,\n","    mask: torch.Tensor,\n","    default: Union[float, int] = 0.0,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Per-batch masked mean over time dimension 1.\n","\n","    x: (B, T, D) or (B, T)\n","    mask: (B, T) with 1/True = valid entries\n","    \"\"\"\n","    if x.dim() == 2:\n","        # (B, T) -> (B, T, 1) so we reuse the same logic\n","        x = x.unsqueeze(-1)\n","        squeeze_back = True\n","    else:\n","        squeeze_back = False\n","\n","    # broadcast mask to (B, T, 1)\n","    mask_exp = mask.unsqueeze(-1)\n","\n","    means = masked_mean(\n","        x,\n","        mask_exp,\n","        dim=1,           # average over time\n","        keepdim=False,\n","        default=default,\n","    )\n","\n","    if squeeze_back:\n","        means = means.squeeze(-1)\n","\n","    return means\n","\n","B, T, D = 2, 5, 3\n","x = torch.randn(B, T, D)\n","mask = torch.tensor([\n","    [1, 1, 1, 0, 0],   # first sequence length 3\n","    [0, 0, 0, 0, 0],   # second is fully padded\n","])\n","\n","m = batch_safe_mean(x, mask, default=0.0)\n","print(m.shape)   # (2, 3)\n","# row 0: mean over first 3 time steps\n","# row 1: [0., 0., 0.] from default\n","\n","import numpy as np\n","\n","def np_safe_mean(x: np.ndarray, axis=None, keepdims=False):\n","  if np.size(x) == 0:\n","      # Empty → return 0 with requested shape\n","      if axis is None:\n","          return 0.0\n","      # build shape as if mean had been taken, but filled with 0\n","      return np.zeros(np.mean(x, axis=axis, keepdims=keepdims).shape, dtype=float)\n","\n","  # normal mean is fine when non-empty\n","  return np.mean(x, axis=axis, keepdims=keepdims)\n","\n","\n","\n","def np_safe_masked_mean(x: np.ndarray,\n","                        mask: np.ndarray,\n","                        axis=None,\n","                        keepdims=False):\n","    m = mask.astype(float)\n","    masked = x * m\n","\n","    num = masked.sum(axis=axis, keepdims=keepdims)\n","    count = m.sum(axis=axis, keepdims=keepdims)\n","    safe_count = np.clip(count, 1.0, None)\n","    return num / safe_count\n","\n","\n","\n","def np_safe_segment_mean(values: np.ndarray,\n","                         segment_ids: np.ndarray,\n","                         num_segments: int | None = None):\n","    if num_segments is None:\n","        num_segments = int(segment_ids.max()) + 1\n","\n","    rest_shape = values.shape[1:]\n","    sums = np.zeros((num_segments, *rest_shape), dtype=values.dtype)\n","    counts = np.zeros(num_segments, dtype=float)\n","\n","    np.add.at(sums, segment_ids, values)\n","    np.add.at(counts, segment_ids, 1.0)\n","\n","    counts = np.clip(counts, 1.0, None)\n","    # reshape for broadcast\n","    while counts.ndim < sums.ndim:\n","        counts = counts[..., None]\n","\n","    return sums / counts\n","\n","\n"],"metadata":{"id":"_NJPCrNE44OS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# triton_safe_masked_mean.py\n","import torch\n","import triton\n","import triton.language as tl\n","\n","@triton.jit\n","def safe_masked_mean_rowwise_kernel(\n","    x_ptr,         # *f32\n","    mask_ptr,      # *i32 or *f32 (0/1); pass nullptr for unmasked\n","    out_ptr,       # *f32\n","    B, N,\n","    stride_xb, stride_xn,\n","    stride_mb, stride_mn,\n","    BLOCK_N: tl.constexpr,\n","):\n","    b = tl.program_id(0)  # batch index\n","\n","    offs_n = tl.arange(0, BLOCK_N)\n","    row_x_ptr = x_ptr + b * stride_xb + offs_n * stride_xn\n","\n","    has_mask = mask_ptr != 0\n","    if has_mask:\n","        row_m_ptr = mask_ptr + b * stride_mb + offs_n * stride_mn\n","\n","    acc_sum = tl.zeros((), dtype=tl.float32)\n","    acc_count = tl.zeros((), dtype=tl.float32)\n","\n","    for start_n in range(0, N, BLOCK_N):\n","        cur_mask = start_n + offs_n < N\n","\n","        x_vals = tl.load(row_x_ptr + start_n * stride_xn,\n","                         mask=cur_mask,\n","                         other=0.0)\n","\n","        if has_mask:\n","            m_vals = tl.load(row_m_ptr + start_n * stride_mn,\n","                             mask=cur_mask,\n","                             other=0)\n","            # assume mask is 0/1 or bool, cast to float\n","            m_vals_f = m_vals.to(tl.float32)\n","        else:\n","            m_vals_f = tl.where(cur_mask, 1.0, 0.0)\n","\n","        acc_sum += tl.sum(x_vals * m_vals_f, axis=0)\n","        acc_count += tl.sum(m_vals_f, axis=0)\n","\n","    # safe mean: if acc_count == 0, define mean = 0\n","    safe_count = tl.maximum(acc_count, 1.0)\n","    mean = acc_sum / safe_count\n","    mean = tl.where(acc_count > 0, mean, 0.0)\n","\n","    tl.store(out_ptr + b, mean)\n","\n","def safe_masked_mean_rowwise(x: torch.Tensor, mask: torch.Tensor | None = None):\n","    \"\"\"\n","    x: (B, N), float32\n","    mask: (B, N) or None; 0/1 or bool\n","    returns: (B,)\n","    \"\"\"\n","    assert x.dim() == 2\n","    B, N = x.shape\n","    x = x.contiguous()\n","\n","    if mask is not None:\n","        mask = mask.to(torch.int32).contiguous()\n","        mask_ptr = mask\n","    else:\n","        mask_ptr = torch.tensor([], device=x.device, dtype=torch.int32)  # dummy\n","        # We'll treat 'mask_ptr != 0' as 'has_mask', so ensure it's not literally None.\n","        # Instead: we pass mask_ptr.data_ptr()==0? Triton can't; hack: pass 0 below.\n","\n","    out = torch.empty(B, device=x.device, dtype=torch.float32)\n","\n","    BLOCK_N = 128\n","    grid = (B,)\n","\n","    # carefully pass 0 for mask_ptr when mask is None\n","    mask_arg = mask_ptr if mask is not None else 0\n","\n","    safe_masked_mean_rowwise_kernel[grid](\n","        x,\n","        mask_arg,\n","        out,\n","        B, N,\n","        x.stride(0), x.stride(1),\n","        mask.stride(0) if mask is not None else 0,\n","        mask.stride(1) if mask is not None else 0,\n","        BLOCK_N=BLOCK_N,\n","    )\n","    return out\n","\n","// safe_masked_mean.cu\n","#include <cuda_runtime.h>\n","#include <stdint.h>\n","\n","__global__ void safe_masked_mean_kernel(\n","    const float* __restrict__ x,\n","    const uint8_t* __restrict__ mask,  // 0 or 1; nullptr for unmasked\n","    int64_t N,\n","    float* __restrict__ out_sum,\n","    float* __restrict__ out_count\n",") {\n","    extern __shared__ float shmem[];\n","    float* sh_sum   = shmem;\n","    float* sh_count = shmem + blockDim.x;\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    float local_sum = 0.0f;\n","    float local_count = 0.0f;\n","\n","    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n","        uint8_t m = mask ? mask[i] : 1;\n","        if (m) {\n","            local_sum += x[i];\n","            local_count += 1.0f;\n","        }\n","    }\n","\n","    sh_sum[threadIdx.x] = local_sum;\n","    sh_count[threadIdx.x] = local_count;\n","    __syncthreads();\n","\n","    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n","        if (threadIdx.x < stride) {\n","            sh_sum[threadIdx.x] += sh_sum[threadIdx.x + stride];\n","            sh_count[threadIdx.x] += sh_count[threadIdx.x + stride];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (threadIdx.x == 0) {\n","        atomicAdd(out_sum, sh_sum[0]);\n","        atomicAdd(out_count, sh_count[0]);\n","    }\n","}\n","\n","\n","#include <torch/extension.h>  // or your own wrapper\n","#include <cuda_runtime.h>\n","\n","std::pair<float, float> safe_masked_mean_cuda(\n","    const float* d_x,\n","    const uint8_t* d_mask,\n","    int64_t N\n",") {\n","    float h_sum = 0.0f;\n","    float h_count = 0.0f;\n","\n","    float* d_sum;\n","    float* d_count;\n","    cudaMalloc(&d_sum, sizeof(float));\n","    cudaMalloc(&d_count, sizeof(float));\n","    cudaMemcpy(d_sum, &h_sum, sizeof(float), cudaMemcpyHostToDevice);\n","    cudaMemcpy(d_count, &h_count, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    int threads = 256;\n","    int blocks = (N + threads - 1) / threads;\n","    size_t shmem = 2 * threads * sizeof(float);\n","\n","    safe_masked_mean_kernel<<<blocks, threads, shmem>>>(d_x, d_mask, N, d_sum, d_count);\n","\n","    cudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n","    cudaMemcpy(&h_count, d_count, sizeof(float), cudaMemcpyDeviceToHost);\n","\n","    cudaFree(d_sum);\n","    cudaFree(d_count);\n","\n","    // safe mean\n","    float mean = (h_count > 0.0f) ? (h_sum / h_count) : 0.0f;\n","    return {mean, h_count};\n","}\n","\n","sum[seg] = Σ x[i] for i with segment_ids[i] == seg and mask[i]==1\n","count[seg] = Σ 1    for same\n","mean[seg] = sum[seg] / max(count[seg], 1)\n","\n","// segment_mean.cu\n","#include <cuda_runtime.h>\n","#include <stdint.h>\n","\n","__global__ void segment_sum_count_kernel(\n","    const float* __restrict__ x,\n","    const int32_t* __restrict__ segment_ids,\n","    const uint8_t* __restrict__ mask,    // 0/1 or nullptr\n","    int64_t N,\n","    float* __restrict__ seg_sums,\n","    float* __restrict__ seg_counts,\n","    int32_t num_segments\n",") {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n","        uint8_t m = mask ? mask[i] : 1;\n","        if (!m) {\n","            continue;\n","        }\n","\n","        int32_t seg = segment_ids[i];\n","        if (seg < 0 || seg >= num_segments) {\n","            continue;  // or assert\n","        }\n","\n","        float val = x[i];\n","        atomicAdd(&seg_sums[seg], val);\n","        atomicAdd(&seg_counts[seg], 1.0f);\n","    }\n","}\n","\n","__global__ void segment_safe_mean_kernel(\n","    const float* __restrict__ seg_sums,\n","    const float* __restrict__ seg_counts,\n","    float* __restrict__ seg_means,\n","    int32_t num_segments\n",") {\n","    int seg = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (seg >= num_segments) return;\n","\n","    float s = seg_sums[seg];\n","    float c = seg_counts[seg];\n","\n","    if (c > 0.0f) {\n","        seg_means[seg] = s / c;\n","    } else {\n","        seg_means[seg] = 0.0f;  // safe mean for empty segment\n","    }\n","}\n","\n","#include <torch/extension.h>  // or your own wrapper\n","#include <cuda_runtime.h>\n","\n","std::pair<float, float> safe_masked_mean_cuda(\n","    const float* d_x,\n","    const uint8_t* d_mask,\n","    int64_t N\n",") {\n","    float h_sum = 0.0f;\n","    float h_count = 0.0f;\n","\n","    float* d_sum;\n","    float* d_count;\n","    cudaMalloc(&d_sum, sizeof(float));\n","    cudaMalloc(&d_count, sizeof(float));\n","    cudaMemcpy(d_sum, &h_sum, sizeof(float), cudaMemcpyHostToDevice);\n","    cudaMemcpy(d_count, &h_count, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    int threads = 256;\n","    int blocks = (N + threads - 1) / threads;\n","    size_t shmem = 2 * threads * sizeof(float);\n","\n","    safe_masked_mean_kernel<<<blocks, threads, shmem>>>(d_x, d_mask, N, d_sum, d_count);\n","\n","    cudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n","    cudaMemcpy(&h_count, d_count, sizeof(float), cudaMemcpyDeviceToHost);\n","\n","    cudaFree(d_sum);\n","    cudaFree(d_count);\n","\n","    // safe mean\n","    float mean = (h_count > 0.0f) ? (h_sum / h_count) : 0.0f;\n","    return {mean, h_count};\n","}\n","\n","#sum[seg] = Σ x[i] for i with segment_ids[i] == seg and mask[i]==1\n","#count[seg] = Σ 1    for same\n","#mean[seg] = sum[seg] / max(count[seg], 1)\n","\n","// segment_mean.cu\n","#include <cuda_runtime.h>\n","#include <stdint.h>\n","\n","__global__ void segment_sum_count_kernel(\n","    const float* __restrict__ x,\n","    const int32_t* __restrict__ segment_ids,\n","    const uint8_t* __restrict__ mask,    // 0/1 or nullptr\n","    int64_t N,\n","    float* __restrict__ seg_sums,\n","    float* __restrict__ seg_counts,\n","    int32_t num_segments\n",") {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    for (int64_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n","        uint8_t m = mask ? mask[i] : 1;\n","        if (!m) {\n","            continue;\n","        }\n","\n","        int32_t seg = segment_ids[i];\n","        if (seg < 0 || seg >= num_segments) {\n","            continue;  // or assert\n","        }\n","\n","        float val = x[i];\n","        atomicAdd(&seg_sums[seg], val);\n","        atomicAdd(&seg_counts[seg], 1.0f);\n","    }\n","}\n","\n","__global__ void segment_safe_mean_kernel(\n","    const float* __restrict__ seg_sums,\n","    const float* __restrict__ seg_counts,\n","    float* __restrict__ seg_means,\n","    int32_t num_segments\n",") {\n","    int seg = blockIdx.x * blockDim.x + threadIdx.x;\n","    if (seg >= num_segments) return;\n","\n","    float s = seg_sums[seg];\n","    float c = seg_counts[seg];\n","\n","    if (c > 0.0f) {\n","        seg_means[seg] = s / c;\n","    } else {\n","        seg_means[seg] = 0.0f;  // safe mean for empty segment\n","    }\n","}\n","\n","void segment_mean_cuda(\n","    const float* d_x,\n","    const int32_t* d_segment_ids,\n","    const uint8_t* d_mask,      // may be nullptr\n","    int64_t N,\n","    int32_t num_segments,\n","    float* d_out_means\n",") {\n","    float* d_sums;\n","    float* d_counts;\n","    cudaMalloc(&d_sums,   num_segments * sizeof(float));\n","    cudaMalloc(&d_counts, num_segments * sizeof(float));\n","    cudaMemset(d_sums,   0, num_segments * sizeof(float));\n","    cudaMemset(d_counts, 0, num_segments * sizeof(float));\n","\n","    int threads = 256;\n","    int blocks = (N + threads - 1) / threads;\n","\n","    segment_sum_count_kernel<<<blocks, threads>>>(\n","        d_x, d_segment_ids, d_mask, N, d_sums, d_counts, num_segments\n","    );\n","\n","    int blocks_seg = (num_segments + threads - 1) / threads;\n","    segment_safe_mean_kernel<<<blocks_seg, threads>>>(\n","        d_sums, d_counts, d_out_means, num_segments\n","    );\n","\n","    cudaFree(d_sums);\n","    cudaFree(d_counts);\n","}\n","\n","\n","#triton segment mean sketch\n","@triton.jit\n","def segment_sum_count_kernel_triton(\n","    x_ptr, seg_id_ptr, mask_ptr,\n","    seg_sums_ptr, seg_counts_ptr,\n","    N, NUM_SEGMENTS: tl.constexpr,\n","    BLOCK_SIZE: tl.constexpr,\n","):\n","    pid = tl.program_id(0)\n","    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n","    mask = offs < N\n","\n","    x = tl.load(x_ptr + offs, mask=mask, other=0.0)\n","    seg = tl.load(seg_id_ptr + offs, mask=mask, other=0)\n","    has_mask = mask_ptr != 0\n","    if has_mask:\n","        m = tl.load(mask_ptr + offs, mask=mask, other=0)\n","        valid = mask & (m != 0)\n","    else:\n","        valid = mask\n","\n","    x = tl.where(valid, x, 0.0)\n","    seg = tl.where(valid, seg, 0)\n","\n","    # atomic adds\n","    tl.atomic_add(seg_sums_ptr + seg, x, mask=valid)\n","    tl.atomic_add(seg_counts_ptr + seg,\n","                  tl.where(valid, 1.0, 0.0),\n","                  mask=valid)"],"metadata":{"id":"TDCfqhM7pyEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Colab-ready benchmark: safe mean, masked mean, segment mean, batch mean\n","# Across: PyTorch CPU, PyTorch GPU, NumPy CPU, Triton kernel, CUDA (CuPy) kernel\n","import torch\n","!pip install -q triton==3.0.0 cupy-cuda12x\n","\n","import time\n","import math\n","from dataclasses import dataclass\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","\n","import triton\n","import triton.language as tl\n","\n","import cupy as cp\n","\n","# ------------------------------------------------------------\n","# 0. Config\n","# ------------------------------------------------------------\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", DEVICE)\n","\n","# Problem sizes\n","BATCH = 4096     # number of rows\n","N      = 1024    # row length\n","SEG_K  = 128     # number of segments for segment mean\n","\n","WARMUP_ITERS = 5\n","BENCH_ITERS  = 20\n","\n","# ------------------------------------------------------------\n","# 1. Safe mean helpers (PyTorch & NumPy)\n","# ------------------------------------------------------------\n","\n","def torch_safe_mean(x: torch.Tensor, dim=None, keepdim=False) -> torch.Tensor:\n","    \"\"\"\n","    Safe mean: if count == 0, returns 0 (no NaN).\n","    Works with arbitrary dimension, keeps gradients.\n","    \"\"\"\n","    if dim is None:\n","        # flatten\n","        x_flat = x.view(-1)\n","        count = x_flat.numel()\n","        if count == 0:\n","            return x_flat.new_zeros(())\n","        return x_flat.sum() / max(count, 1)\n","    else:\n","        # general dim\n","        x = x.float()\n","        ones = torch.ones_like(x, dtype=x.dtype)\n","        count = ones.sum(dim=dim, keepdim=keepdim)\n","        s = x.sum(dim=dim, keepdim=keepdim)\n","        # clamp denominator to at least 1 to avoid NaNs; where count==0, we force 0\n","        denom = count.clamp_min(1.0)\n","        mean = s / denom\n","        mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n","        return mean\n","\n","def torch_masked_mean(x: torch.Tensor, mask: torch.Tensor, dim=-1, keepdim=False):\n","    \"\"\"\n","    mask: bool or 0/1, same shape as x.\n","    \"\"\"\n","    x = x.float()\n","    mask = mask.to(dtype=x.dtype)\n","    s = (x * mask).sum(dim=dim, keepdim=keepdim)\n","    count = mask.sum(dim=dim, keepdim=keepdim)\n","    denom = count.clamp_min(1.0)\n","    mean = s / denom\n","    mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n","    return mean\n","\n","def torch_segment_mean(x: torch.Tensor, segment_ids: torch.Tensor, num_segments: int):\n","    \"\"\"\n","    x: [N, D] or [N]; segment_ids: [N] in [0, num_segments-1]\n","    Returns [num_segments, D] or [num_segments]\n","    \"\"\"\n","    if x.dim() == 1:\n","        x = x[:, None]\n","        squeeze = True\n","    else:\n","        squeeze = False\n","\n","    N, D = x.shape\n","    device = x.device\n","\n","    segment_ids = segment_ids.to(device=device, dtype=torch.long)\n","    out = torch.zeros(num_segments, D, device=device, dtype=x.dtype)\n","    count = torch.zeros(num_segments, 1, device=device, dtype=x.dtype)\n","\n","    out.index_add_(0, segment_ids, x)\n","    ones = torch.ones(N, 1, device=device, dtype=x.dtype)\n","    count.index_add_(0, segment_ids, ones)\n","\n","    denom = count.clamp_min(1.0)\n","    mean = out / denom\n","    mean = torch.where(count > 0, mean, torch.zeros_like(mean))\n","\n","    if squeeze:\n","        mean = mean[:, 0]\n","    return mean\n","\n","# NumPy equivalents (no gradient)\n","def np_safe_mean(x: np.ndarray, axis=None, keepdims=False):\n","    if x.size == 0:\n","        return np.zeros((), dtype=x.dtype)\n","    count = x.shape[axis] if axis is not None else x.size\n","    s = x.sum(axis=axis, keepdims=keepdims)\n","    return s / max(count, 1)\n","\n","def np_masked_mean(x: np.ndarray, mask: np.ndarray, axis=-1, keepdims=False):\n","    x = x.astype(np.float32)\n","    mask = mask.astype(np.float32)\n","    s = (x * mask).sum(axis=axis, keepdims=keepdims)\n","    count = mask.sum(axis=axis, keepdims=keepdims)\n","    denom = np.clip(count, 1.0, None)\n","    out = s / denom\n","    out = np.where(count > 0, out, np.zeros_like(out))\n","    return out\n","\n","import numpy as np\n","\n","def np_segment_mean(x: np.ndarray,\n","                    segment_ids: np.ndarray,\n","                    num_segments: int):\n","    # Handle 1D x by temporarily promoting to 2D\n","    if x.ndim == 1:\n","        x = x[:, None]   # (N,) -> (N, 1)\n","        squeeze = True\n","    else:\n","        squeeze = False\n","\n","    N, D = x.shape\n","    out = np.zeros((num_segments, D), dtype=x.dtype)      # sum per segment\n","    count = np.zeros((num_segments,), dtype=np.int64)     # count per segment\n","\n","    # Accumulate per segment\n","    for i in range(N):\n","        seg = int(segment_ids[i])\n","        if 0 <= seg < num_segments:\n","            out[seg] += x[i]\n","            count[seg] += 1\n","\n","    # Avoid div-by-zero by clamping denominator to at least 1\n","    denom = np.maximum(count, 1).reshape(num_segments, 1)  # (K, 1)\n","    mean = out / denom                                    # (K, D)\n","\n","    # For segments where count == 0, explicitly set mean to 0\n","    zero_mask = (count == 0)  # (K,)\n","    mean[zero_mask] = 0.0\n","\n","    if squeeze:\n","        mean = mean[:, 0]  # (K,)\n","    return mean\n","\n","# ------------------------------------------------------------\n","# 2. Data setup\n","# ------------------------------------------------------------\n","torch.manual_seed(0)\n","np.random.seed(0)\n","\n","x_torch_cpu = torch.randn(BATCH, N, dtype=torch.float32)\n","mask_torch_cpu = (torch.rand(BATCH, N) > 0.3).to(torch.bool)\n","\n","seg_ids_cpu = torch.randint(0, SEG_K, (BATCH,), dtype=torch.long)\n","\n","x_np = x_torch_cpu.numpy()\n","mask_np = mask_torch_cpu.numpy().astype(np.bool_)\n","seg_ids_np = seg_ids_cpu.numpy()\n","\n","if DEVICE == \"cuda\":\n","    x_torch_gpu = x_torch_cpu.to(\"cuda\")\n","    mask_torch_gpu = mask_torch_cpu.to(\"cuda\")\n","    seg_ids_gpu = seg_ids_cpu.to(\"cuda\")\n","else:\n","    x_torch_gpu = None\n","    mask_torch_gpu = None\n","    seg_ids_gpu = None\n","\n","# ------------------------------------------------------------\n","# 3. Triton kernel: row-wise mean (batch mean) for 2D tensor\n","# ------------------------------------------------------------\n","\n","@triton.jit\n","def row_mean_kernel(X_ptr, Y_ptr, BATCH, N, BLOCK_SIZE: tl.constexpr):\n","    row_id = tl.program_id(0)\n","    offs = row_id * N + tl.arange(0, BLOCK_SIZE)\n","    mask = offs < (row_id * N + N)\n","\n","    x = tl.load(X_ptr + offs, mask=mask, other=0.0)\n","    # parallel reduction in block\n","    # here we just sum and rely on BLOCK_SIZE == N for simplicity\n","    # (you can generalize to partial tiles if needed)\n","    s = tl.sum(x, axis=0)\n","    # each program handles a whole row\n","    denom = N\n","    mean = s / denom\n","    tl.store(Y_ptr + row_id, mean)\n","\n","def triton_row_mean(x: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    x: [B, N] on CUDA\n","    returns: [B]\n","    \"\"\"\n","    assert x.is_cuda\n","    B, N = x.shape\n","    y = torch.empty(B, device=x.device, dtype=x.dtype)\n","\n","    BLOCK_SIZE = N  # simple case: one block per row\n","    grid = (B,)\n","\n","    row_mean_kernel[grid](\n","        x, y,\n","        BATCH=B,\n","        N=N,\n","        BLOCK_SIZE=BLOCK_SIZE,\n","        num_warps=4,\n","    )\n","    return y\n","\n","# ------------------------------------------------------------\n","# 4. CUDA kernel with CuPy: row-wise mean\n","# ------------------------------------------------------------\n","\n","cuda_row_mean_src = r\"\"\"\n","extern \"C\" __global__\n","void row_mean(const float* __restrict__ x,\n","              float* __restrict__ y,\n","              int B, int N) {\n","    int row = blockIdx.x;\n","    if (row >= B) return;\n","\n","    float sum = 0.0f;\n","    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n","        sum += x[row * N + i];\n","    }\n","\n","    __shared__ float smem[256]; // up to 256 threads\n","    int tid = threadIdx.x;\n","    smem[tid] = sum;\n","    __syncthreads();\n","\n","    // simple reduction in shared memory\n","    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n","        if (tid < stride) {\n","            smem[tid] += smem[tid + stride];\n","        }\n","        __syncthreads();\n","    }\n","\n","    if (tid == 0) {\n","        y[row] = smem[0] / (float)N;\n","    }\n","}\n","\"\"\"\n","\n","row_mean_kernel_cuda = cp.RawKernel(cuda_row_mean_src, \"row_mean\")\n","\n","def cuda_row_mean(x_torch: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Takes a CUDA torch tensor [B, N], uses CuPy to run custom kernel,\n","    returns torch tensor [B].\n","    \"\"\"\n","    assert x_torch.is_cuda\n","    B, N = x_torch.shape\n","    # zero-copy via DLPack\n","    x_cu = cp.fromDlpack(torch.utils.dlpack.to_dlpack(x_torch))\n","    y_cu = cp.empty((B,), dtype=cp.float32)\n","\n","    threads_per_block = 256\n","    blocks = (B,)\n","\n","    row_mean_kernel_cuda(blocks, (threads_per_block,),\n","                         (x_cu, y_cu, B, N))\n","\n","    # back to torch\n","    y_torch = torch.utils.dlpack.from_dlpack(y_cu.toDlpack())\n","    return y_torch\n","\n","# ------------------------------------------------------------\n","# 5. Benchmark helpers\n","# ------------------------------------------------------------\n","\n","@dataclass\n","class BenchResult:\n","    name: str\n","    time_ms: float\n","\n","def bench(fn, iters=BENCH_ITERS, warmup=WARMUP_ITERS):\n","    # Warmup\n","    for _ in range(warmup):\n","        out = fn()\n","        if isinstance(out, torch.Tensor) and out.is_cuda:\n","            torch.cuda.synchronize()\n","\n","    t0 = time.perf_counter()\n","    for _ in range(iters):\n","        out = fn()\n","        if isinstance(out, torch.Tensor) and out.is_cuda:\n","            torch.cuda.synchronize()\n","    t1 = time.perf_counter()\n","    return (t1 - t0) * 1000.0 / iters\n","\n","results = []\n","\n","# ------------------------------------------------------------\n","# 6. PyTorch CPU benchmarks\n","# ------------------------------------------------------------\n","print(\"\\n=== PyTorch CPU ===\")\n","\n","# mean (batch-level: row-wise)\n","def fn_torch_cpu_batch_mean():\n","    return torch_safe_mean(x_torch_cpu, dim=1)  # [B]\n","\n","t = bench(fn_torch_cpu_batch_mean)\n","results.append(BenchResult(\"torch_cpu_batch_mean\", t))\n","print(\"torch_cpu_batch_mean: %.3f ms\" % t)\n","\n","# masked mean (row-wise)\n","def fn_torch_cpu_masked_mean():\n","    return torch_masked_mean(x_torch_cpu, mask_torch_cpu, dim=1)\n","\n","t = bench(fn_torch_cpu_masked_mean)\n","results.append(BenchResult(\"torch_cpu_masked_mean\", t))\n","print(\"torch_cpu_masked_mean: %.3f ms\" % t)\n","\n","# segment mean (over batch dimension)\n","def fn_torch_cpu_segment_mean():\n","    return torch_segment_mean(x_torch_cpu, seg_ids_cpu, num_segments=SEG_K)\n","\n","t = bench(fn_torch_cpu_segment_mean)\n","results.append(BenchResult(\"torch_cpu_segment_mean\", t))\n","print(\"torch_cpu_segment_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 7. PyTorch GPU benchmarks (if available)\n","# ------------------------------------------------------------\n","if DEVICE == \"cuda\":\n","    print(\"\\n=== PyTorch GPU ===\")\n","\n","    def fn_torch_gpu_batch_mean():\n","        return torch_safe_mean(x_torch_gpu, dim=1)\n","\n","    t = bench(fn_torch_gpu_batch_mean)\n","    results.append(BenchResult(\"torch_gpu_batch_mean\", t))\n","    print(\"torch_gpu_batch_mean: %.3f ms\" % t)\n","\n","    def fn_torch_gpu_masked_mean():\n","        return torch_masked_mean(x_torch_gpu, mask_torch_gpu, dim=1)\n","\n","    t = bench(fn_torch_gpu_masked_mean)\n","    results.append(BenchResult(\"torch_gpu_masked_mean\", t))\n","    print(\"torch_gpu_masked_mean: %.3f ms\" % t)\n","\n","    def fn_torch_gpu_segment_mean():\n","        return torch_segment_mean(x_torch_gpu, seg_ids_gpu, num_segments=SEG_K)\n","\n","    t = bench(fn_torch_gpu_segment_mean)\n","    results.append(BenchResult(\"torch_gpu_segment_mean\", t))\n","    print(\"torch_gpu_segment_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 8. NumPy CPU benchmarks\n","# ------------------------------------------------------------\n","print(\"\\n=== NumPy CPU ===\")\n","\n","def fn_numpy_batch_mean():\n","    return np_safe_mean(x_np, axis=1)\n","\n","t = bench(fn_numpy_batch_mean)\n","results.append(BenchResult(\"numpy_batch_mean\", t))\n","print(\"numpy_batch_mean: %.3f ms\" % t)\n","\n","def fn_numpy_masked_mean():\n","    return np_masked_mean(x_np, mask_np, axis=1)\n","\n","t = bench(fn_numpy_masked_mean)\n","results.append(BenchResult(\"numpy_masked_mean\", t))\n","print(\"numpy_masked_mean: %.3f ms\" % t)\n","\n","def fn_numpy_segment_mean():\n","    return np_segment_mean(x_np, seg_ids_np, num_segments=SEG_K)\n","\n","t = bench(fn_numpy_segment_mean)\n","results.append(BenchResult(\"numpy_segment_mean\", t))\n","print(\"numpy_segment_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 9. Triton benchmark (row-wise mean only)\n","# ------------------------------------------------------------\n","if DEVICE == \"cuda\":\n","    print(\"\\n=== Triton row-wise mean (CUDA) ===\")\n","\n","    def fn_triton_row_mean():\n","        return triton_row_mean(x_torch_gpu)\n","\n","    t = bench(fn_triton_row_mean)\n","    results.append(BenchResult(\"triton_row_mean\", t))\n","    print(\"triton_row_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 10. CUDA (CuPy) kernel benchmark (row-wise mean only)\n","# ------------------------------------------------------------\n","if DEVICE == \"cuda\":\n","    print(\"\\n=== CUDA (CuPy) row-wise mean ===\")\n","\n","    def fn_cuda_row_mean():\n","        return cuda_row_mean(x_torch_gpu)\n","\n","    t = bench(fn_cuda_row_mean)\n","    results.append(BenchResult(\"cuda_row_mean\", t))\n","    print(\"cuda_row_mean: %.3f ms\" % t)\n","\n","# ------------------------------------------------------------\n","# 11. Summary\n","# ------------------------------------------------------------\n","print(\"\\n=== Summary (ms per call) ===\")\n","for r in results:\n","    print(f\"{r.name:30s}: {r.time_ms:8.3f} ms\")"],"metadata":{"id":"cF_qp5KNuyyz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Math trick in self attention\n","Karpathy YT https://www.youtube.com/watch?v=kCc8FmEb1nY 42:27\n"],"metadata":{"id":"wUPZFtNNE40E"}},{"cell_type":"code","source":["\n","torch.manual_seed(1337)\n","B, T, C = 4,8,2\n","x = torch.randn(B,T,C)\n","x.shape\n","#we want to look at past tokens from currnt positio\n","# Batch, Time, Channels\n","#simplest way to communicate with past tokens is to take average of tokens before\n","# current token. This vector of 5 past tokens with an average becomes\n","#\n","xbow = torch.zeros(B,T,C)\n","for batch in range(B):\n","  for time in range(T):\n","    xprev = x[batch, time+1, ] #t,C\n","    xbow[b,t] = torch.mean()\n"],"metadata":{"id":"aoQwFMnrAw1U"},"execution_count":null,"outputs":[]}]}